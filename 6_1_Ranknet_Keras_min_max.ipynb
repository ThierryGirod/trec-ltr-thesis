{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/ubuntu/.local/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.10/site-packages (3.6.2)\n",
      "Requirement already satisfied: scikit-learn in /home/ubuntu/.local/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: pydot in /home/ubuntu/.local/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (2.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (0.29.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (22.12.6)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (1.24.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow matplotlib scikit-learn pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import backend\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Activation, Subtract, Dense, Input\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import requests\n",
    "from ltr.data import CorpusApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 24\n",
    "h_1_dim = 128\n",
    "h_2_dim = h_1_dim // 2\n",
    "h_3_dim = h_2_dim // 2\n",
    "\n",
    "# Model.\n",
    "h_1 = Dense(h_1_dim, activation = \"relu\")\n",
    "h_2 = Dense(h_2_dim, activation = \"relu\")\n",
    "h_3 = Dense(h_3_dim, activation = \"relu\")\n",
    "s = Dense(1)\n",
    "\n",
    "# Relevant document score.\n",
    "rel_doc = Input(shape = (INPUT_DIM, ), dtype = \"float32\")\n",
    "h_1_rel = h_1(rel_doc)\n",
    "h_2_rel = h_2(h_1_rel)\n",
    "h_3_rel = h_3(h_2_rel)\n",
    "rel_score = s(h_3_rel)\n",
    "\n",
    "# Irrelevant document score.\n",
    "irr_doc = Input(shape = (INPUT_DIM, ), dtype = \"float32\")\n",
    "h_1_irr = h_1(irr_doc)\n",
    "h_2_irr = h_2(h_1_irr)\n",
    "h_3_irr = h_3(h_2_irr)\n",
    "irr_score = s(h_3_irr)\n",
    "\n",
    "# Subtract scores.\n",
    "diff = Subtract()([rel_score, irr_score])\n",
    "\n",
    "# Pass difference through sigmoid function.\n",
    "prob = Activation(\"sigmoid\")(diff)\n",
    "\n",
    "# Build model.\n",
    "model = Model(inputs = [rel_doc, irr_doc], outputs = prob)\n",
    "model.compile(optimizer = \"adadelta\", loss = \"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "def getJudgmentsBatchFileByFile():\n",
    "    \"\"\"\n",
    "    Returns a generator function that returns all the judgment batches files from the directory\n",
    "    \"\"\"\n",
    "    files = []\n",
    "   \n",
    "    files = [join('./loggedFeatures', file) for file in listdir('./loggedFeatures') if isfile(join('./loggedFeatures', file))]\n",
    "    for file in files:\n",
    "        yield file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "judgments = []\n",
    "for file in getJudgmentsBatchFileByFile():\n",
    "  with open(file,'r') as f:\n",
    "    reader = csv.reader(f, delimiter=' ')\n",
    "    for row in reader:\n",
    "      data = []\n",
    "      for element in row:\n",
    "        data.append(element.replace(',', ''))\n",
    "      judgments.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '10000', 'msmarco_doc_05_72507775', '1.0', '2.0', '8.0', '8.0', '0.083333336', '0.16666667', '0.6666667', '0.6666667', '1.0', '2.4142137', '36.53677', '36.764153', '90.677284', '72.23833', '41.410515', '41.410515', '7.282527', '10.645987', '84.99544', '85.76555', '1.655931', '1.747792', '5.9285426', '6.021519']\n",
      "['1', '10000', 'msmarco_doc_10_1691063043', '1.0', '8.0', '9.0', '9.0', '0.083333336', '0.6666667', '0.75', '0.75', '1.0', '11.69213', '55.452248', '56.990223', '90.677284', '72.23833', '41.410515', '41.410515', '7.7639008', '64.12446', '153.44191', '158.3429', '3.4576335', '13.961745', '14.04283', '14.179642']\n",
      "['1', '1000005', 'msmarco_doc_19_673141443', '2.0', '2.0', '4.0', '4.0', '0.5', '0.5', '1.0', '1.0', '2.0', '2.828427', '18.934044', '19.41233', '27.758165', '22.795256', '13.165058', '13.165058', '17.45566', '22.153984', '77.66603', '79.93644', '9.476915', '9.96361', '7.746788', '7.7889223']\n",
      "['0', '1000005', 'msmarco_doc_19_673231526', '1.0', '1.0', '2.0', '2.0', '0.25', '0.25', '0.5', '0.5', '1.0', '1.4142135', '8.70179', '8.973479', '27.758165', '22.795256', '13.165058', '13.165058', '10.839218', '13.648185', '38.889122', '40.672127', '6.04047', '6.534829', '5.3023043', '5.3333135']\n"
     ]
    }
   ],
   "source": [
    "judgments.sort(key = lambda judgments: judgments[1])\n",
    "print(judgments[0])\n",
    "print(judgments[1])\n",
    "print(judgments[2])\n",
    "print(judgments[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '10000', '1.0', '2.0', '8.0', '8.0', '0.083333336', '0.16666667', '0.6666667', '0.6666667', '1.0', '2.4142137', '36.53677', '36.764153', '90.677284', '72.23833', '41.410515', '41.410515', '7.282527', '10.645987', '84.99544', '85.76555', '1.655931', '1.747792', '5.9285426', '6.021519']\n",
      "['1', '10000', '1.0', '8.0', '9.0', '9.0', '0.083333336', '0.6666667', '0.75', '0.75', '1.0', '11.69213', '55.452248', '56.990223', '90.677284', '72.23833', '41.410515', '41.410515', '7.7639008', '64.12446', '153.44191', '158.3429', '3.4576335', '13.961745', '14.04283', '14.179642']\n",
      "['1', '1000005', '2.0', '2.0', '4.0', '4.0', '0.5', '0.5', '1.0', '1.0', '2.0', '2.828427', '18.934044', '19.41233', '27.758165', '22.795256', '13.165058', '13.165058', '17.45566', '22.153984', '77.66603', '79.93644', '9.476915', '9.96361', '7.746788', '7.7889223']\n",
      "['0', '1000005', '1.0', '1.0', '2.0', '2.0', '0.25', '0.25', '0.5', '0.5', '1.0', '1.4142135', '8.70179', '8.973479', '27.758165', '22.795256', '13.165058', '13.165058', '10.839218', '13.648185', '38.889122', '40.672127', '6.04047', '6.534829', '5.3023043', '5.3333135']\n",
      "['1', '1000005', '2.0', '2.0', '3.0', '3.0', '0.5', '0.5', '0.75', '0.75', '2.0', '7.8929224', '22.769188', '24.567732', '27.758165', '22.795256', '13.165058', '13.165058', '17.45566', '68.04159', '103.90141', '114.40033', '4.07918', '10.506668', '7.384706', '7.5171304']\n",
      "['0', '1000005', '0.0', '0.0', '1.0', '1.0', '0.0', '0.0', '0.25', '0.25', '0.0', '0.0', '4.1231055', '4.1231055', '27.758165', '22.795256', '13.165058', '13.165058', '0.0', '0.0', '4.4108863', '4.410886', '0.0', '0.0', '0.0', '0.0']\n",
      "['0', '1000007', '1.0', '1.0', '3.0', '3.0', '0.25', '0.25', '0.75', '0.75', '1.0', '1.4142135', '13.934044', '14.120829', '29.15374', '24.19186', '14.106765', '14.106753', '10.839218', '13.648185', '59.74831', '60.974113', '6.04047', '6.3175864', '5.3736296', '5.388391']\n",
      "['1', '1000007', '1.0', '1.0', '2.0', '2.0', '0.25', '0.25', '0.5', '0.5', '1.0', '1.4142135', '8.70179', '8.973479', '29.15374', '24.19186', '14.106765', '14.106753', '10.839218', '13.648185', '38.889122', '40.672127', '6.04047', '6.534829', '5.3023043', '5.3333135']\n"
     ]
    }
   ],
   "source": [
    "for j in judgments:\n",
    "  del j[2]\n",
    "\n",
    "print(judgments[0])\n",
    "print(judgments[1])\n",
    "print(judgments[2])\n",
    "print(judgments[3])\n",
    "print(judgments[4])\n",
    "print(judgments[5])\n",
    "print(judgments[6])\n",
    "print(judgments[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeFeatures(loggedJudgments):\n",
    "    minimums = [100 for _ in loggedJudgments[0][2:]]\n",
    "    maximums = [0 for _ in loggedJudgments[0][2:]]\n",
    "    \n",
    "    for judgment in loggedJudgments:\n",
    "        for idx, feature in enumerate(judgment[2:]):\n",
    "            if minimums[idx] > float(feature):\n",
    "                minimums[idx] = float(feature)\n",
    "            \n",
    "            if maximums[idx] < float(feature):\n",
    "                maximums[idx] = float(feature)\n",
    "    \n",
    "    \n",
    "        \n",
    "    normedJudgments = []\n",
    "    for judgment in loggedJudgments:\n",
    "        normedFeatures = [0 for _ in judgment[2:]]\n",
    "        for idx, feature in enumerate(judgment[2:]):\n",
    "            normedFeatures[idx] = (float(feature) - minimums[idx]) / (maximums[idx] - minimums[idx])\n",
    "        normedJudgment = judgment[:2]\n",
    "        normedJudgment.extend(normedFeatures)\n",
    "        normedJudgments.append(normedJudgment)\n",
    "    \n",
    "    return minimums, maximums, normedJudgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimums, maximums, normalizedJudgments = normalizeFeatures(judgments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written: ./submission/ranknet/data/minMaxNormalizedFeatures.csv\n"
     ]
    }
   ],
   "source": [
    "CorpusApi.saveListAsFile('./submission/ranknet/data/minMaxNormalizedFeatures.csv', normalizedJudgments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairs\n",
    "xi = []\n",
    "xj = []\n",
    "pij = []\n",
    "\n",
    "for i in range(1, len(normalizedJudgments), 2):\n",
    "  if normalizedJudgments[i-1][0] < normalizedJudgments[i][0]:\n",
    "    _pij = 0\n",
    "    xi.append(normalizedJudgments[i - 1][2:])\n",
    "    xj.append(normalizedJudgments[i][2:])\n",
    "    pij.append(_pij)\n",
    "  elif normalizedJudgments[i-1][0] > normalizedJudgments[i][0]:\n",
    "    _pij = 1\n",
    "    xi.append(normalizedJudgments[i - 1][2:])\n",
    "    xj.append(normalizedJudgments[i][2:])\n",
    "    pij.append(_pij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = np.array(xi, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xj = np.array(xj, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pij = np.array(pij, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xi_train, xi_test, xj_train, xj_test, pij_train, pij_test = train_test_split(\n",
    "    xi, xj, pij, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written: ./submission/ranknet/data/minMaxNormalizedFeatures_xiTrain.csv\n",
      "File written: ./submission/ranknet/data/minMaxNormalizedFeatures_xiTest.csv\n",
      "File written: ./submission/ranknet/data/minMaxNormalizedFeatures_xjTrain.csv\n",
      "File written: ./submission/ranknet/data/minMaxNormalizedFeatures_xjTest.csv\n",
      "File written: ./submission/ranknet/data/minMaxNormalizedFeatures_pijTrain.csv\n",
      "File written: ./submission/ranknet/data/minMaxNormalizedFeatures_pijTest.csv\n"
     ]
    }
   ],
   "source": [
    "CorpusApi.saveListAsFile('./submission/ranknet/data/minMaxNormalizedFeatures_xiTrain.csv', xi_train)\n",
    "CorpusApi.saveListAsFile('./submission/ranknet/data/minMaxNormalizedFeatures_xiTest.csv', xi_test)\n",
    "\n",
    "CorpusApi.saveListAsFile('./submission/ranknet/data/minMaxNormalizedFeatures_xjTrain.csv', xj_train)\n",
    "CorpusApi.saveListAsFile('./submission/ranknet/data/minMaxNormalizedFeatures_xjTest.csv', xj_test)\n",
    "\n",
    "CorpusApi.saveListAsFile('./submission/ranknet/data/minMaxNormalizedFeatures_pijTrain.csv', pij_train)\n",
    "CorpusApi.saveListAsFile('./submission/ranknet/data/minMaxNormalizedFeatures_pijTest.csv', pij_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.6671 - accuracy: 0.8838\n",
      "Epoch 1: val_loss improved from inf to 0.65147, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 8s 7ms/step - loss: 0.6671 - accuracy: 0.8839 - val_loss: 0.6515 - val_accuracy: 0.9079\n",
      "Epoch 2/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.6319 - accuracy: 0.9357\n",
      "Epoch 2: val_loss improved from 0.65147 to 0.61337, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.6318 - accuracy: 0.9357 - val_loss: 0.6134 - val_accuracy: 0.9173\n",
      "Epoch 3/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.5916 - accuracy: 0.9376\n",
      "Epoch 3: val_loss improved from 0.61337 to 0.57186, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.5916 - accuracy: 0.9376 - val_loss: 0.5719 - val_accuracy: 0.9155\n",
      "Epoch 4/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.5444 - accuracy: 0.9364\n",
      "Epoch 4: val_loss improved from 0.57186 to 0.52048, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.5444 - accuracy: 0.9364 - val_loss: 0.5205 - val_accuracy: 0.9148\n",
      "Epoch 5/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.4856 - accuracy: 0.9359\n",
      "Epoch 5: val_loss improved from 0.52048 to 0.45835, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.4855 - accuracy: 0.9359 - val_loss: 0.4584 - val_accuracy: 0.9140\n",
      "Epoch 6/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.4174 - accuracy: 0.9356\n",
      "Epoch 6: val_loss improved from 0.45835 to 0.39225, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.4173 - accuracy: 0.9356 - val_loss: 0.3922 - val_accuracy: 0.9145\n",
      "Epoch 7/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.3495 - accuracy: 0.9358\n",
      "Epoch 7: val_loss improved from 0.39225 to 0.33286, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.3492 - accuracy: 0.9359 - val_loss: 0.3329 - val_accuracy: 0.9155\n",
      "Epoch 8/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.9368\n",
      "Epoch 8: val_loss improved from 0.33286 to 0.28796, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.2917 - accuracy: 0.9367 - val_loss: 0.2880 - val_accuracy: 0.9164\n",
      "Epoch 9/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.2497 - accuracy: 0.9374\n",
      "Epoch 9: val_loss improved from 0.28796 to 0.25781, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.2497 - accuracy: 0.9374 - val_loss: 0.2578 - val_accuracy: 0.9171\n",
      "Epoch 10/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.2216 - accuracy: 0.9378\n",
      "Epoch 10: val_loss improved from 0.25781 to 0.23863, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.2215 - accuracy: 0.9378 - val_loss: 0.2386 - val_accuracy: 0.9177\n",
      "Epoch 11/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.2032 - accuracy: 0.9384\n",
      "Epoch 11: val_loss improved from 0.23863 to 0.22647, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.2031 - accuracy: 0.9385 - val_loss: 0.2265 - val_accuracy: 0.9183\n",
      "Epoch 12/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.1910 - accuracy: 0.9390\n",
      "Epoch 12: val_loss improved from 0.22647 to 0.21855, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1908 - accuracy: 0.9390 - val_loss: 0.2185 - val_accuracy: 0.9189\n",
      "Epoch 13/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.1823 - accuracy: 0.9395\n",
      "Epoch 13: val_loss improved from 0.21855 to 0.21313, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1824 - accuracy: 0.9394 - val_loss: 0.2131 - val_accuracy: 0.9189\n",
      "Epoch 14/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.1763 - accuracy: 0.9398\n",
      "Epoch 14: val_loss improved from 0.21313 to 0.20921, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1763 - accuracy: 0.9398 - val_loss: 0.2092 - val_accuracy: 0.9194\n",
      "Epoch 15/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.1717 - accuracy: 0.9401\n",
      "Epoch 15: val_loss improved from 0.20921 to 0.20619, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1717 - accuracy: 0.9401 - val_loss: 0.2062 - val_accuracy: 0.9197\n",
      "Epoch 16/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.1681 - accuracy: 0.9403\n",
      "Epoch 16: val_loss improved from 0.20619 to 0.20377, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1679 - accuracy: 0.9404 - val_loss: 0.2038 - val_accuracy: 0.9202\n",
      "Epoch 17/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1649 - accuracy: 0.9407\n",
      "Epoch 17: val_loss improved from 0.20377 to 0.20178, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1649 - accuracy: 0.9407 - val_loss: 0.2018 - val_accuracy: 0.9205\n",
      "Epoch 18/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.1623 - accuracy: 0.9410\n",
      "Epoch 18: val_loss improved from 0.20178 to 0.20008, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1622 - accuracy: 0.9409 - val_loss: 0.2001 - val_accuracy: 0.9209\n",
      "Epoch 19/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1599 - accuracy: 0.9414\n",
      "Epoch 19: val_loss improved from 0.20008 to 0.19862, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1600 - accuracy: 0.9414 - val_loss: 0.1986 - val_accuracy: 0.9209\n",
      "Epoch 20/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.1580 - accuracy: 0.9416\n",
      "Epoch 20: val_loss improved from 0.19862 to 0.19733, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1580 - accuracy: 0.9416 - val_loss: 0.1973 - val_accuracy: 0.9213\n",
      "Epoch 21/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.1562 - accuracy: 0.9418\n",
      "Epoch 21: val_loss improved from 0.19733 to 0.19616, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1563 - accuracy: 0.9418 - val_loss: 0.1962 - val_accuracy: 0.9216\n",
      "Epoch 22/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.9424\n",
      "Epoch 22: val_loss improved from 0.19616 to 0.19506, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1547 - accuracy: 0.9423 - val_loss: 0.1951 - val_accuracy: 0.9219\n",
      "Epoch 23/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.1532 - accuracy: 0.9425\n",
      "Epoch 23: val_loss improved from 0.19506 to 0.19401, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.1532 - accuracy: 0.9425 - val_loss: 0.1940 - val_accuracy: 0.9224\n",
      "Epoch 24/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.1518 - accuracy: 0.9429\n",
      "Epoch 24: val_loss improved from 0.19401 to 0.19301, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1518 - accuracy: 0.9429 - val_loss: 0.1930 - val_accuracy: 0.9229\n",
      "Epoch 25/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.1505 - accuracy: 0.9433\n",
      "Epoch 25: val_loss improved from 0.19301 to 0.19206, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1505 - accuracy: 0.9432 - val_loss: 0.1921 - val_accuracy: 0.9236\n",
      "Epoch 26/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.1494 - accuracy: 0.9435\n",
      "Epoch 26: val_loss improved from 0.19206 to 0.19114, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1493 - accuracy: 0.9435 - val_loss: 0.1911 - val_accuracy: 0.9241\n",
      "Epoch 27/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.9437\n",
      "Epoch 27: val_loss improved from 0.19114 to 0.19021, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1482 - accuracy: 0.9437 - val_loss: 0.1902 - val_accuracy: 0.9244\n",
      "Epoch 28/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.1472 - accuracy: 0.9439\n",
      "Epoch 28: val_loss improved from 0.19021 to 0.18928, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1471 - accuracy: 0.9440 - val_loss: 0.1893 - val_accuracy: 0.9248\n",
      "Epoch 29/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1460 - accuracy: 0.9442\n",
      "Epoch 29: val_loss improved from 0.18928 to 0.18835, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1460 - accuracy: 0.9442 - val_loss: 0.1883 - val_accuracy: 0.9253\n",
      "Epoch 30/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.1449 - accuracy: 0.9443\n",
      "Epoch 30: val_loss improved from 0.18835 to 0.18746, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1449 - accuracy: 0.9443 - val_loss: 0.1875 - val_accuracy: 0.9253\n",
      "Epoch 31/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.1439 - accuracy: 0.9445\n",
      "Epoch 31: val_loss improved from 0.18746 to 0.18656, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1439 - accuracy: 0.9446 - val_loss: 0.1866 - val_accuracy: 0.9254\n",
      "Epoch 32/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1428 - accuracy: 0.9449\n",
      "Epoch 32: val_loss improved from 0.18656 to 0.18566, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1428 - accuracy: 0.9448 - val_loss: 0.1857 - val_accuracy: 0.9257\n",
      "Epoch 33/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1418 - accuracy: 0.9450\n",
      "Epoch 33: val_loss improved from 0.18566 to 0.18475, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1418 - accuracy: 0.9450 - val_loss: 0.1848 - val_accuracy: 0.9260\n",
      "Epoch 34/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.1408 - accuracy: 0.9454\n",
      "Epoch 34: val_loss improved from 0.18475 to 0.18384, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1409 - accuracy: 0.9453 - val_loss: 0.1838 - val_accuracy: 0.9260\n",
      "Epoch 35/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.9454\n",
      "Epoch 35: val_loss improved from 0.18384 to 0.18291, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1399 - accuracy: 0.9454 - val_loss: 0.1829 - val_accuracy: 0.9264\n",
      "Epoch 36/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.9456\n",
      "Epoch 36: val_loss improved from 0.18291 to 0.18198, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1389 - accuracy: 0.9456 - val_loss: 0.1820 - val_accuracy: 0.9267\n",
      "Epoch 37/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.1377 - accuracy: 0.9459\n",
      "Epoch 37: val_loss improved from 0.18198 to 0.18107, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1379 - accuracy: 0.9459 - val_loss: 0.1811 - val_accuracy: 0.9269\n",
      "Epoch 38/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9462\n",
      "Epoch 38: val_loss improved from 0.18107 to 0.18018, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1370 - accuracy: 0.9462 - val_loss: 0.1802 - val_accuracy: 0.9271\n",
      "Epoch 39/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.1358 - accuracy: 0.9467\n",
      "Epoch 39: val_loss improved from 0.18018 to 0.17926, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1360 - accuracy: 0.9466 - val_loss: 0.1793 - val_accuracy: 0.9274\n",
      "Epoch 40/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.1353 - accuracy: 0.9467\n",
      "Epoch 40: val_loss improved from 0.17926 to 0.17834, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.1351 - accuracy: 0.9468 - val_loss: 0.1783 - val_accuracy: 0.9277\n",
      "Epoch 41/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.1342 - accuracy: 0.9470\n",
      "Epoch 41: val_loss improved from 0.17834 to 0.17742, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1342 - accuracy: 0.9470 - val_loss: 0.1774 - val_accuracy: 0.9278\n",
      "Epoch 42/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.1334 - accuracy: 0.9472\n",
      "Epoch 42: val_loss improved from 0.17742 to 0.17650, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1332 - accuracy: 0.9472 - val_loss: 0.1765 - val_accuracy: 0.9278\n",
      "Epoch 43/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.1324 - accuracy: 0.9474\n",
      "Epoch 43: val_loss improved from 0.17650 to 0.17556, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1323 - accuracy: 0.9474 - val_loss: 0.1756 - val_accuracy: 0.9280\n",
      "Epoch 44/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.1316 - accuracy: 0.9475\n",
      "Epoch 44: val_loss improved from 0.17556 to 0.17463, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1314 - accuracy: 0.9476 - val_loss: 0.1746 - val_accuracy: 0.9281\n",
      "Epoch 45/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9477\n",
      "Epoch 45: val_loss improved from 0.17463 to 0.17374, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1305 - accuracy: 0.9477 - val_loss: 0.1737 - val_accuracy: 0.9284\n",
      "Epoch 46/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.1295 - accuracy: 0.9480\n",
      "Epoch 46: val_loss improved from 0.17374 to 0.17280, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1296 - accuracy: 0.9480 - val_loss: 0.1728 - val_accuracy: 0.9287\n",
      "Epoch 47/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9482\n",
      "Epoch 47: val_loss improved from 0.17280 to 0.17186, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1287 - accuracy: 0.9481 - val_loss: 0.1719 - val_accuracy: 0.9289\n",
      "Epoch 48/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.1281 - accuracy: 0.9481\n",
      "Epoch 48: val_loss improved from 0.17186 to 0.17093, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1278 - accuracy: 0.9482 - val_loss: 0.1709 - val_accuracy: 0.9291\n",
      "Epoch 49/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 0.9484\n",
      "Epoch 49: val_loss improved from 0.17093 to 0.17000, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.1269 - accuracy: 0.9485 - val_loss: 0.1700 - val_accuracy: 0.9295\n",
      "Epoch 50/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9486\n",
      "Epoch 50: val_loss improved from 0.17000 to 0.16910, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1260 - accuracy: 0.9486 - val_loss: 0.1691 - val_accuracy: 0.9298\n",
      "Epoch 51/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9490\n",
      "Epoch 51: val_loss improved from 0.16910 to 0.16817, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1252 - accuracy: 0.9489 - val_loss: 0.1682 - val_accuracy: 0.9298\n",
      "Epoch 52/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9492\n",
      "Epoch 52: val_loss improved from 0.16817 to 0.16724, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1243 - accuracy: 0.9491 - val_loss: 0.1672 - val_accuracy: 0.9302\n",
      "Epoch 53/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9495\n",
      "Epoch 53: val_loss improved from 0.16724 to 0.16632, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1234 - accuracy: 0.9494 - val_loss: 0.1663 - val_accuracy: 0.9306\n",
      "Epoch 54/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9495\n",
      "Epoch 54: val_loss improved from 0.16632 to 0.16541, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1226 - accuracy: 0.9495 - val_loss: 0.1654 - val_accuracy: 0.9309\n",
      "Epoch 55/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9497\n",
      "Epoch 55: val_loss improved from 0.16541 to 0.16450, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1217 - accuracy: 0.9497 - val_loss: 0.1645 - val_accuracy: 0.9311\n",
      "Epoch 56/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9499\n",
      "Epoch 56: val_loss improved from 0.16450 to 0.16364, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.1209 - accuracy: 0.9500 - val_loss: 0.1636 - val_accuracy: 0.9312\n",
      "Epoch 57/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9501\n",
      "Epoch 57: val_loss improved from 0.16364 to 0.16278, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1201 - accuracy: 0.9501 - val_loss: 0.1628 - val_accuracy: 0.9315\n",
      "Epoch 58/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.9504\n",
      "Epoch 58: val_loss improved from 0.16278 to 0.16193, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1193 - accuracy: 0.9504 - val_loss: 0.1619 - val_accuracy: 0.9317\n",
      "Epoch 59/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.1185 - accuracy: 0.9505\n",
      "Epoch 59: val_loss improved from 0.16193 to 0.16109, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1185 - accuracy: 0.9505 - val_loss: 0.1611 - val_accuracy: 0.9319\n",
      "Epoch 60/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.1178 - accuracy: 0.9506\n",
      "Epoch 60: val_loss improved from 0.16109 to 0.16027, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1177 - accuracy: 0.9506 - val_loss: 0.1603 - val_accuracy: 0.9321\n",
      "Epoch 61/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1170 - accuracy: 0.9509\n",
      "Epoch 61: val_loss improved from 0.16027 to 0.15947, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1170 - accuracy: 0.9509 - val_loss: 0.1595 - val_accuracy: 0.9324\n",
      "Epoch 62/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.1162 - accuracy: 0.9510\n",
      "Epoch 62: val_loss improved from 0.15947 to 0.15869, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1162 - accuracy: 0.9510 - val_loss: 0.1587 - val_accuracy: 0.9329\n",
      "Epoch 63/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.1156 - accuracy: 0.9512\n",
      "Epoch 63: val_loss improved from 0.15869 to 0.15791, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1155 - accuracy: 0.9512 - val_loss: 0.1579 - val_accuracy: 0.9332\n",
      "Epoch 64/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9514\n",
      "Epoch 64: val_loss improved from 0.15791 to 0.15718, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1148 - accuracy: 0.9514 - val_loss: 0.1572 - val_accuracy: 0.9334\n",
      "Epoch 65/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.1141 - accuracy: 0.9517\n",
      "Epoch 65: val_loss improved from 0.15718 to 0.15647, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1141 - accuracy: 0.9517 - val_loss: 0.1565 - val_accuracy: 0.9337\n",
      "Epoch 66/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1135 - accuracy: 0.9519\n",
      "Epoch 66: val_loss improved from 0.15647 to 0.15575, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1135 - accuracy: 0.9519 - val_loss: 0.1558 - val_accuracy: 0.9344\n",
      "Epoch 67/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1128 - accuracy: 0.9523\n",
      "Epoch 67: val_loss improved from 0.15575 to 0.15507, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1128 - accuracy: 0.9523 - val_loss: 0.1551 - val_accuracy: 0.9347\n",
      "Epoch 68/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9525\n",
      "Epoch 68: val_loss improved from 0.15507 to 0.15439, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1122 - accuracy: 0.9525 - val_loss: 0.1544 - val_accuracy: 0.9348\n",
      "Epoch 69/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1116 - accuracy: 0.9527\n",
      "Epoch 69: val_loss improved from 0.15439 to 0.15376, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1115 - accuracy: 0.9527 - val_loss: 0.1538 - val_accuracy: 0.9350\n",
      "Epoch 70/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1109 - accuracy: 0.9529\n",
      "Epoch 70: val_loss improved from 0.15376 to 0.15312, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1109 - accuracy: 0.9529 - val_loss: 0.1531 - val_accuracy: 0.9353\n",
      "Epoch 71/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9530\n",
      "Epoch 71: val_loss improved from 0.15312 to 0.15249, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1103 - accuracy: 0.9530 - val_loss: 0.1525 - val_accuracy: 0.9357\n",
      "Epoch 72/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1097 - accuracy: 0.9533\n",
      "Epoch 72: val_loss improved from 0.15249 to 0.15189, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1098 - accuracy: 0.9533 - val_loss: 0.1519 - val_accuracy: 0.9360\n",
      "Epoch 73/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.1092 - accuracy: 0.9536\n",
      "Epoch 73: val_loss improved from 0.15189 to 0.15130, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1092 - accuracy: 0.9536 - val_loss: 0.1513 - val_accuracy: 0.9363\n",
      "Epoch 74/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9537\n",
      "Epoch 74: val_loss improved from 0.15130 to 0.15074, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1086 - accuracy: 0.9537 - val_loss: 0.1507 - val_accuracy: 0.9366\n",
      "Epoch 75/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.1081 - accuracy: 0.9539\n",
      "Epoch 75: val_loss improved from 0.15074 to 0.15021, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1081 - accuracy: 0.9539 - val_loss: 0.1502 - val_accuracy: 0.9367\n",
      "Epoch 76/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9541\n",
      "Epoch 76: val_loss improved from 0.15021 to 0.14967, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1076 - accuracy: 0.9541 - val_loss: 0.1497 - val_accuracy: 0.9368\n",
      "Epoch 77/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.1069 - accuracy: 0.9543\n",
      "Epoch 77: val_loss improved from 0.14967 to 0.14914, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1071 - accuracy: 0.9542 - val_loss: 0.1491 - val_accuracy: 0.9372\n",
      "Epoch 78/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.1065 - accuracy: 0.9545\n",
      "Epoch 78: val_loss improved from 0.14914 to 0.14863, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 7ms/step - loss: 0.1066 - accuracy: 0.9545 - val_loss: 0.1486 - val_accuracy: 0.9374\n",
      "Epoch 79/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9547\n",
      "Epoch 79: val_loss improved from 0.14863 to 0.14812, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1061 - accuracy: 0.9547 - val_loss: 0.1481 - val_accuracy: 0.9375\n",
      "Epoch 80/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.1056 - accuracy: 0.9551\n",
      "Epoch 80: val_loss improved from 0.14812 to 0.14765, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1056 - accuracy: 0.9551 - val_loss: 0.1476 - val_accuracy: 0.9376\n",
      "Epoch 81/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.1051 - accuracy: 0.9552\n",
      "Epoch 81: val_loss improved from 0.14765 to 0.14717, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1052 - accuracy: 0.9553 - val_loss: 0.1472 - val_accuracy: 0.9378\n",
      "Epoch 82/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.1047 - accuracy: 0.9556\n",
      "Epoch 82: val_loss improved from 0.14717 to 0.14670, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1047 - accuracy: 0.9555 - val_loss: 0.1467 - val_accuracy: 0.9383\n",
      "Epoch 83/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.1043 - accuracy: 0.9557\n",
      "Epoch 83: val_loss improved from 0.14670 to 0.14623, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1043 - accuracy: 0.9557 - val_loss: 0.1462 - val_accuracy: 0.9385\n",
      "Epoch 84/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.1038 - accuracy: 0.9560\n",
      "Epoch 84: val_loss improved from 0.14623 to 0.14578, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1038 - accuracy: 0.9560 - val_loss: 0.1458 - val_accuracy: 0.9388\n",
      "Epoch 85/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.1034 - accuracy: 0.9560\n",
      "Epoch 85: val_loss improved from 0.14578 to 0.14534, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1034 - accuracy: 0.9560 - val_loss: 0.1453 - val_accuracy: 0.9392\n",
      "Epoch 86/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.1030 - accuracy: 0.9562\n",
      "Epoch 86: val_loss improved from 0.14534 to 0.14491, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1030 - accuracy: 0.9563 - val_loss: 0.1449 - val_accuracy: 0.9395\n",
      "Epoch 87/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.1028 - accuracy: 0.9563\n",
      "Epoch 87: val_loss improved from 0.14491 to 0.14451, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1026 - accuracy: 0.9565 - val_loss: 0.1445 - val_accuracy: 0.9395\n",
      "Epoch 88/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.1022 - accuracy: 0.9567\n",
      "Epoch 88: val_loss improved from 0.14451 to 0.14412, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1022 - accuracy: 0.9567 - val_loss: 0.1441 - val_accuracy: 0.9396\n",
      "Epoch 89/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9570\n",
      "Epoch 89: val_loss improved from 0.14412 to 0.14371, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1018 - accuracy: 0.9570 - val_loss: 0.1437 - val_accuracy: 0.9398\n",
      "Epoch 90/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.1015 - accuracy: 0.9571\n",
      "Epoch 90: val_loss improved from 0.14371 to 0.14334, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1014 - accuracy: 0.9571 - val_loss: 0.1433 - val_accuracy: 0.9403\n",
      "Epoch 91/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.1011 - accuracy: 0.9574\n",
      "Epoch 91: val_loss improved from 0.14334 to 0.14297, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1011 - accuracy: 0.9574 - val_loss: 0.1430 - val_accuracy: 0.9407\n",
      "Epoch 92/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9577\n",
      "Epoch 92: val_loss improved from 0.14297 to 0.14261, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1007 - accuracy: 0.9577 - val_loss: 0.1426 - val_accuracy: 0.9410\n",
      "Epoch 93/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.1004 - accuracy: 0.9579\n",
      "Epoch 93: val_loss improved from 0.14261 to 0.14225, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1003 - accuracy: 0.9579 - val_loss: 0.1423 - val_accuracy: 0.9411\n",
      "Epoch 94/1000\n",
      "984/995 [============================>.] - ETA: 0s - loss: 0.1001 - accuracy: 0.9581\n",
      "Epoch 94: val_loss improved from 0.14225 to 0.14191, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.1000 - accuracy: 0.9581 - val_loss: 0.1419 - val_accuracy: 0.9411\n",
      "Epoch 95/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0997 - accuracy: 0.9583\n",
      "Epoch 95: val_loss improved from 0.14191 to 0.14158, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0997 - accuracy: 0.9583 - val_loss: 0.1416 - val_accuracy: 0.9414\n",
      "Epoch 96/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0995 - accuracy: 0.9583\n",
      "Epoch 96: val_loss improved from 0.14158 to 0.14124, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0993 - accuracy: 0.9585 - val_loss: 0.1412 - val_accuracy: 0.9417\n",
      "Epoch 97/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0991 - accuracy: 0.9585\n",
      "Epoch 97: val_loss improved from 0.14124 to 0.14092, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0990 - accuracy: 0.9586 - val_loss: 0.1409 - val_accuracy: 0.9417\n",
      "Epoch 98/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9588\n",
      "Epoch 98: val_loss improved from 0.14092 to 0.14060, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0987 - accuracy: 0.9588 - val_loss: 0.1406 - val_accuracy: 0.9419\n",
      "Epoch 99/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9590\n",
      "Epoch 99: val_loss improved from 0.14060 to 0.14029, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0984 - accuracy: 0.9590 - val_loss: 0.1403 - val_accuracy: 0.9420\n",
      "Epoch 100/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0982 - accuracy: 0.9590\n",
      "Epoch 100: val_loss improved from 0.14029 to 0.13998, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0981 - accuracy: 0.9591 - val_loss: 0.1400 - val_accuracy: 0.9422\n",
      "Epoch 101/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0978 - accuracy: 0.9591\n",
      "Epoch 101: val_loss improved from 0.13998 to 0.13968, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0978 - accuracy: 0.9592 - val_loss: 0.1397 - val_accuracy: 0.9423\n",
      "Epoch 102/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0975 - accuracy: 0.9593\n",
      "Epoch 102: val_loss improved from 0.13968 to 0.13940, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0975 - accuracy: 0.9593 - val_loss: 0.1394 - val_accuracy: 0.9424\n",
      "Epoch 103/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0970 - accuracy: 0.9594\n",
      "Epoch 103: val_loss improved from 0.13940 to 0.13912, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0972 - accuracy: 0.9594 - val_loss: 0.1391 - val_accuracy: 0.9426\n",
      "Epoch 104/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0970 - accuracy: 0.9596\n",
      "Epoch 104: val_loss improved from 0.13912 to 0.13884, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0969 - accuracy: 0.9596 - val_loss: 0.1388 - val_accuracy: 0.9427\n",
      "Epoch 105/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0968 - accuracy: 0.9598\n",
      "Epoch 105: val_loss improved from 0.13884 to 0.13857, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0966 - accuracy: 0.9599 - val_loss: 0.1386 - val_accuracy: 0.9427\n",
      "Epoch 106/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0963 - accuracy: 0.9600\n",
      "Epoch 106: val_loss improved from 0.13857 to 0.13831, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0964 - accuracy: 0.9600 - val_loss: 0.1383 - val_accuracy: 0.9430\n",
      "Epoch 107/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9601\n",
      "Epoch 107: val_loss improved from 0.13831 to 0.13804, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0961 - accuracy: 0.9600 - val_loss: 0.1380 - val_accuracy: 0.9431\n",
      "Epoch 108/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0959 - accuracy: 0.9601\n",
      "Epoch 108: val_loss improved from 0.13804 to 0.13778, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0958 - accuracy: 0.9601 - val_loss: 0.1378 - val_accuracy: 0.9432\n",
      "Epoch 109/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.9603\n",
      "Epoch 109: val_loss improved from 0.13778 to 0.13755, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0956 - accuracy: 0.9603 - val_loss: 0.1375 - val_accuracy: 0.9432\n",
      "Epoch 110/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9603\n",
      "Epoch 110: val_loss improved from 0.13755 to 0.13731, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0954 - accuracy: 0.9604 - val_loss: 0.1373 - val_accuracy: 0.9432\n",
      "Epoch 111/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0951 - accuracy: 0.9606\n",
      "Epoch 111: val_loss improved from 0.13731 to 0.13707, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0951 - accuracy: 0.9606 - val_loss: 0.1371 - val_accuracy: 0.9434\n",
      "Epoch 112/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0949 - accuracy: 0.9606\n",
      "Epoch 112: val_loss improved from 0.13707 to 0.13683, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0949 - accuracy: 0.9606 - val_loss: 0.1368 - val_accuracy: 0.9434\n",
      "Epoch 113/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0946 - accuracy: 0.9608\n",
      "Epoch 113: val_loss improved from 0.13683 to 0.13661, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0946 - accuracy: 0.9608 - val_loss: 0.1366 - val_accuracy: 0.9436\n",
      "Epoch 114/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0943 - accuracy: 0.9609\n",
      "Epoch 114: val_loss improved from 0.13661 to 0.13639, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0944 - accuracy: 0.9608 - val_loss: 0.1364 - val_accuracy: 0.9438\n",
      "Epoch 115/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0943 - accuracy: 0.9609\n",
      "Epoch 115: val_loss improved from 0.13639 to 0.13616, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0942 - accuracy: 0.9609 - val_loss: 0.1362 - val_accuracy: 0.9439\n",
      "Epoch 116/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0941 - accuracy: 0.9610\n",
      "Epoch 116: val_loss improved from 0.13616 to 0.13595, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0940 - accuracy: 0.9610 - val_loss: 0.1359 - val_accuracy: 0.9441\n",
      "Epoch 117/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0936 - accuracy: 0.9611\n",
      "Epoch 117: val_loss improved from 0.13595 to 0.13573, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0938 - accuracy: 0.9611 - val_loss: 0.1357 - val_accuracy: 0.9440\n",
      "Epoch 118/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0936 - accuracy: 0.9613\n",
      "Epoch 118: val_loss improved from 0.13573 to 0.13554, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0935 - accuracy: 0.9613 - val_loss: 0.1355 - val_accuracy: 0.9441\n",
      "Epoch 119/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9615\n",
      "Epoch 119: val_loss improved from 0.13554 to 0.13533, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0933 - accuracy: 0.9614 - val_loss: 0.1353 - val_accuracy: 0.9441\n",
      "Epoch 120/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9616\n",
      "Epoch 120: val_loss improved from 0.13533 to 0.13513, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0931 - accuracy: 0.9616 - val_loss: 0.1351 - val_accuracy: 0.9442\n",
      "Epoch 121/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0927 - accuracy: 0.9618\n",
      "Epoch 121: val_loss improved from 0.13513 to 0.13494, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0929 - accuracy: 0.9617 - val_loss: 0.1349 - val_accuracy: 0.9444\n",
      "Epoch 122/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0928 - accuracy: 0.9618\n",
      "Epoch 122: val_loss improved from 0.13494 to 0.13475, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0927 - accuracy: 0.9618 - val_loss: 0.1348 - val_accuracy: 0.9444\n",
      "Epoch 123/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0925 - accuracy: 0.9619\n",
      "Epoch 123: val_loss improved from 0.13475 to 0.13456, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0925 - accuracy: 0.9620 - val_loss: 0.1346 - val_accuracy: 0.9445\n",
      "Epoch 124/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0924 - accuracy: 0.9621\n",
      "Epoch 124: val_loss improved from 0.13456 to 0.13438, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0923 - accuracy: 0.9620 - val_loss: 0.1344 - val_accuracy: 0.9449\n",
      "Epoch 125/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9621\n",
      "Epoch 125: val_loss improved from 0.13438 to 0.13420, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0922 - accuracy: 0.9621 - val_loss: 0.1342 - val_accuracy: 0.9450\n",
      "Epoch 126/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0920 - accuracy: 0.9621\n",
      "Epoch 126: val_loss improved from 0.13420 to 0.13402, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0920 - accuracy: 0.9621 - val_loss: 0.1340 - val_accuracy: 0.9454\n",
      "Epoch 127/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0920 - accuracy: 0.9621\n",
      "Epoch 127: val_loss improved from 0.13402 to 0.13386, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0918 - accuracy: 0.9622 - val_loss: 0.1339 - val_accuracy: 0.9454\n",
      "Epoch 128/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0916 - accuracy: 0.9623\n",
      "Epoch 128: val_loss improved from 0.13386 to 0.13369, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0916 - accuracy: 0.9623 - val_loss: 0.1337 - val_accuracy: 0.9454\n",
      "Epoch 129/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9623\n",
      "Epoch 129: val_loss improved from 0.13369 to 0.13352, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0914 - accuracy: 0.9623 - val_loss: 0.1335 - val_accuracy: 0.9454\n",
      "Epoch 130/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0913 - accuracy: 0.9624\n",
      "Epoch 130: val_loss improved from 0.13352 to 0.13337, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0913 - accuracy: 0.9624 - val_loss: 0.1334 - val_accuracy: 0.9453\n",
      "Epoch 131/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0912 - accuracy: 0.9624\n",
      "Epoch 131: val_loss improved from 0.13337 to 0.13321, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0911 - accuracy: 0.9625 - val_loss: 0.1332 - val_accuracy: 0.9454\n",
      "Epoch 132/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0910 - accuracy: 0.9627\n",
      "Epoch 132: val_loss improved from 0.13321 to 0.13304, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0909 - accuracy: 0.9627 - val_loss: 0.1330 - val_accuracy: 0.9455\n",
      "Epoch 133/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0908 - accuracy: 0.9629\n",
      "Epoch 133: val_loss improved from 0.13304 to 0.13287, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0908 - accuracy: 0.9629 - val_loss: 0.1329 - val_accuracy: 0.9456\n",
      "Epoch 134/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0907 - accuracy: 0.9629\n",
      "Epoch 134: val_loss improved from 0.13287 to 0.13272, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0906 - accuracy: 0.9629 - val_loss: 0.1327 - val_accuracy: 0.9456\n",
      "Epoch 135/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0905 - accuracy: 0.9630\n",
      "Epoch 135: val_loss improved from 0.13272 to 0.13256, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0904 - accuracy: 0.9630 - val_loss: 0.1326 - val_accuracy: 0.9456\n",
      "Epoch 136/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0903 - accuracy: 0.9630\n",
      "Epoch 136: val_loss improved from 0.13256 to 0.13241, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0903 - accuracy: 0.9631 - val_loss: 0.1324 - val_accuracy: 0.9459\n",
      "Epoch 137/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0902 - accuracy: 0.9632\n",
      "Epoch 137: val_loss improved from 0.13241 to 0.13228, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0901 - accuracy: 0.9632 - val_loss: 0.1323 - val_accuracy: 0.9459\n",
      "Epoch 138/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.9633\n",
      "Epoch 138: val_loss improved from 0.13228 to 0.13213, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0900 - accuracy: 0.9633 - val_loss: 0.1321 - val_accuracy: 0.9459\n",
      "Epoch 139/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0898 - accuracy: 0.9635\n",
      "Epoch 139: val_loss improved from 0.13213 to 0.13200, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0898 - accuracy: 0.9634 - val_loss: 0.1320 - val_accuracy: 0.9461\n",
      "Epoch 140/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0898 - accuracy: 0.9635\n",
      "Epoch 140: val_loss improved from 0.13200 to 0.13186, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0897 - accuracy: 0.9635 - val_loss: 0.1319 - val_accuracy: 0.9461\n",
      "Epoch 141/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0896 - accuracy: 0.9636\n",
      "Epoch 141: val_loss improved from 0.13186 to 0.13174, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0895 - accuracy: 0.9636 - val_loss: 0.1317 - val_accuracy: 0.9461\n",
      "Epoch 142/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0895 - accuracy: 0.9637\n",
      "Epoch 142: val_loss improved from 0.13174 to 0.13161, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0894 - accuracy: 0.9637 - val_loss: 0.1316 - val_accuracy: 0.9461\n",
      "Epoch 143/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0893 - accuracy: 0.9639\n",
      "Epoch 143: val_loss improved from 0.13161 to 0.13148, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0893 - accuracy: 0.9639 - val_loss: 0.1315 - val_accuracy: 0.9459\n",
      "Epoch 144/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0891 - accuracy: 0.9639\n",
      "Epoch 144: val_loss improved from 0.13148 to 0.13135, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0891 - accuracy: 0.9639 - val_loss: 0.1313 - val_accuracy: 0.9461\n",
      "Epoch 145/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9640\n",
      "Epoch 145: val_loss improved from 0.13135 to 0.13123, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0890 - accuracy: 0.9640 - val_loss: 0.1312 - val_accuracy: 0.9461\n",
      "Epoch 146/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0888 - accuracy: 0.9640\n",
      "Epoch 146: val_loss improved from 0.13123 to 0.13110, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0889 - accuracy: 0.9640 - val_loss: 0.1311 - val_accuracy: 0.9462\n",
      "Epoch 147/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0887 - accuracy: 0.9640\n",
      "Epoch 147: val_loss improved from 0.13110 to 0.13098, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0887 - accuracy: 0.9640 - val_loss: 0.1310 - val_accuracy: 0.9463\n",
      "Epoch 148/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.9641\n",
      "Epoch 148: val_loss improved from 0.13098 to 0.13087, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0886 - accuracy: 0.9641 - val_loss: 0.1309 - val_accuracy: 0.9464\n",
      "Epoch 149/1000\n",
      "983/995 [============================>.] - ETA: 0s - loss: 0.0885 - accuracy: 0.9643\n",
      "Epoch 149: val_loss improved from 0.13087 to 0.13075, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0885 - accuracy: 0.9642 - val_loss: 0.1307 - val_accuracy: 0.9465\n",
      "Epoch 150/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9643\n",
      "Epoch 150: val_loss improved from 0.13075 to 0.13062, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0884 - accuracy: 0.9644 - val_loss: 0.1306 - val_accuracy: 0.9466\n",
      "Epoch 151/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9645\n",
      "Epoch 151: val_loss improved from 0.13062 to 0.13049, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0882 - accuracy: 0.9645 - val_loss: 0.1305 - val_accuracy: 0.9466\n",
      "Epoch 152/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9645\n",
      "Epoch 152: val_loss improved from 0.13049 to 0.13039, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0881 - accuracy: 0.9645 - val_loss: 0.1304 - val_accuracy: 0.9466\n",
      "Epoch 153/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0880 - accuracy: 0.9645\n",
      "Epoch 153: val_loss improved from 0.13039 to 0.13028, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0880 - accuracy: 0.9645 - val_loss: 0.1303 - val_accuracy: 0.9466\n",
      "Epoch 154/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0880 - accuracy: 0.9645\n",
      "Epoch 154: val_loss improved from 0.13028 to 0.13017, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0879 - accuracy: 0.9645 - val_loss: 0.1302 - val_accuracy: 0.9466\n",
      "Epoch 155/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0879 - accuracy: 0.9646\n",
      "Epoch 155: val_loss improved from 0.13017 to 0.13005, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0877 - accuracy: 0.9646 - val_loss: 0.1301 - val_accuracy: 0.9466\n",
      "Epoch 156/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0876 - accuracy: 0.9646\n",
      "Epoch 156: val_loss improved from 0.13005 to 0.12994, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0876 - accuracy: 0.9646 - val_loss: 0.1299 - val_accuracy: 0.9466\n",
      "Epoch 157/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0876 - accuracy: 0.9646\n",
      "Epoch 157: val_loss improved from 0.12994 to 0.12984, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0875 - accuracy: 0.9647 - val_loss: 0.1298 - val_accuracy: 0.9468\n",
      "Epoch 158/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0874 - accuracy: 0.9647\n",
      "Epoch 158: val_loss improved from 0.12984 to 0.12973, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0874 - accuracy: 0.9647 - val_loss: 0.1297 - val_accuracy: 0.9468\n",
      "Epoch 159/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0873 - accuracy: 0.9647\n",
      "Epoch 159: val_loss improved from 0.12973 to 0.12963, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0873 - accuracy: 0.9647 - val_loss: 0.1296 - val_accuracy: 0.9470\n",
      "Epoch 160/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9648\n",
      "Epoch 160: val_loss improved from 0.12963 to 0.12951, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0872 - accuracy: 0.9648 - val_loss: 0.1295 - val_accuracy: 0.9471\n",
      "Epoch 161/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9648\n",
      "Epoch 161: val_loss improved from 0.12951 to 0.12940, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0871 - accuracy: 0.9648 - val_loss: 0.1294 - val_accuracy: 0.9472\n",
      "Epoch 162/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0869 - accuracy: 0.9648\n",
      "Epoch 162: val_loss improved from 0.12940 to 0.12929, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0869 - accuracy: 0.9648 - val_loss: 0.1293 - val_accuracy: 0.9472\n",
      "Epoch 163/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9648\n",
      "Epoch 163: val_loss improved from 0.12929 to 0.12918, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0868 - accuracy: 0.9648 - val_loss: 0.1292 - val_accuracy: 0.9473\n",
      "Epoch 164/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0868 - accuracy: 0.9648\n",
      "Epoch 164: val_loss improved from 0.12918 to 0.12908, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0867 - accuracy: 0.9648 - val_loss: 0.1291 - val_accuracy: 0.9474\n",
      "Epoch 165/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9649\n",
      "Epoch 165: val_loss improved from 0.12908 to 0.12899, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0866 - accuracy: 0.9649 - val_loss: 0.1290 - val_accuracy: 0.9476\n",
      "Epoch 166/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9649\n",
      "Epoch 166: val_loss improved from 0.12899 to 0.12890, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0865 - accuracy: 0.9649 - val_loss: 0.1289 - val_accuracy: 0.9476\n",
      "Epoch 167/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0863 - accuracy: 0.9650\n",
      "Epoch 167: val_loss improved from 0.12890 to 0.12881, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0864 - accuracy: 0.9649 - val_loss: 0.1288 - val_accuracy: 0.9477\n",
      "Epoch 168/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0864 - accuracy: 0.9649\n",
      "Epoch 168: val_loss improved from 0.12881 to 0.12872, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0863 - accuracy: 0.9650 - val_loss: 0.1287 - val_accuracy: 0.9476\n",
      "Epoch 169/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0862 - accuracy: 0.9650\n",
      "Epoch 169: val_loss improved from 0.12872 to 0.12864, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0862 - accuracy: 0.9650 - val_loss: 0.1286 - val_accuracy: 0.9476\n",
      "Epoch 170/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0861 - accuracy: 0.9650\n",
      "Epoch 170: val_loss improved from 0.12864 to 0.12856, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0861 - accuracy: 0.9650 - val_loss: 0.1286 - val_accuracy: 0.9478\n",
      "Epoch 171/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0861 - accuracy: 0.9650\n",
      "Epoch 171: val_loss improved from 0.12856 to 0.12847, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0860 - accuracy: 0.9650 - val_loss: 0.1285 - val_accuracy: 0.9478\n",
      "Epoch 172/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0860 - accuracy: 0.9650\n",
      "Epoch 172: val_loss improved from 0.12847 to 0.12838, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0859 - accuracy: 0.9650 - val_loss: 0.1284 - val_accuracy: 0.9480\n",
      "Epoch 173/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0858 - accuracy: 0.9651\n",
      "Epoch 173: val_loss improved from 0.12838 to 0.12829, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0858 - accuracy: 0.9651 - val_loss: 0.1283 - val_accuracy: 0.9481\n",
      "Epoch 174/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0857 - accuracy: 0.9652\n",
      "Epoch 174: val_loss improved from 0.12829 to 0.12819, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0857 - accuracy: 0.9651 - val_loss: 0.1282 - val_accuracy: 0.9481\n",
      "Epoch 175/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0856 - accuracy: 0.9652\n",
      "Epoch 175: val_loss improved from 0.12819 to 0.12812, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0856 - accuracy: 0.9652 - val_loss: 0.1281 - val_accuracy: 0.9483\n",
      "Epoch 176/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0855 - accuracy: 0.9651\n",
      "Epoch 176: val_loss improved from 0.12812 to 0.12803, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0855 - accuracy: 0.9651 - val_loss: 0.1280 - val_accuracy: 0.9483\n",
      "Epoch 177/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0855 - accuracy: 0.9652\n",
      "Epoch 177: val_loss improved from 0.12803 to 0.12794, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0854 - accuracy: 0.9652 - val_loss: 0.1279 - val_accuracy: 0.9484\n",
      "Epoch 178/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0855 - accuracy: 0.9652\n",
      "Epoch 178: val_loss improved from 0.12794 to 0.12786, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0853 - accuracy: 0.9652 - val_loss: 0.1279 - val_accuracy: 0.9485\n",
      "Epoch 179/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0853 - accuracy: 0.9652\n",
      "Epoch 179: val_loss improved from 0.12786 to 0.12777, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0853 - accuracy: 0.9652 - val_loss: 0.1278 - val_accuracy: 0.9485\n",
      "Epoch 180/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0852 - accuracy: 0.9652\n",
      "Epoch 180: val_loss improved from 0.12777 to 0.12769, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0852 - accuracy: 0.9653 - val_loss: 0.1277 - val_accuracy: 0.9486\n",
      "Epoch 181/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0851 - accuracy: 0.9653\n",
      "Epoch 181: val_loss improved from 0.12769 to 0.12761, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0851 - accuracy: 0.9653 - val_loss: 0.1276 - val_accuracy: 0.9486\n",
      "Epoch 182/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9654\n",
      "Epoch 182: val_loss improved from 0.12761 to 0.12753, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0850 - accuracy: 0.9653 - val_loss: 0.1275 - val_accuracy: 0.9486\n",
      "Epoch 183/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9654\n",
      "Epoch 183: val_loss improved from 0.12753 to 0.12746, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0849 - accuracy: 0.9654 - val_loss: 0.1275 - val_accuracy: 0.9488\n",
      "Epoch 184/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0848 - accuracy: 0.9654\n",
      "Epoch 184: val_loss improved from 0.12746 to 0.12738, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0848 - accuracy: 0.9654 - val_loss: 0.1274 - val_accuracy: 0.9488\n",
      "Epoch 185/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9654\n",
      "Epoch 185: val_loss improved from 0.12738 to 0.12730, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0847 - accuracy: 0.9654 - val_loss: 0.1273 - val_accuracy: 0.9489\n",
      "Epoch 186/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0847 - accuracy: 0.9654\n",
      "Epoch 186: val_loss improved from 0.12730 to 0.12721, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0846 - accuracy: 0.9655 - val_loss: 0.1272 - val_accuracy: 0.9490\n",
      "Epoch 187/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.9655\n",
      "Epoch 187: val_loss improved from 0.12721 to 0.12714, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0845 - accuracy: 0.9655 - val_loss: 0.1271 - val_accuracy: 0.9490\n",
      "Epoch 188/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0846 - accuracy: 0.9654\n",
      "Epoch 188: val_loss improved from 0.12714 to 0.12705, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0845 - accuracy: 0.9655 - val_loss: 0.1271 - val_accuracy: 0.9492\n",
      "Epoch 189/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.9656\n",
      "Epoch 189: val_loss improved from 0.12705 to 0.12697, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0844 - accuracy: 0.9656 - val_loss: 0.1270 - val_accuracy: 0.9492\n",
      "Epoch 190/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0843 - accuracy: 0.9657\n",
      "Epoch 190: val_loss improved from 0.12697 to 0.12690, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0843 - accuracy: 0.9657 - val_loss: 0.1269 - val_accuracy: 0.9494\n",
      "Epoch 191/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0841 - accuracy: 0.9657\n",
      "Epoch 191: val_loss improved from 0.12690 to 0.12683, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0842 - accuracy: 0.9656 - val_loss: 0.1268 - val_accuracy: 0.9494\n",
      "Epoch 192/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0841 - accuracy: 0.9658\n",
      "Epoch 192: val_loss improved from 0.12683 to 0.12675, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0841 - accuracy: 0.9658 - val_loss: 0.1267 - val_accuracy: 0.9494\n",
      "Epoch 193/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0840 - accuracy: 0.9659\n",
      "Epoch 193: val_loss improved from 0.12675 to 0.12668, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0840 - accuracy: 0.9658 - val_loss: 0.1267 - val_accuracy: 0.9495\n",
      "Epoch 194/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0840 - accuracy: 0.9658\n",
      "Epoch 194: val_loss improved from 0.12668 to 0.12661, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0840 - accuracy: 0.9659 - val_loss: 0.1266 - val_accuracy: 0.9495\n",
      "Epoch 195/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.9659\n",
      "Epoch 195: val_loss improved from 0.12661 to 0.12655, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0839 - accuracy: 0.9659 - val_loss: 0.1265 - val_accuracy: 0.9495\n",
      "Epoch 196/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.9659\n",
      "Epoch 196: val_loss improved from 0.12655 to 0.12648, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0838 - accuracy: 0.9660 - val_loss: 0.1265 - val_accuracy: 0.9495\n",
      "Epoch 197/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.9659\n",
      "Epoch 197: val_loss improved from 0.12648 to 0.12641, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0837 - accuracy: 0.9660 - val_loss: 0.1264 - val_accuracy: 0.9495\n",
      "Epoch 198/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0837 - accuracy: 0.9661\n",
      "Epoch 198: val_loss improved from 0.12641 to 0.12634, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0837 - accuracy: 0.9661 - val_loss: 0.1263 - val_accuracy: 0.9495\n",
      "Epoch 199/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0837 - accuracy: 0.9660\n",
      "Epoch 199: val_loss improved from 0.12634 to 0.12628, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0836 - accuracy: 0.9661 - val_loss: 0.1263 - val_accuracy: 0.9496\n",
      "Epoch 200/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9661\n",
      "Epoch 200: val_loss improved from 0.12628 to 0.12622, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0835 - accuracy: 0.9661 - val_loss: 0.1262 - val_accuracy: 0.9496\n",
      "Epoch 201/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0835 - accuracy: 0.9662\n",
      "Epoch 201: val_loss improved from 0.12622 to 0.12615, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0834 - accuracy: 0.9662 - val_loss: 0.1262 - val_accuracy: 0.9495\n",
      "Epoch 202/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9662\n",
      "Epoch 202: val_loss improved from 0.12615 to 0.12609, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0834 - accuracy: 0.9662 - val_loss: 0.1261 - val_accuracy: 0.9496\n",
      "Epoch 203/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0833 - accuracy: 0.9662\n",
      "Epoch 203: val_loss improved from 0.12609 to 0.12601, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0833 - accuracy: 0.9662 - val_loss: 0.1260 - val_accuracy: 0.9496\n",
      "Epoch 204/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0832 - accuracy: 0.9662\n",
      "Epoch 204: val_loss improved from 0.12601 to 0.12596, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0832 - accuracy: 0.9663 - val_loss: 0.1260 - val_accuracy: 0.9497\n",
      "Epoch 205/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0831 - accuracy: 0.9663\n",
      "Epoch 205: val_loss improved from 0.12596 to 0.12589, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0831 - accuracy: 0.9663 - val_loss: 0.1259 - val_accuracy: 0.9497\n",
      "Epoch 206/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0832 - accuracy: 0.9663\n",
      "Epoch 206: val_loss improved from 0.12589 to 0.12583, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0831 - accuracy: 0.9664 - val_loss: 0.1258 - val_accuracy: 0.9497\n",
      "Epoch 207/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0830 - accuracy: 0.9664\n",
      "Epoch 207: val_loss improved from 0.12583 to 0.12577, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0830 - accuracy: 0.9664 - val_loss: 0.1258 - val_accuracy: 0.9498\n",
      "Epoch 208/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0829 - accuracy: 0.9665\n",
      "Epoch 208: val_loss improved from 0.12577 to 0.12571, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0829 - accuracy: 0.9664 - val_loss: 0.1257 - val_accuracy: 0.9498\n",
      "Epoch 209/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0830 - accuracy: 0.9664\n",
      "Epoch 209: val_loss improved from 0.12571 to 0.12565, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0829 - accuracy: 0.9664 - val_loss: 0.1257 - val_accuracy: 0.9498\n",
      "Epoch 210/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9665\n",
      "Epoch 210: val_loss improved from 0.12565 to 0.12559, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0828 - accuracy: 0.9665 - val_loss: 0.1256 - val_accuracy: 0.9497\n",
      "Epoch 211/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9665\n",
      "Epoch 211: val_loss improved from 0.12559 to 0.12553, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0827 - accuracy: 0.9665 - val_loss: 0.1255 - val_accuracy: 0.9497\n",
      "Epoch 212/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9665\n",
      "Epoch 212: val_loss improved from 0.12553 to 0.12548, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0827 - accuracy: 0.9665 - val_loss: 0.1255 - val_accuracy: 0.9497\n",
      "Epoch 213/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9665\n",
      "Epoch 213: val_loss improved from 0.12548 to 0.12542, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0826 - accuracy: 0.9665 - val_loss: 0.1254 - val_accuracy: 0.9497\n",
      "Epoch 214/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9664\n",
      "Epoch 214: val_loss improved from 0.12542 to 0.12536, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0825 - accuracy: 0.9665 - val_loss: 0.1254 - val_accuracy: 0.9498\n",
      "Epoch 215/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0824 - accuracy: 0.9666\n",
      "Epoch 215: val_loss improved from 0.12536 to 0.12530, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0825 - accuracy: 0.9665 - val_loss: 0.1253 - val_accuracy: 0.9498\n",
      "Epoch 216/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0824 - accuracy: 0.9665\n",
      "Epoch 216: val_loss improved from 0.12530 to 0.12524, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0824 - accuracy: 0.9665 - val_loss: 0.1252 - val_accuracy: 0.9498\n",
      "Epoch 217/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0824 - accuracy: 0.9666\n",
      "Epoch 217: val_loss improved from 0.12524 to 0.12519, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0824 - accuracy: 0.9666 - val_loss: 0.1252 - val_accuracy: 0.9499\n",
      "Epoch 218/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9666\n",
      "Epoch 218: val_loss improved from 0.12519 to 0.12513, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0823 - accuracy: 0.9667 - val_loss: 0.1251 - val_accuracy: 0.9500\n",
      "Epoch 219/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9667\n",
      "Epoch 219: val_loss improved from 0.12513 to 0.12508, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0822 - accuracy: 0.9667 - val_loss: 0.1251 - val_accuracy: 0.9499\n",
      "Epoch 220/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0822 - accuracy: 0.9667\n",
      "Epoch 220: val_loss improved from 0.12508 to 0.12503, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0822 - accuracy: 0.9667 - val_loss: 0.1250 - val_accuracy: 0.9499\n",
      "Epoch 221/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0822 - accuracy: 0.9667\n",
      "Epoch 221: val_loss improved from 0.12503 to 0.12498, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0821 - accuracy: 0.9667 - val_loss: 0.1250 - val_accuracy: 0.9499\n",
      "Epoch 222/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0820 - accuracy: 0.9669\n",
      "Epoch 222: val_loss improved from 0.12498 to 0.12494, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0821 - accuracy: 0.9668 - val_loss: 0.1249 - val_accuracy: 0.9498\n",
      "Epoch 223/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0821 - accuracy: 0.9668\n",
      "Epoch 223: val_loss improved from 0.12494 to 0.12488, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0820 - accuracy: 0.9668 - val_loss: 0.1249 - val_accuracy: 0.9499\n",
      "Epoch 224/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0820 - accuracy: 0.9668\n",
      "Epoch 224: val_loss improved from 0.12488 to 0.12483, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0819 - accuracy: 0.9668 - val_loss: 0.1248 - val_accuracy: 0.9498\n",
      "Epoch 225/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0819 - accuracy: 0.9668\n",
      "Epoch 225: val_loss improved from 0.12483 to 0.12479, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0819 - accuracy: 0.9668 - val_loss: 0.1248 - val_accuracy: 0.9498\n",
      "Epoch 226/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0818 - accuracy: 0.9668\n",
      "Epoch 226: val_loss improved from 0.12479 to 0.12474, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0818 - accuracy: 0.9668 - val_loss: 0.1247 - val_accuracy: 0.9499\n",
      "Epoch 227/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0817 - accuracy: 0.9668\n",
      "Epoch 227: val_loss improved from 0.12474 to 0.12469, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0818 - accuracy: 0.9668 - val_loss: 0.1247 - val_accuracy: 0.9499\n",
      "Epoch 228/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0816 - accuracy: 0.9669\n",
      "Epoch 228: val_loss improved from 0.12469 to 0.12464, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0817 - accuracy: 0.9668 - val_loss: 0.1246 - val_accuracy: 0.9499\n",
      "Epoch 229/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0818 - accuracy: 0.9667\n",
      "Epoch 229: val_loss improved from 0.12464 to 0.12460, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0817 - accuracy: 0.9668 - val_loss: 0.1246 - val_accuracy: 0.9499\n",
      "Epoch 230/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0815 - accuracy: 0.9668\n",
      "Epoch 230: val_loss improved from 0.12460 to 0.12455, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0816 - accuracy: 0.9668 - val_loss: 0.1246 - val_accuracy: 0.9498\n",
      "Epoch 231/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0815 - accuracy: 0.9668\n",
      "Epoch 231: val_loss improved from 0.12455 to 0.12449, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0815 - accuracy: 0.9668 - val_loss: 0.1245 - val_accuracy: 0.9498\n",
      "Epoch 232/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0815 - accuracy: 0.9669\n",
      "Epoch 232: val_loss improved from 0.12449 to 0.12445, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0815 - accuracy: 0.9669 - val_loss: 0.1244 - val_accuracy: 0.9498\n",
      "Epoch 233/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0813 - accuracy: 0.9669\n",
      "Epoch 233: val_loss improved from 0.12445 to 0.12440, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0814 - accuracy: 0.9669 - val_loss: 0.1244 - val_accuracy: 0.9497\n",
      "Epoch 234/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9670\n",
      "Epoch 234: val_loss improved from 0.12440 to 0.12436, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0814 - accuracy: 0.9670 - val_loss: 0.1244 - val_accuracy: 0.9498\n",
      "Epoch 235/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9670\n",
      "Epoch 235: val_loss improved from 0.12436 to 0.12433, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0813 - accuracy: 0.9669 - val_loss: 0.1243 - val_accuracy: 0.9498\n",
      "Epoch 236/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9670\n",
      "Epoch 236: val_loss improved from 0.12433 to 0.12429, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0813 - accuracy: 0.9669 - val_loss: 0.1243 - val_accuracy: 0.9498\n",
      "Epoch 237/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9670\n",
      "Epoch 237: val_loss improved from 0.12429 to 0.12425, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0812 - accuracy: 0.9670 - val_loss: 0.1242 - val_accuracy: 0.9497\n",
      "Epoch 238/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9670\n",
      "Epoch 238: val_loss improved from 0.12425 to 0.12421, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0812 - accuracy: 0.9670 - val_loss: 0.1242 - val_accuracy: 0.9498\n",
      "Epoch 239/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0811 - accuracy: 0.9670\n",
      "Epoch 239: val_loss improved from 0.12421 to 0.12417, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0811 - accuracy: 0.9670 - val_loss: 0.1242 - val_accuracy: 0.9498\n",
      "Epoch 240/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9671\n",
      "Epoch 240: val_loss improved from 0.12417 to 0.12413, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0811 - accuracy: 0.9670 - val_loss: 0.1241 - val_accuracy: 0.9498\n",
      "Epoch 241/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9670\n",
      "Epoch 241: val_loss improved from 0.12413 to 0.12409, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0810 - accuracy: 0.9670 - val_loss: 0.1241 - val_accuracy: 0.9499\n",
      "Epoch 242/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9670\n",
      "Epoch 242: val_loss improved from 0.12409 to 0.12405, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0810 - accuracy: 0.9670 - val_loss: 0.1241 - val_accuracy: 0.9499\n",
      "Epoch 243/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9670\n",
      "Epoch 243: val_loss improved from 0.12405 to 0.12402, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0809 - accuracy: 0.9670 - val_loss: 0.1240 - val_accuracy: 0.9500\n",
      "Epoch 244/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9671\n",
      "Epoch 244: val_loss improved from 0.12402 to 0.12398, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0809 - accuracy: 0.9670 - val_loss: 0.1240 - val_accuracy: 0.9500\n",
      "Epoch 245/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9671\n",
      "Epoch 245: val_loss improved from 0.12398 to 0.12392, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0808 - accuracy: 0.9672 - val_loss: 0.1239 - val_accuracy: 0.9500\n",
      "Epoch 246/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9672\n",
      "Epoch 246: val_loss improved from 0.12392 to 0.12387, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 5s 5ms/step - loss: 0.0808 - accuracy: 0.9672 - val_loss: 0.1239 - val_accuracy: 0.9501\n",
      "Epoch 247/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9672\n",
      "Epoch 247: val_loss improved from 0.12387 to 0.12383, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 5s 5ms/step - loss: 0.0807 - accuracy: 0.9672 - val_loss: 0.1238 - val_accuracy: 0.9501\n",
      "Epoch 248/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9673\n",
      "Epoch 248: val_loss improved from 0.12383 to 0.12379, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 5s 5ms/step - loss: 0.0807 - accuracy: 0.9673 - val_loss: 0.1238 - val_accuracy: 0.9501\n",
      "Epoch 249/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0806 - accuracy: 0.9673\n",
      "Epoch 249: val_loss improved from 0.12379 to 0.12376, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 5s 5ms/step - loss: 0.0806 - accuracy: 0.9673 - val_loss: 0.1238 - val_accuracy: 0.9500\n",
      "Epoch 250/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0805 - accuracy: 0.9673\n",
      "Epoch 250: val_loss improved from 0.12376 to 0.12373, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0806 - accuracy: 0.9673 - val_loss: 0.1237 - val_accuracy: 0.9500\n",
      "Epoch 251/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9673\n",
      "Epoch 251: val_loss improved from 0.12373 to 0.12369, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0805 - accuracy: 0.9673 - val_loss: 0.1237 - val_accuracy: 0.9501\n",
      "Epoch 252/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0805 - accuracy: 0.9673\n",
      "Epoch 252: val_loss improved from 0.12369 to 0.12366, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0805 - accuracy: 0.9673 - val_loss: 0.1237 - val_accuracy: 0.9501\n",
      "Epoch 253/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0805 - accuracy: 0.9673\n",
      "Epoch 253: val_loss improved from 0.12366 to 0.12362, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0805 - accuracy: 0.9673 - val_loss: 0.1236 - val_accuracy: 0.9500\n",
      "Epoch 254/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0803 - accuracy: 0.9673\n",
      "Epoch 254: val_loss improved from 0.12362 to 0.12358, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0804 - accuracy: 0.9673 - val_loss: 0.1236 - val_accuracy: 0.9500\n",
      "Epoch 255/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0804 - accuracy: 0.9673\n",
      "Epoch 255: val_loss improved from 0.12358 to 0.12354, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0804 - accuracy: 0.9674 - val_loss: 0.1235 - val_accuracy: 0.9502\n",
      "Epoch 256/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9675\n",
      "Epoch 256: val_loss improved from 0.12354 to 0.12349, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0803 - accuracy: 0.9674 - val_loss: 0.1235 - val_accuracy: 0.9502\n",
      "Epoch 257/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0802 - accuracy: 0.9675\n",
      "Epoch 257: val_loss improved from 0.12349 to 0.12346, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0803 - accuracy: 0.9675 - val_loss: 0.1235 - val_accuracy: 0.9502\n",
      "Epoch 258/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0803 - accuracy: 0.9675\n",
      "Epoch 258: val_loss improved from 0.12346 to 0.12342, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0802 - accuracy: 0.9676 - val_loss: 0.1234 - val_accuracy: 0.9502\n",
      "Epoch 259/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0803 - accuracy: 0.9675\n",
      "Epoch 259: val_loss improved from 0.12342 to 0.12339, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0802 - accuracy: 0.9676 - val_loss: 0.1234 - val_accuracy: 0.9502\n",
      "Epoch 260/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0802 - accuracy: 0.9676\n",
      "Epoch 260: val_loss improved from 0.12339 to 0.12335, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0801 - accuracy: 0.9676 - val_loss: 0.1233 - val_accuracy: 0.9502\n",
      "Epoch 261/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0802 - accuracy: 0.9676\n",
      "Epoch 261: val_loss improved from 0.12335 to 0.12331, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0801 - accuracy: 0.9676 - val_loss: 0.1233 - val_accuracy: 0.9502\n",
      "Epoch 262/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0797 - accuracy: 0.9677\n",
      "Epoch 262: val_loss improved from 0.12331 to 0.12328, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0801 - accuracy: 0.9676 - val_loss: 0.1233 - val_accuracy: 0.9503\n",
      "Epoch 263/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9677\n",
      "Epoch 263: val_loss improved from 0.12328 to 0.12325, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0800 - accuracy: 0.9677 - val_loss: 0.1232 - val_accuracy: 0.9503\n",
      "Epoch 264/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9677\n",
      "Epoch 264: val_loss improved from 0.12325 to 0.12322, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0800 - accuracy: 0.9676 - val_loss: 0.1232 - val_accuracy: 0.9504\n",
      "Epoch 265/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9677\n",
      "Epoch 265: val_loss improved from 0.12322 to 0.12318, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0799 - accuracy: 0.9676 - val_loss: 0.1232 - val_accuracy: 0.9504\n",
      "Epoch 266/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9678\n",
      "Epoch 266: val_loss improved from 0.12318 to 0.12314, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0799 - accuracy: 0.9678 - val_loss: 0.1231 - val_accuracy: 0.9504\n",
      "Epoch 267/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9678\n",
      "Epoch 267: val_loss improved from 0.12314 to 0.12312, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0799 - accuracy: 0.9678 - val_loss: 0.1231 - val_accuracy: 0.9504\n",
      "Epoch 268/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9678\n",
      "Epoch 268: val_loss improved from 0.12312 to 0.12308, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0798 - accuracy: 0.9678 - val_loss: 0.1231 - val_accuracy: 0.9504\n",
      "Epoch 269/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0797 - accuracy: 0.9678\n",
      "Epoch 269: val_loss improved from 0.12308 to 0.12305, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0798 - accuracy: 0.9678 - val_loss: 0.1230 - val_accuracy: 0.9503\n",
      "Epoch 270/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9678\n",
      "Epoch 270: val_loss improved from 0.12305 to 0.12302, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0797 - accuracy: 0.9678 - val_loss: 0.1230 - val_accuracy: 0.9503\n",
      "Epoch 271/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0796 - accuracy: 0.9679\n",
      "Epoch 271: val_loss improved from 0.12302 to 0.12300, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0797 - accuracy: 0.9679 - val_loss: 0.1230 - val_accuracy: 0.9504\n",
      "Epoch 272/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9679\n",
      "Epoch 272: val_loss improved from 0.12300 to 0.12296, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0797 - accuracy: 0.9679 - val_loss: 0.1230 - val_accuracy: 0.9505\n",
      "Epoch 273/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9679\n",
      "Epoch 273: val_loss improved from 0.12296 to 0.12292, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0796 - accuracy: 0.9679 - val_loss: 0.1229 - val_accuracy: 0.9505\n",
      "Epoch 274/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9680\n",
      "Epoch 274: val_loss improved from 0.12292 to 0.12288, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0796 - accuracy: 0.9679 - val_loss: 0.1229 - val_accuracy: 0.9505\n",
      "Epoch 275/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0797 - accuracy: 0.9679\n",
      "Epoch 275: val_loss improved from 0.12288 to 0.12285, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0795 - accuracy: 0.9679 - val_loss: 0.1228 - val_accuracy: 0.9505\n",
      "Epoch 276/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9680\n",
      "Epoch 276: val_loss improved from 0.12285 to 0.12282, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 7ms/step - loss: 0.0795 - accuracy: 0.9680 - val_loss: 0.1228 - val_accuracy: 0.9505\n",
      "Epoch 277/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9680\n",
      "Epoch 277: val_loss improved from 0.12282 to 0.12278, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0795 - accuracy: 0.9680 - val_loss: 0.1228 - val_accuracy: 0.9504\n",
      "Epoch 278/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9679\n",
      "Epoch 278: val_loss improved from 0.12278 to 0.12275, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0794 - accuracy: 0.9680 - val_loss: 0.1227 - val_accuracy: 0.9504\n",
      "Epoch 279/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9680\n",
      "Epoch 279: val_loss improved from 0.12275 to 0.12273, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0794 - accuracy: 0.9680 - val_loss: 0.1227 - val_accuracy: 0.9504\n",
      "Epoch 280/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0794 - accuracy: 0.9679\n",
      "Epoch 280: val_loss improved from 0.12273 to 0.12270, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0794 - accuracy: 0.9680 - val_loss: 0.1227 - val_accuracy: 0.9505\n",
      "Epoch 281/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0794 - accuracy: 0.9680\n",
      "Epoch 281: val_loss improved from 0.12270 to 0.12267, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0793 - accuracy: 0.9680 - val_loss: 0.1227 - val_accuracy: 0.9505\n",
      "Epoch 282/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0792 - accuracy: 0.9681\n",
      "Epoch 282: val_loss improved from 0.12267 to 0.12265, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0793 - accuracy: 0.9681 - val_loss: 0.1226 - val_accuracy: 0.9505\n",
      "Epoch 283/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0793 - accuracy: 0.9681\n",
      "Epoch 283: val_loss improved from 0.12265 to 0.12262, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0793 - accuracy: 0.9681 - val_loss: 0.1226 - val_accuracy: 0.9505\n",
      "Epoch 284/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 0.9681\n",
      "Epoch 284: val_loss improved from 0.12262 to 0.12259, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0792 - accuracy: 0.9681 - val_loss: 0.1226 - val_accuracy: 0.9506\n",
      "Epoch 285/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0790 - accuracy: 0.9682\n",
      "Epoch 285: val_loss improved from 0.12259 to 0.12256, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0792 - accuracy: 0.9681 - val_loss: 0.1226 - val_accuracy: 0.9507\n",
      "Epoch 286/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0789 - accuracy: 0.9682\n",
      "Epoch 286: val_loss improved from 0.12256 to 0.12252, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0792 - accuracy: 0.9682 - val_loss: 0.1225 - val_accuracy: 0.9507\n",
      "Epoch 287/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0789 - accuracy: 0.9683\n",
      "Epoch 287: val_loss improved from 0.12252 to 0.12249, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0791 - accuracy: 0.9682 - val_loss: 0.1225 - val_accuracy: 0.9507\n",
      "Epoch 288/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0792 - accuracy: 0.9682\n",
      "Epoch 288: val_loss improved from 0.12249 to 0.12246, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0791 - accuracy: 0.9682 - val_loss: 0.1225 - val_accuracy: 0.9506\n",
      "Epoch 289/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0789 - accuracy: 0.9684\n",
      "Epoch 289: val_loss improved from 0.12246 to 0.12243, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0790 - accuracy: 0.9683 - val_loss: 0.1224 - val_accuracy: 0.9507\n",
      "Epoch 290/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 0.9682\n",
      "Epoch 290: val_loss improved from 0.12243 to 0.12240, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0790 - accuracy: 0.9683 - val_loss: 0.1224 - val_accuracy: 0.9506\n",
      "Epoch 291/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0790 - accuracy: 0.9682\n",
      "Epoch 291: val_loss improved from 0.12240 to 0.12238, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0790 - accuracy: 0.9683 - val_loss: 0.1224 - val_accuracy: 0.9507\n",
      "Epoch 292/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0790 - accuracy: 0.9683\n",
      "Epoch 292: val_loss improved from 0.12238 to 0.12236, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0789 - accuracy: 0.9683 - val_loss: 0.1224 - val_accuracy: 0.9507\n",
      "Epoch 293/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0789 - accuracy: 0.9683\n",
      "Epoch 293: val_loss improved from 0.12236 to 0.12233, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0789 - accuracy: 0.9683 - val_loss: 0.1223 - val_accuracy: 0.9507\n",
      "Epoch 294/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0790 - accuracy: 0.9683\n",
      "Epoch 294: val_loss improved from 0.12233 to 0.12230, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0789 - accuracy: 0.9683 - val_loss: 0.1223 - val_accuracy: 0.9507\n",
      "Epoch 295/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9684\n",
      "Epoch 295: val_loss improved from 0.12230 to 0.12228, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0788 - accuracy: 0.9683 - val_loss: 0.1223 - val_accuracy: 0.9507\n",
      "Epoch 296/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0789 - accuracy: 0.9684\n",
      "Epoch 296: val_loss improved from 0.12228 to 0.12225, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0788 - accuracy: 0.9684 - val_loss: 0.1223 - val_accuracy: 0.9507\n",
      "Epoch 297/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0788 - accuracy: 0.9685\n",
      "Epoch 297: val_loss improved from 0.12225 to 0.12222, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0788 - accuracy: 0.9684 - val_loss: 0.1222 - val_accuracy: 0.9507\n",
      "Epoch 298/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9684\n",
      "Epoch 298: val_loss improved from 0.12222 to 0.12220, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0788 - accuracy: 0.9684 - val_loss: 0.1222 - val_accuracy: 0.9507\n",
      "Epoch 299/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9685\n",
      "Epoch 299: val_loss improved from 0.12220 to 0.12217, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 7ms/step - loss: 0.0787 - accuracy: 0.9684 - val_loss: 0.1222 - val_accuracy: 0.9507\n",
      "Epoch 300/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9684\n",
      "Epoch 300: val_loss improved from 0.12217 to 0.12214, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 7ms/step - loss: 0.0787 - accuracy: 0.9684 - val_loss: 0.1221 - val_accuracy: 0.9507\n",
      "Epoch 301/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9684\n",
      "Epoch 301: val_loss improved from 0.12214 to 0.12212, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0787 - accuracy: 0.9683 - val_loss: 0.1221 - val_accuracy: 0.9507\n",
      "Epoch 302/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0784 - accuracy: 0.9684\n",
      "Epoch 302: val_loss improved from 0.12212 to 0.12209, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0786 - accuracy: 0.9684 - val_loss: 0.1221 - val_accuracy: 0.9507\n",
      "Epoch 303/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9684\n",
      "Epoch 303: val_loss improved from 0.12209 to 0.12206, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0786 - accuracy: 0.9684 - val_loss: 0.1221 - val_accuracy: 0.9507\n",
      "Epoch 304/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9684\n",
      "Epoch 304: val_loss improved from 0.12206 to 0.12203, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0786 - accuracy: 0.9684 - val_loss: 0.1220 - val_accuracy: 0.9506\n",
      "Epoch 305/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9684\n",
      "Epoch 305: val_loss improved from 0.12203 to 0.12201, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0785 - accuracy: 0.9684 - val_loss: 0.1220 - val_accuracy: 0.9506\n",
      "Epoch 306/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9684\n",
      "Epoch 306: val_loss improved from 0.12201 to 0.12198, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0785 - accuracy: 0.9684 - val_loss: 0.1220 - val_accuracy: 0.9507\n",
      "Epoch 307/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9684\n",
      "Epoch 307: val_loss improved from 0.12198 to 0.12195, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0785 - accuracy: 0.9685 - val_loss: 0.1219 - val_accuracy: 0.9507\n",
      "Epoch 308/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0784 - accuracy: 0.9685\n",
      "Epoch 308: val_loss improved from 0.12195 to 0.12193, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0784 - accuracy: 0.9685 - val_loss: 0.1219 - val_accuracy: 0.9507\n",
      "Epoch 309/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9685\n",
      "Epoch 309: val_loss improved from 0.12193 to 0.12190, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0784 - accuracy: 0.9685 - val_loss: 0.1219 - val_accuracy: 0.9507\n",
      "Epoch 310/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9686\n",
      "Epoch 310: val_loss improved from 0.12190 to 0.12188, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0784 - accuracy: 0.9685 - val_loss: 0.1219 - val_accuracy: 0.9507\n",
      "Epoch 311/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0784 - accuracy: 0.9685\n",
      "Epoch 311: val_loss improved from 0.12188 to 0.12185, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0784 - accuracy: 0.9685 - val_loss: 0.1218 - val_accuracy: 0.9507\n",
      "Epoch 312/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9685\n",
      "Epoch 312: val_loss improved from 0.12185 to 0.12182, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0783 - accuracy: 0.9686 - val_loss: 0.1218 - val_accuracy: 0.9507\n",
      "Epoch 313/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9686\n",
      "Epoch 313: val_loss improved from 0.12182 to 0.12179, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0783 - accuracy: 0.9686 - val_loss: 0.1218 - val_accuracy: 0.9508\n",
      "Epoch 314/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9686\n",
      "Epoch 314: val_loss improved from 0.12179 to 0.12176, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0783 - accuracy: 0.9686 - val_loss: 0.1218 - val_accuracy: 0.9508\n",
      "Epoch 315/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9685\n",
      "Epoch 315: val_loss improved from 0.12176 to 0.12174, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0782 - accuracy: 0.9685 - val_loss: 0.1217 - val_accuracy: 0.9509\n",
      "Epoch 316/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9686\n",
      "Epoch 316: val_loss improved from 0.12174 to 0.12171, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0782 - accuracy: 0.9686 - val_loss: 0.1217 - val_accuracy: 0.9509\n",
      "Epoch 317/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9685\n",
      "Epoch 317: val_loss improved from 0.12171 to 0.12169, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0782 - accuracy: 0.9685 - val_loss: 0.1217 - val_accuracy: 0.9508\n",
      "Epoch 318/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0782 - accuracy: 0.9686\n",
      "Epoch 318: val_loss improved from 0.12169 to 0.12168, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0782 - accuracy: 0.9686 - val_loss: 0.1217 - val_accuracy: 0.9508\n",
      "Epoch 319/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0782 - accuracy: 0.9685\n",
      "Epoch 319: val_loss improved from 0.12168 to 0.12166, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0781 - accuracy: 0.9686 - val_loss: 0.1217 - val_accuracy: 0.9508\n",
      "Epoch 320/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9686\n",
      "Epoch 320: val_loss improved from 0.12166 to 0.12164, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0781 - accuracy: 0.9686 - val_loss: 0.1216 - val_accuracy: 0.9509\n",
      "Epoch 321/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9685\n",
      "Epoch 321: val_loss improved from 0.12164 to 0.12161, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0781 - accuracy: 0.9686 - val_loss: 0.1216 - val_accuracy: 0.9509\n",
      "Epoch 322/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9685\n",
      "Epoch 322: val_loss improved from 0.12161 to 0.12158, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0780 - accuracy: 0.9685 - val_loss: 0.1216 - val_accuracy: 0.9510\n",
      "Epoch 323/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9685\n",
      "Epoch 323: val_loss improved from 0.12158 to 0.12155, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0780 - accuracy: 0.9685 - val_loss: 0.1216 - val_accuracy: 0.9510\n",
      "Epoch 324/1000\n",
      "984/995 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9686\n",
      "Epoch 324: val_loss improved from 0.12155 to 0.12153, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0780 - accuracy: 0.9686 - val_loss: 0.1215 - val_accuracy: 0.9511\n",
      "Epoch 325/1000\n",
      "984/995 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9685\n",
      "Epoch 325: val_loss improved from 0.12153 to 0.12151, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0780 - accuracy: 0.9686 - val_loss: 0.1215 - val_accuracy: 0.9512\n",
      "Epoch 326/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9686\n",
      "Epoch 326: val_loss improved from 0.12151 to 0.12149, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0779 - accuracy: 0.9686 - val_loss: 0.1215 - val_accuracy: 0.9512\n",
      "Epoch 327/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9686\n",
      "Epoch 327: val_loss improved from 0.12149 to 0.12147, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0779 - accuracy: 0.9686 - val_loss: 0.1215 - val_accuracy: 0.9512\n",
      "Epoch 328/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9685\n",
      "Epoch 328: val_loss improved from 0.12147 to 0.12144, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0779 - accuracy: 0.9686 - val_loss: 0.1214 - val_accuracy: 0.9512\n",
      "Epoch 329/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9686\n",
      "Epoch 329: val_loss improved from 0.12144 to 0.12142, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0779 - accuracy: 0.9686 - val_loss: 0.1214 - val_accuracy: 0.9513\n",
      "Epoch 330/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9685\n",
      "Epoch 330: val_loss improved from 0.12142 to 0.12140, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0778 - accuracy: 0.9686 - val_loss: 0.1214 - val_accuracy: 0.9513\n",
      "Epoch 331/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9685\n",
      "Epoch 331: val_loss improved from 0.12140 to 0.12138, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0778 - accuracy: 0.9686 - val_loss: 0.1214 - val_accuracy: 0.9513\n",
      "Epoch 332/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9685\n",
      "Epoch 332: val_loss improved from 0.12138 to 0.12135, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0778 - accuracy: 0.9686 - val_loss: 0.1214 - val_accuracy: 0.9513\n",
      "Epoch 333/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9685\n",
      "Epoch 333: val_loss improved from 0.12135 to 0.12133, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0778 - accuracy: 0.9685 - val_loss: 0.1213 - val_accuracy: 0.9513\n",
      "Epoch 334/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0777 - accuracy: 0.9686\n",
      "Epoch 334: val_loss improved from 0.12133 to 0.12131, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0777 - accuracy: 0.9686 - val_loss: 0.1213 - val_accuracy: 0.9513\n",
      "Epoch 335/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9685\n",
      "Epoch 335: val_loss improved from 0.12131 to 0.12129, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0777 - accuracy: 0.9686 - val_loss: 0.1213 - val_accuracy: 0.9513\n",
      "Epoch 336/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9686\n",
      "Epoch 336: val_loss improved from 0.12129 to 0.12126, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0777 - accuracy: 0.9686 - val_loss: 0.1213 - val_accuracy: 0.9514\n",
      "Epoch 337/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9686\n",
      "Epoch 337: val_loss improved from 0.12126 to 0.12124, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0777 - accuracy: 0.9685 - val_loss: 0.1212 - val_accuracy: 0.9514\n",
      "Epoch 338/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9687\n",
      "Epoch 338: val_loss improved from 0.12124 to 0.12122, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0776 - accuracy: 0.9686 - val_loss: 0.1212 - val_accuracy: 0.9514\n",
      "Epoch 339/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9686\n",
      "Epoch 339: val_loss improved from 0.12122 to 0.12120, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0776 - accuracy: 0.9686 - val_loss: 0.1212 - val_accuracy: 0.9514\n",
      "Epoch 340/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9686\n",
      "Epoch 340: val_loss improved from 0.12120 to 0.12116, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0776 - accuracy: 0.9686 - val_loss: 0.1212 - val_accuracy: 0.9515\n",
      "Epoch 341/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9685\n",
      "Epoch 341: val_loss improved from 0.12116 to 0.12114, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0776 - accuracy: 0.9686 - val_loss: 0.1211 - val_accuracy: 0.9514\n",
      "Epoch 342/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9685\n",
      "Epoch 342: val_loss improved from 0.12114 to 0.12112, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0775 - accuracy: 0.9686 - val_loss: 0.1211 - val_accuracy: 0.9514\n",
      "Epoch 343/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9686\n",
      "Epoch 343: val_loss improved from 0.12112 to 0.12110, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0775 - accuracy: 0.9686 - val_loss: 0.1211 - val_accuracy: 0.9514\n",
      "Epoch 344/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0774 - accuracy: 0.9687\n",
      "Epoch 344: val_loss improved from 0.12110 to 0.12108, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0775 - accuracy: 0.9686 - val_loss: 0.1211 - val_accuracy: 0.9514\n",
      "Epoch 345/1000\n",
      "984/995 [============================>.] - ETA: 0s - loss: 0.0773 - accuracy: 0.9686\n",
      "Epoch 345: val_loss improved from 0.12108 to 0.12106, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0775 - accuracy: 0.9686 - val_loss: 0.1211 - val_accuracy: 0.9514\n",
      "Epoch 346/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9685\n",
      "Epoch 346: val_loss improved from 0.12106 to 0.12104, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0774 - accuracy: 0.9686 - val_loss: 0.1210 - val_accuracy: 0.9514\n",
      "Epoch 347/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9685\n",
      "Epoch 347: val_loss improved from 0.12104 to 0.12102, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0774 - accuracy: 0.9686 - val_loss: 0.1210 - val_accuracy: 0.9513\n",
      "Epoch 348/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9686\n",
      "Epoch 348: val_loss improved from 0.12102 to 0.12100, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0774 - accuracy: 0.9686 - val_loss: 0.1210 - val_accuracy: 0.9514\n",
      "Epoch 349/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0774 - accuracy: 0.9685\n",
      "Epoch 349: val_loss improved from 0.12100 to 0.12099, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0774 - accuracy: 0.9686 - val_loss: 0.1210 - val_accuracy: 0.9514\n",
      "Epoch 350/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0774 - accuracy: 0.9686\n",
      "Epoch 350: val_loss improved from 0.12099 to 0.12096, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0773 - accuracy: 0.9686 - val_loss: 0.1210 - val_accuracy: 0.9514\n",
      "Epoch 351/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0774 - accuracy: 0.9685\n",
      "Epoch 351: val_loss improved from 0.12096 to 0.12094, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0773 - accuracy: 0.9686 - val_loss: 0.1209 - val_accuracy: 0.9514\n",
      "Epoch 352/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0772 - accuracy: 0.9686\n",
      "Epoch 352: val_loss improved from 0.12094 to 0.12091, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0773 - accuracy: 0.9686 - val_loss: 0.1209 - val_accuracy: 0.9514\n",
      "Epoch 353/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9686\n",
      "Epoch 353: val_loss improved from 0.12091 to 0.12088, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0773 - accuracy: 0.9686 - val_loss: 0.1209 - val_accuracy: 0.9515\n",
      "Epoch 354/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0772 - accuracy: 0.9687\n",
      "Epoch 354: val_loss improved from 0.12088 to 0.12086, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0772 - accuracy: 0.9687 - val_loss: 0.1209 - val_accuracy: 0.9515\n",
      "Epoch 355/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0773 - accuracy: 0.9687\n",
      "Epoch 355: val_loss improved from 0.12086 to 0.12083, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 7ms/step - loss: 0.0772 - accuracy: 0.9687 - val_loss: 0.1208 - val_accuracy: 0.9515\n",
      "Epoch 356/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9687\n",
      "Epoch 356: val_loss improved from 0.12083 to 0.12081, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0772 - accuracy: 0.9687 - val_loss: 0.1208 - val_accuracy: 0.9515\n",
      "Epoch 357/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9687\n",
      "Epoch 357: val_loss improved from 0.12081 to 0.12079, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0772 - accuracy: 0.9687 - val_loss: 0.1208 - val_accuracy: 0.9515\n",
      "Epoch 358/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0772 - accuracy: 0.9687\n",
      "Epoch 358: val_loss improved from 0.12079 to 0.12077, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0772 - accuracy: 0.9687 - val_loss: 0.1208 - val_accuracy: 0.9516\n",
      "Epoch 359/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9688\n",
      "Epoch 359: val_loss improved from 0.12077 to 0.12074, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0771 - accuracy: 0.9687 - val_loss: 0.1207 - val_accuracy: 0.9516\n",
      "Epoch 360/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9687\n",
      "Epoch 360: val_loss improved from 0.12074 to 0.12072, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0771 - accuracy: 0.9687 - val_loss: 0.1207 - val_accuracy: 0.9516\n",
      "Epoch 361/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9687\n",
      "Epoch 361: val_loss improved from 0.12072 to 0.12070, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0771 - accuracy: 0.9687 - val_loss: 0.1207 - val_accuracy: 0.9517\n",
      "Epoch 362/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9688\n",
      "Epoch 362: val_loss improved from 0.12070 to 0.12067, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0771 - accuracy: 0.9687 - val_loss: 0.1207 - val_accuracy: 0.9517\n",
      "Epoch 363/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0770 - accuracy: 0.9687\n",
      "Epoch 363: val_loss improved from 0.12067 to 0.12065, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0770 - accuracy: 0.9687 - val_loss: 0.1206 - val_accuracy: 0.9517\n",
      "Epoch 364/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9687\n",
      "Epoch 364: val_loss improved from 0.12065 to 0.12063, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0770 - accuracy: 0.9687 - val_loss: 0.1206 - val_accuracy: 0.9517\n",
      "Epoch 365/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9687\n",
      "Epoch 365: val_loss improved from 0.12063 to 0.12061, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0770 - accuracy: 0.9687 - val_loss: 0.1206 - val_accuracy: 0.9517\n",
      "Epoch 366/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0770 - accuracy: 0.9687\n",
      "Epoch 366: val_loss improved from 0.12061 to 0.12060, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0770 - accuracy: 0.9687 - val_loss: 0.1206 - val_accuracy: 0.9518\n",
      "Epoch 367/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0770 - accuracy: 0.9687\n",
      "Epoch 367: val_loss improved from 0.12060 to 0.12056, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0770 - accuracy: 0.9687 - val_loss: 0.1206 - val_accuracy: 0.9518\n",
      "Epoch 368/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9688\n",
      "Epoch 368: val_loss improved from 0.12056 to 0.12054, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0769 - accuracy: 0.9686 - val_loss: 0.1205 - val_accuracy: 0.9519\n",
      "Epoch 369/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9687\n",
      "Epoch 369: val_loss improved from 0.12054 to 0.12052, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0769 - accuracy: 0.9687 - val_loss: 0.1205 - val_accuracy: 0.9519\n",
      "Epoch 370/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9687\n",
      "Epoch 370: val_loss improved from 0.12052 to 0.12050, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0769 - accuracy: 0.9686 - val_loss: 0.1205 - val_accuracy: 0.9519\n",
      "Epoch 371/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0767 - accuracy: 0.9687\n",
      "Epoch 371: val_loss improved from 0.12050 to 0.12048, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0769 - accuracy: 0.9687 - val_loss: 0.1205 - val_accuracy: 0.9519\n",
      "Epoch 372/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9686\n",
      "Epoch 372: val_loss improved from 0.12048 to 0.12047, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0768 - accuracy: 0.9686 - val_loss: 0.1205 - val_accuracy: 0.9519\n",
      "Epoch 373/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9687\n",
      "Epoch 373: val_loss improved from 0.12047 to 0.12044, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0768 - accuracy: 0.9687 - val_loss: 0.1204 - val_accuracy: 0.9519\n",
      "Epoch 374/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9686\n",
      "Epoch 374: val_loss improved from 0.12044 to 0.12042, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0768 - accuracy: 0.9686 - val_loss: 0.1204 - val_accuracy: 0.9519\n",
      "Epoch 375/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9686\n",
      "Epoch 375: val_loss improved from 0.12042 to 0.12040, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0768 - accuracy: 0.9687 - val_loss: 0.1204 - val_accuracy: 0.9519\n",
      "Epoch 376/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9687\n",
      "Epoch 376: val_loss improved from 0.12040 to 0.12037, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0768 - accuracy: 0.9687 - val_loss: 0.1204 - val_accuracy: 0.9519\n",
      "Epoch 377/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0767 - accuracy: 0.9688\n",
      "Epoch 377: val_loss improved from 0.12037 to 0.12034, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0767 - accuracy: 0.9687 - val_loss: 0.1203 - val_accuracy: 0.9519\n",
      "Epoch 378/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9688\n",
      "Epoch 378: val_loss improved from 0.12034 to 0.12033, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0767 - accuracy: 0.9687 - val_loss: 0.1203 - val_accuracy: 0.9519\n",
      "Epoch 379/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0767 - accuracy: 0.9687\n",
      "Epoch 379: val_loss improved from 0.12033 to 0.12030, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0767 - accuracy: 0.9687 - val_loss: 0.1203 - val_accuracy: 0.9519\n",
      "Epoch 380/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0767 - accuracy: 0.9687\n",
      "Epoch 380: val_loss improved from 0.12030 to 0.12029, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0767 - accuracy: 0.9687 - val_loss: 0.1203 - val_accuracy: 0.9519\n",
      "Epoch 381/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9688\n",
      "Epoch 381: val_loss improved from 0.12029 to 0.12027, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0767 - accuracy: 0.9688 - val_loss: 0.1203 - val_accuracy: 0.9519\n",
      "Epoch 382/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9687\n",
      "Epoch 382: val_loss improved from 0.12027 to 0.12025, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0766 - accuracy: 0.9687 - val_loss: 0.1202 - val_accuracy: 0.9519\n",
      "Epoch 383/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0767 - accuracy: 0.9687\n",
      "Epoch 383: val_loss improved from 0.12025 to 0.12022, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0766 - accuracy: 0.9688 - val_loss: 0.1202 - val_accuracy: 0.9518\n",
      "Epoch 384/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0767 - accuracy: 0.9688\n",
      "Epoch 384: val_loss improved from 0.12022 to 0.12019, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0766 - accuracy: 0.9688 - val_loss: 0.1202 - val_accuracy: 0.9518\n",
      "Epoch 385/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9688\n",
      "Epoch 385: val_loss improved from 0.12019 to 0.12018, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0766 - accuracy: 0.9688 - val_loss: 0.1202 - val_accuracy: 0.9518\n",
      "Epoch 386/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0767 - accuracy: 0.9687\n",
      "Epoch 386: val_loss improved from 0.12018 to 0.12016, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0766 - accuracy: 0.9688 - val_loss: 0.1202 - val_accuracy: 0.9518\n",
      "Epoch 387/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9688\n",
      "Epoch 387: val_loss improved from 0.12016 to 0.12014, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0765 - accuracy: 0.9688 - val_loss: 0.1201 - val_accuracy: 0.9518\n",
      "Epoch 388/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9688\n",
      "Epoch 388: val_loss improved from 0.12014 to 0.12012, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0765 - accuracy: 0.9688 - val_loss: 0.1201 - val_accuracy: 0.9518\n",
      "Epoch 389/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9688\n",
      "Epoch 389: val_loss improved from 0.12012 to 0.12009, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0765 - accuracy: 0.9688 - val_loss: 0.1201 - val_accuracy: 0.9518\n",
      "Epoch 390/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9689\n",
      "Epoch 390: val_loss improved from 0.12009 to 0.12007, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0765 - accuracy: 0.9688 - val_loss: 0.1201 - val_accuracy: 0.9518\n",
      "Epoch 391/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9689\n",
      "Epoch 391: val_loss improved from 0.12007 to 0.12005, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0765 - accuracy: 0.9689 - val_loss: 0.1200 - val_accuracy: 0.9518\n",
      "Epoch 392/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9688\n",
      "Epoch 392: val_loss improved from 0.12005 to 0.12003, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0764 - accuracy: 0.9688 - val_loss: 0.1200 - val_accuracy: 0.9519\n",
      "Epoch 393/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9688\n",
      "Epoch 393: val_loss improved from 0.12003 to 0.12002, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0764 - accuracy: 0.9688 - val_loss: 0.1200 - val_accuracy: 0.9519\n",
      "Epoch 394/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9690\n",
      "Epoch 394: val_loss improved from 0.12002 to 0.11999, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0764 - accuracy: 0.9689 - val_loss: 0.1200 - val_accuracy: 0.9519\n",
      "Epoch 395/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9689\n",
      "Epoch 395: val_loss improved from 0.11999 to 0.11998, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0764 - accuracy: 0.9689 - val_loss: 0.1200 - val_accuracy: 0.9519\n",
      "Epoch 396/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9689\n",
      "Epoch 396: val_loss improved from 0.11998 to 0.11995, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0764 - accuracy: 0.9689 - val_loss: 0.1200 - val_accuracy: 0.9518\n",
      "Epoch 397/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9689\n",
      "Epoch 397: val_loss improved from 0.11995 to 0.11994, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0763 - accuracy: 0.9689 - val_loss: 0.1199 - val_accuracy: 0.9517\n",
      "Epoch 398/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9689\n",
      "Epoch 398: val_loss improved from 0.11994 to 0.11991, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0763 - accuracy: 0.9689 - val_loss: 0.1199 - val_accuracy: 0.9517\n",
      "Epoch 399/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9688\n",
      "Epoch 399: val_loss improved from 0.11991 to 0.11989, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0763 - accuracy: 0.9689 - val_loss: 0.1199 - val_accuracy: 0.9517\n",
      "Epoch 400/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9689\n",
      "Epoch 400: val_loss improved from 0.11989 to 0.11987, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0763 - accuracy: 0.9689 - val_loss: 0.1199 - val_accuracy: 0.9517\n",
      "Epoch 401/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9689\n",
      "Epoch 401: val_loss improved from 0.11987 to 0.11984, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0763 - accuracy: 0.9689 - val_loss: 0.1198 - val_accuracy: 0.9517\n",
      "Epoch 402/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9689\n",
      "Epoch 402: val_loss improved from 0.11984 to 0.11981, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0762 - accuracy: 0.9689 - val_loss: 0.1198 - val_accuracy: 0.9517\n",
      "Epoch 403/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9689\n",
      "Epoch 403: val_loss improved from 0.11981 to 0.11979, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 7ms/step - loss: 0.0762 - accuracy: 0.9690 - val_loss: 0.1198 - val_accuracy: 0.9517\n",
      "Epoch 404/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9690\n",
      "Epoch 404: val_loss improved from 0.11979 to 0.11978, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0762 - accuracy: 0.9690 - val_loss: 0.1198 - val_accuracy: 0.9519\n",
      "Epoch 405/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9690\n",
      "Epoch 405: val_loss improved from 0.11978 to 0.11976, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0762 - accuracy: 0.9690 - val_loss: 0.1198 - val_accuracy: 0.9519\n",
      "Epoch 406/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9690\n",
      "Epoch 406: val_loss improved from 0.11976 to 0.11973, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0762 - accuracy: 0.9689 - val_loss: 0.1197 - val_accuracy: 0.9519\n",
      "Epoch 407/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9689\n",
      "Epoch 407: val_loss improved from 0.11973 to 0.11971, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0761 - accuracy: 0.9690 - val_loss: 0.1197 - val_accuracy: 0.9519\n",
      "Epoch 408/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9690\n",
      "Epoch 408: val_loss improved from 0.11971 to 0.11967, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0761 - accuracy: 0.9690 - val_loss: 0.1197 - val_accuracy: 0.9519\n",
      "Epoch 409/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9690\n",
      "Epoch 409: val_loss improved from 0.11967 to 0.11965, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0761 - accuracy: 0.9690 - val_loss: 0.1197 - val_accuracy: 0.9519\n",
      "Epoch 410/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9690\n",
      "Epoch 410: val_loss improved from 0.11965 to 0.11963, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0761 - accuracy: 0.9690 - val_loss: 0.1196 - val_accuracy: 0.9519\n",
      "Epoch 411/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9690\n",
      "Epoch 411: val_loss improved from 0.11963 to 0.11961, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0761 - accuracy: 0.9690 - val_loss: 0.1196 - val_accuracy: 0.9519\n",
      "Epoch 412/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9689\n",
      "Epoch 412: val_loss improved from 0.11961 to 0.11960, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0760 - accuracy: 0.9690 - val_loss: 0.1196 - val_accuracy: 0.9519\n",
      "Epoch 413/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9690\n",
      "Epoch 413: val_loss improved from 0.11960 to 0.11958, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0760 - accuracy: 0.9690 - val_loss: 0.1196 - val_accuracy: 0.9520\n",
      "Epoch 414/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9691\n",
      "Epoch 414: val_loss improved from 0.11958 to 0.11956, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0760 - accuracy: 0.9690 - val_loss: 0.1196 - val_accuracy: 0.9519\n",
      "Epoch 415/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9691\n",
      "Epoch 415: val_loss improved from 0.11956 to 0.11953, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0760 - accuracy: 0.9690 - val_loss: 0.1195 - val_accuracy: 0.9520\n",
      "Epoch 416/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9690\n",
      "Epoch 416: val_loss improved from 0.11953 to 0.11951, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0760 - accuracy: 0.9690 - val_loss: 0.1195 - val_accuracy: 0.9520\n",
      "Epoch 417/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9690\n",
      "Epoch 417: val_loss improved from 0.11951 to 0.11949, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0759 - accuracy: 0.9690 - val_loss: 0.1195 - val_accuracy: 0.9520\n",
      "Epoch 418/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9691\n",
      "Epoch 418: val_loss improved from 0.11949 to 0.11946, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 7ms/step - loss: 0.0759 - accuracy: 0.9690 - val_loss: 0.1195 - val_accuracy: 0.9519\n",
      "Epoch 419/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9691\n",
      "Epoch 419: val_loss improved from 0.11946 to 0.11944, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0759 - accuracy: 0.9690 - val_loss: 0.1194 - val_accuracy: 0.9519\n",
      "Epoch 420/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9691\n",
      "Epoch 420: val_loss improved from 0.11944 to 0.11942, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0759 - accuracy: 0.9691 - val_loss: 0.1194 - val_accuracy: 0.9519\n",
      "Epoch 421/1000\n",
      "984/995 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9689\n",
      "Epoch 421: val_loss improved from 0.11942 to 0.11939, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0759 - accuracy: 0.9690 - val_loss: 0.1194 - val_accuracy: 0.9519\n",
      "Epoch 422/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9690\n",
      "Epoch 422: val_loss improved from 0.11939 to 0.11937, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 5s 5ms/step - loss: 0.0758 - accuracy: 0.9690 - val_loss: 0.1194 - val_accuracy: 0.9519\n",
      "Epoch 423/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9691\n",
      "Epoch 423: val_loss improved from 0.11937 to 0.11935, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0758 - accuracy: 0.9690 - val_loss: 0.1193 - val_accuracy: 0.9519\n",
      "Epoch 424/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9690\n",
      "Epoch 424: val_loss improved from 0.11935 to 0.11932, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0758 - accuracy: 0.9691 - val_loss: 0.1193 - val_accuracy: 0.9519\n",
      "Epoch 425/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9691\n",
      "Epoch 425: val_loss improved from 0.11932 to 0.11930, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0758 - accuracy: 0.9691 - val_loss: 0.1193 - val_accuracy: 0.9519\n",
      "Epoch 426/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9691\n",
      "Epoch 426: val_loss improved from 0.11930 to 0.11928, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0758 - accuracy: 0.9691 - val_loss: 0.1193 - val_accuracy: 0.9520\n",
      "Epoch 427/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9690\n",
      "Epoch 427: val_loss improved from 0.11928 to 0.11926, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0757 - accuracy: 0.9691 - val_loss: 0.1193 - val_accuracy: 0.9520\n",
      "Epoch 428/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9690\n",
      "Epoch 428: val_loss improved from 0.11926 to 0.11924, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0757 - accuracy: 0.9690 - val_loss: 0.1192 - val_accuracy: 0.9520\n",
      "Epoch 429/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0755 - accuracy: 0.9691\n",
      "Epoch 429: val_loss improved from 0.11924 to 0.11922, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0757 - accuracy: 0.9690 - val_loss: 0.1192 - val_accuracy: 0.9520\n",
      "Epoch 430/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9691\n",
      "Epoch 430: val_loss improved from 0.11922 to 0.11921, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0757 - accuracy: 0.9691 - val_loss: 0.1192 - val_accuracy: 0.9520\n",
      "Epoch 431/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9691\n",
      "Epoch 431: val_loss improved from 0.11921 to 0.11918, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0757 - accuracy: 0.9690 - val_loss: 0.1192 - val_accuracy: 0.9520\n",
      "Epoch 432/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9691\n",
      "Epoch 432: val_loss improved from 0.11918 to 0.11916, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0757 - accuracy: 0.9691 - val_loss: 0.1192 - val_accuracy: 0.9520\n",
      "Epoch 433/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9691\n",
      "Epoch 433: val_loss improved from 0.11916 to 0.11914, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0756 - accuracy: 0.9691 - val_loss: 0.1191 - val_accuracy: 0.9520\n",
      "Epoch 434/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9691\n",
      "Epoch 434: val_loss improved from 0.11914 to 0.11911, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0756 - accuracy: 0.9691 - val_loss: 0.1191 - val_accuracy: 0.9520\n",
      "Epoch 435/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9692\n",
      "Epoch 435: val_loss improved from 0.11911 to 0.11910, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0756 - accuracy: 0.9691 - val_loss: 0.1191 - val_accuracy: 0.9520\n",
      "Epoch 436/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9691\n",
      "Epoch 436: val_loss improved from 0.11910 to 0.11908, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0756 - accuracy: 0.9691 - val_loss: 0.1191 - val_accuracy: 0.9520\n",
      "Epoch 437/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9692\n",
      "Epoch 437: val_loss improved from 0.11908 to 0.11905, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0756 - accuracy: 0.9692 - val_loss: 0.1191 - val_accuracy: 0.9520\n",
      "Epoch 438/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9691\n",
      "Epoch 438: val_loss improved from 0.11905 to 0.11904, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0755 - accuracy: 0.9691 - val_loss: 0.1190 - val_accuracy: 0.9520\n",
      "Epoch 439/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9691\n",
      "Epoch 439: val_loss improved from 0.11904 to 0.11901, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0755 - accuracy: 0.9692 - val_loss: 0.1190 - val_accuracy: 0.9520\n",
      "Epoch 440/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0755 - accuracy: 0.9691\n",
      "Epoch 440: val_loss improved from 0.11901 to 0.11900, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0755 - accuracy: 0.9691 - val_loss: 0.1190 - val_accuracy: 0.9520\n",
      "Epoch 441/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0755 - accuracy: 0.9691\n",
      "Epoch 441: val_loss improved from 0.11900 to 0.11898, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0755 - accuracy: 0.9691 - val_loss: 0.1190 - val_accuracy: 0.9520\n",
      "Epoch 442/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9691\n",
      "Epoch 442: val_loss improved from 0.11898 to 0.11896, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0755 - accuracy: 0.9691 - val_loss: 0.1190 - val_accuracy: 0.9520\n",
      "Epoch 443/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9692\n",
      "Epoch 443: val_loss improved from 0.11896 to 0.11893, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0755 - accuracy: 0.9692 - val_loss: 0.1189 - val_accuracy: 0.9520\n",
      "Epoch 444/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0755 - accuracy: 0.9691\n",
      "Epoch 444: val_loss improved from 0.11893 to 0.11891, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0754 - accuracy: 0.9691 - val_loss: 0.1189 - val_accuracy: 0.9520\n",
      "Epoch 445/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9692\n",
      "Epoch 445: val_loss improved from 0.11891 to 0.11889, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0754 - accuracy: 0.9692 - val_loss: 0.1189 - val_accuracy: 0.9520\n",
      "Epoch 446/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9692\n",
      "Epoch 446: val_loss improved from 0.11889 to 0.11887, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0754 - accuracy: 0.9692 - val_loss: 0.1189 - val_accuracy: 0.9520\n",
      "Epoch 447/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9692\n",
      "Epoch 447: val_loss improved from 0.11887 to 0.11885, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0754 - accuracy: 0.9692 - val_loss: 0.1189 - val_accuracy: 0.9520\n",
      "Epoch 448/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9692\n",
      "Epoch 448: val_loss improved from 0.11885 to 0.11882, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0754 - accuracy: 0.9692 - val_loss: 0.1188 - val_accuracy: 0.9520\n",
      "Epoch 449/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9692\n",
      "Epoch 449: val_loss improved from 0.11882 to 0.11880, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0754 - accuracy: 0.9692 - val_loss: 0.1188 - val_accuracy: 0.9520\n",
      "Epoch 450/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9692\n",
      "Epoch 450: val_loss improved from 0.11880 to 0.11878, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0753 - accuracy: 0.9692 - val_loss: 0.1188 - val_accuracy: 0.9520\n",
      "Epoch 451/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9693\n",
      "Epoch 451: val_loss improved from 0.11878 to 0.11876, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0753 - accuracy: 0.9692 - val_loss: 0.1188 - val_accuracy: 0.9520\n",
      "Epoch 452/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9693\n",
      "Epoch 452: val_loss improved from 0.11876 to 0.11874, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0753 - accuracy: 0.9692 - val_loss: 0.1187 - val_accuracy: 0.9521\n",
      "Epoch 453/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9693\n",
      "Epoch 453: val_loss improved from 0.11874 to 0.11872, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0753 - accuracy: 0.9692 - val_loss: 0.1187 - val_accuracy: 0.9521\n",
      "Epoch 454/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9692\n",
      "Epoch 454: val_loss improved from 0.11872 to 0.11870, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0753 - accuracy: 0.9692 - val_loss: 0.1187 - val_accuracy: 0.9521\n",
      "Epoch 455/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9692\n",
      "Epoch 455: val_loss improved from 0.11870 to 0.11867, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0752 - accuracy: 0.9692 - val_loss: 0.1187 - val_accuracy: 0.9521\n",
      "Epoch 456/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9692\n",
      "Epoch 456: val_loss improved from 0.11867 to 0.11866, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0752 - accuracy: 0.9692 - val_loss: 0.1187 - val_accuracy: 0.9521\n",
      "Epoch 457/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9692\n",
      "Epoch 457: val_loss improved from 0.11866 to 0.11864, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0752 - accuracy: 0.9692 - val_loss: 0.1186 - val_accuracy: 0.9521\n",
      "Epoch 458/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9691\n",
      "Epoch 458: val_loss improved from 0.11864 to 0.11861, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0752 - accuracy: 0.9692 - val_loss: 0.1186 - val_accuracy: 0.9521\n",
      "Epoch 459/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9691\n",
      "Epoch 459: val_loss improved from 0.11861 to 0.11859, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0752 - accuracy: 0.9692 - val_loss: 0.1186 - val_accuracy: 0.9521\n",
      "Epoch 460/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9690\n",
      "Epoch 460: val_loss improved from 0.11859 to 0.11857, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0752 - accuracy: 0.9691 - val_loss: 0.1186 - val_accuracy: 0.9521\n",
      "Epoch 461/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9691\n",
      "Epoch 461: val_loss improved from 0.11857 to 0.11855, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0751 - accuracy: 0.9691 - val_loss: 0.1185 - val_accuracy: 0.9522\n",
      "Epoch 462/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9691\n",
      "Epoch 462: val_loss improved from 0.11855 to 0.11852, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0751 - accuracy: 0.9691 - val_loss: 0.1185 - val_accuracy: 0.9522\n",
      "Epoch 463/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9691\n",
      "Epoch 463: val_loss improved from 0.11852 to 0.11851, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0751 - accuracy: 0.9691 - val_loss: 0.1185 - val_accuracy: 0.9522\n",
      "Epoch 464/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9691\n",
      "Epoch 464: val_loss improved from 0.11851 to 0.11848, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0751 - accuracy: 0.9691 - val_loss: 0.1185 - val_accuracy: 0.9522\n",
      "Epoch 465/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9691\n",
      "Epoch 465: val_loss improved from 0.11848 to 0.11845, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0751 - accuracy: 0.9691 - val_loss: 0.1185 - val_accuracy: 0.9522\n",
      "Epoch 466/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9691\n",
      "Epoch 466: val_loss improved from 0.11845 to 0.11843, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0750 - accuracy: 0.9691 - val_loss: 0.1184 - val_accuracy: 0.9522\n",
      "Epoch 467/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9692\n",
      "Epoch 467: val_loss improved from 0.11843 to 0.11840, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0750 - accuracy: 0.9691 - val_loss: 0.1184 - val_accuracy: 0.9522\n",
      "Epoch 468/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9691\n",
      "Epoch 468: val_loss improved from 0.11840 to 0.11837, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0750 - accuracy: 0.9691 - val_loss: 0.1184 - val_accuracy: 0.9522\n",
      "Epoch 469/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9691\n",
      "Epoch 469: val_loss improved from 0.11837 to 0.11835, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0750 - accuracy: 0.9691 - val_loss: 0.1183 - val_accuracy: 0.9522\n",
      "Epoch 470/1000\n",
      "984/995 [============================>.] - ETA: 0s - loss: 0.0748 - accuracy: 0.9691\n",
      "Epoch 470: val_loss improved from 0.11835 to 0.11833, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0750 - accuracy: 0.9691 - val_loss: 0.1183 - val_accuracy: 0.9522\n",
      "Epoch 471/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9692\n",
      "Epoch 471: val_loss improved from 0.11833 to 0.11830, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0750 - accuracy: 0.9691 - val_loss: 0.1183 - val_accuracy: 0.9523\n",
      "Epoch 472/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9691\n",
      "Epoch 472: val_loss improved from 0.11830 to 0.11828, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0749 - accuracy: 0.9691 - val_loss: 0.1183 - val_accuracy: 0.9522\n",
      "Epoch 473/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9691\n",
      "Epoch 473: val_loss improved from 0.11828 to 0.11826, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0749 - accuracy: 0.9691 - val_loss: 0.1183 - val_accuracy: 0.9522\n",
      "Epoch 474/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9691\n",
      "Epoch 474: val_loss improved from 0.11826 to 0.11823, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0749 - accuracy: 0.9691 - val_loss: 0.1182 - val_accuracy: 0.9522\n",
      "Epoch 475/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0746 - accuracy: 0.9692\n",
      "Epoch 475: val_loss improved from 0.11823 to 0.11820, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0749 - accuracy: 0.9692 - val_loss: 0.1182 - val_accuracy: 0.9523\n",
      "Epoch 476/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0748 - accuracy: 0.9693\n",
      "Epoch 476: val_loss improved from 0.11820 to 0.11816, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0749 - accuracy: 0.9692 - val_loss: 0.1182 - val_accuracy: 0.9523\n",
      "Epoch 477/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9692\n",
      "Epoch 477: val_loss improved from 0.11816 to 0.11813, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0748 - accuracy: 0.9692 - val_loss: 0.1181 - val_accuracy: 0.9523\n",
      "Epoch 478/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9692\n",
      "Epoch 478: val_loss improved from 0.11813 to 0.11812, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0748 - accuracy: 0.9692 - val_loss: 0.1181 - val_accuracy: 0.9522\n",
      "Epoch 479/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9692\n",
      "Epoch 479: val_loss improved from 0.11812 to 0.11810, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0748 - accuracy: 0.9692 - val_loss: 0.1181 - val_accuracy: 0.9522\n",
      "Epoch 480/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9693\n",
      "Epoch 480: val_loss improved from 0.11810 to 0.11807, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0748 - accuracy: 0.9692 - val_loss: 0.1181 - val_accuracy: 0.9522\n",
      "Epoch 481/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0749 - accuracy: 0.9692\n",
      "Epoch 481: val_loss improved from 0.11807 to 0.11806, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0748 - accuracy: 0.9692 - val_loss: 0.1181 - val_accuracy: 0.9522\n",
      "Epoch 482/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0748 - accuracy: 0.9692\n",
      "Epoch 482: val_loss improved from 0.11806 to 0.11803, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0748 - accuracy: 0.9692 - val_loss: 0.1180 - val_accuracy: 0.9522\n",
      "Epoch 483/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9692\n",
      "Epoch 483: val_loss improved from 0.11803 to 0.11801, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0747 - accuracy: 0.9692 - val_loss: 0.1180 - val_accuracy: 0.9522\n",
      "Epoch 484/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9692\n",
      "Epoch 484: val_loss improved from 0.11801 to 0.11800, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0747 - accuracy: 0.9692 - val_loss: 0.1180 - val_accuracy: 0.9522\n",
      "Epoch 485/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9692\n",
      "Epoch 485: val_loss improved from 0.11800 to 0.11797, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0747 - accuracy: 0.9692 - val_loss: 0.1180 - val_accuracy: 0.9522\n",
      "Epoch 486/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9692\n",
      "Epoch 486: val_loss improved from 0.11797 to 0.11795, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0747 - accuracy: 0.9692 - val_loss: 0.1180 - val_accuracy: 0.9522\n",
      "Epoch 487/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9691\n",
      "Epoch 487: val_loss improved from 0.11795 to 0.11793, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0747 - accuracy: 0.9691 - val_loss: 0.1179 - val_accuracy: 0.9522\n",
      "Epoch 488/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0746 - accuracy: 0.9692\n",
      "Epoch 488: val_loss improved from 0.11793 to 0.11791, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0747 - accuracy: 0.9691 - val_loss: 0.1179 - val_accuracy: 0.9522\n",
      "Epoch 489/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9691\n",
      "Epoch 489: val_loss improved from 0.11791 to 0.11789, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0747 - accuracy: 0.9691 - val_loss: 0.1179 - val_accuracy: 0.9522\n",
      "Epoch 490/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0748 - accuracy: 0.9690\n",
      "Epoch 490: val_loss improved from 0.11789 to 0.11787, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0746 - accuracy: 0.9691 - val_loss: 0.1179 - val_accuracy: 0.9522\n",
      "Epoch 491/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9691\n",
      "Epoch 491: val_loss improved from 0.11787 to 0.11784, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0746 - accuracy: 0.9691 - val_loss: 0.1178 - val_accuracy: 0.9520\n",
      "Epoch 492/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9691\n",
      "Epoch 492: val_loss improved from 0.11784 to 0.11781, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0746 - accuracy: 0.9691 - val_loss: 0.1178 - val_accuracy: 0.9520\n",
      "Epoch 493/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9691\n",
      "Epoch 493: val_loss improved from 0.11781 to 0.11779, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0746 - accuracy: 0.9691 - val_loss: 0.1178 - val_accuracy: 0.9520\n",
      "Epoch 494/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9690\n",
      "Epoch 494: val_loss improved from 0.11779 to 0.11777, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0746 - accuracy: 0.9691 - val_loss: 0.1178 - val_accuracy: 0.9519\n",
      "Epoch 495/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9691\n",
      "Epoch 495: val_loss improved from 0.11777 to 0.11775, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0746 - accuracy: 0.9691 - val_loss: 0.1177 - val_accuracy: 0.9519\n",
      "Epoch 496/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9691\n",
      "Epoch 496: val_loss improved from 0.11775 to 0.11773, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0745 - accuracy: 0.9691 - val_loss: 0.1177 - val_accuracy: 0.9519\n",
      "Epoch 497/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0746 - accuracy: 0.9691\n",
      "Epoch 497: val_loss improved from 0.11773 to 0.11771, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0745 - accuracy: 0.9691 - val_loss: 0.1177 - val_accuracy: 0.9520\n",
      "Epoch 498/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0746 - accuracy: 0.9691\n",
      "Epoch 498: val_loss improved from 0.11771 to 0.11769, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0745 - accuracy: 0.9691 - val_loss: 0.1177 - val_accuracy: 0.9519\n",
      "Epoch 499/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9691\n",
      "Epoch 499: val_loss improved from 0.11769 to 0.11767, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0745 - accuracy: 0.9691 - val_loss: 0.1177 - val_accuracy: 0.9519\n",
      "Epoch 500/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9691\n",
      "Epoch 500: val_loss improved from 0.11767 to 0.11766, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0745 - accuracy: 0.9691 - val_loss: 0.1177 - val_accuracy: 0.9519\n",
      "Epoch 501/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0746 - accuracy: 0.9690\n",
      "Epoch 501: val_loss improved from 0.11766 to 0.11764, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0745 - accuracy: 0.9691 - val_loss: 0.1176 - val_accuracy: 0.9519\n",
      "Epoch 502/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9691\n",
      "Epoch 502: val_loss improved from 0.11764 to 0.11762, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0745 - accuracy: 0.9691 - val_loss: 0.1176 - val_accuracy: 0.9520\n",
      "Epoch 503/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9692\n",
      "Epoch 503: val_loss improved from 0.11762 to 0.11761, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0745 - accuracy: 0.9691 - val_loss: 0.1176 - val_accuracy: 0.9520\n",
      "Epoch 504/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9691\n",
      "Epoch 504: val_loss improved from 0.11761 to 0.11758, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0744 - accuracy: 0.9692 - val_loss: 0.1176 - val_accuracy: 0.9520\n",
      "Epoch 505/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9691\n",
      "Epoch 505: val_loss improved from 0.11758 to 0.11756, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0744 - accuracy: 0.9691 - val_loss: 0.1176 - val_accuracy: 0.9520\n",
      "Epoch 506/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9691\n",
      "Epoch 506: val_loss improved from 0.11756 to 0.11755, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0744 - accuracy: 0.9691 - val_loss: 0.1175 - val_accuracy: 0.9520\n",
      "Epoch 507/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9690\n",
      "Epoch 507: val_loss improved from 0.11755 to 0.11754, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0744 - accuracy: 0.9691 - val_loss: 0.1175 - val_accuracy: 0.9520\n",
      "Epoch 508/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9691\n",
      "Epoch 508: val_loss improved from 0.11754 to 0.11752, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0744 - accuracy: 0.9691 - val_loss: 0.1175 - val_accuracy: 0.9521\n",
      "Epoch 509/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9691\n",
      "Epoch 509: val_loss improved from 0.11752 to 0.11751, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0744 - accuracy: 0.9691 - val_loss: 0.1175 - val_accuracy: 0.9521\n",
      "Epoch 510/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9692\n",
      "Epoch 510: val_loss improved from 0.11751 to 0.11748, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0744 - accuracy: 0.9691 - val_loss: 0.1175 - val_accuracy: 0.9521\n",
      "Epoch 511/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9691\n",
      "Epoch 511: val_loss improved from 0.11748 to 0.11746, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0744 - accuracy: 0.9691 - val_loss: 0.1175 - val_accuracy: 0.9520\n",
      "Epoch 512/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9691\n",
      "Epoch 512: val_loss improved from 0.11746 to 0.11744, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0743 - accuracy: 0.9691 - val_loss: 0.1174 - val_accuracy: 0.9520\n",
      "Epoch 513/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9691\n",
      "Epoch 513: val_loss improved from 0.11744 to 0.11743, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0743 - accuracy: 0.9691 - val_loss: 0.1174 - val_accuracy: 0.9520\n",
      "Epoch 514/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9691\n",
      "Epoch 514: val_loss improved from 0.11743 to 0.11742, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0743 - accuracy: 0.9691 - val_loss: 0.1174 - val_accuracy: 0.9520\n",
      "Epoch 515/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9691\n",
      "Epoch 515: val_loss improved from 0.11742 to 0.11740, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0743 - accuracy: 0.9691 - val_loss: 0.1174 - val_accuracy: 0.9519\n",
      "Epoch 516/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9691\n",
      "Epoch 516: val_loss improved from 0.11740 to 0.11738, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0743 - accuracy: 0.9691 - val_loss: 0.1174 - val_accuracy: 0.9519\n",
      "Epoch 517/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9691\n",
      "Epoch 517: val_loss improved from 0.11738 to 0.11736, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0743 - accuracy: 0.9691 - val_loss: 0.1174 - val_accuracy: 0.9519\n",
      "Epoch 518/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9691\n",
      "Epoch 518: val_loss improved from 0.11736 to 0.11735, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0743 - accuracy: 0.9691 - val_loss: 0.1174 - val_accuracy: 0.9519\n",
      "Epoch 519/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9691\n",
      "Epoch 519: val_loss improved from 0.11735 to 0.11734, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0743 - accuracy: 0.9691 - val_loss: 0.1173 - val_accuracy: 0.9519\n",
      "Epoch 520/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9690\n",
      "Epoch 520: val_loss improved from 0.11734 to 0.11732, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0742 - accuracy: 0.9691 - val_loss: 0.1173 - val_accuracy: 0.9519\n",
      "Epoch 521/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9691\n",
      "Epoch 521: val_loss improved from 0.11732 to 0.11731, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0742 - accuracy: 0.9691 - val_loss: 0.1173 - val_accuracy: 0.9519\n",
      "Epoch 522/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0742 - accuracy: 0.9691\n",
      "Epoch 522: val_loss improved from 0.11731 to 0.11730, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0742 - accuracy: 0.9691 - val_loss: 0.1173 - val_accuracy: 0.9519\n",
      "Epoch 523/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9691\n",
      "Epoch 523: val_loss improved from 0.11730 to 0.11728, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0742 - accuracy: 0.9691 - val_loss: 0.1173 - val_accuracy: 0.9520\n",
      "Epoch 524/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9691\n",
      "Epoch 524: val_loss improved from 0.11728 to 0.11727, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0742 - accuracy: 0.9691 - val_loss: 0.1173 - val_accuracy: 0.9520\n",
      "Epoch 525/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0742 - accuracy: 0.9691\n",
      "Epoch 525: val_loss improved from 0.11727 to 0.11726, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0742 - accuracy: 0.9691 - val_loss: 0.1173 - val_accuracy: 0.9520\n",
      "Epoch 526/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9690\n",
      "Epoch 526: val_loss improved from 0.11726 to 0.11725, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0742 - accuracy: 0.9691 - val_loss: 0.1172 - val_accuracy: 0.9520\n",
      "Epoch 527/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9691\n",
      "Epoch 527: val_loss improved from 0.11725 to 0.11723, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0742 - accuracy: 0.9691 - val_loss: 0.1172 - val_accuracy: 0.9520\n",
      "Epoch 528/1000\n",
      "984/995 [============================>.] - ETA: 0s - loss: 0.0742 - accuracy: 0.9691\n",
      "Epoch 528: val_loss improved from 0.11723 to 0.11721, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0741 - accuracy: 0.9691 - val_loss: 0.1172 - val_accuracy: 0.9520\n",
      "Epoch 529/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9690\n",
      "Epoch 529: val_loss improved from 0.11721 to 0.11720, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0741 - accuracy: 0.9690 - val_loss: 0.1172 - val_accuracy: 0.9520\n",
      "Epoch 530/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9691\n",
      "Epoch 530: val_loss improved from 0.11720 to 0.11718, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0741 - accuracy: 0.9691 - val_loss: 0.1172 - val_accuracy: 0.9520\n",
      "Epoch 531/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9691\n",
      "Epoch 531: val_loss improved from 0.11718 to 0.11717, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0741 - accuracy: 0.9691 - val_loss: 0.1172 - val_accuracy: 0.9520\n",
      "Epoch 532/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9691\n",
      "Epoch 532: val_loss improved from 0.11717 to 0.11716, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0741 - accuracy: 0.9691 - val_loss: 0.1172 - val_accuracy: 0.9520\n",
      "Epoch 533/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0742 - accuracy: 0.9690\n",
      "Epoch 533: val_loss improved from 0.11716 to 0.11714, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0741 - accuracy: 0.9690 - val_loss: 0.1171 - val_accuracy: 0.9520\n",
      "Epoch 534/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9690\n",
      "Epoch 534: val_loss improved from 0.11714 to 0.11713, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0741 - accuracy: 0.9690 - val_loss: 0.1171 - val_accuracy: 0.9520\n",
      "Epoch 535/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9691\n",
      "Epoch 535: val_loss improved from 0.11713 to 0.11712, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0741 - accuracy: 0.9690 - val_loss: 0.1171 - val_accuracy: 0.9520\n",
      "Epoch 536/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9691\n",
      "Epoch 536: val_loss improved from 0.11712 to 0.11711, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0741 - accuracy: 0.9690 - val_loss: 0.1171 - val_accuracy: 0.9520\n",
      "Epoch 537/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0742 - accuracy: 0.9690\n",
      "Epoch 537: val_loss improved from 0.11711 to 0.11710, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0740 - accuracy: 0.9690 - val_loss: 0.1171 - val_accuracy: 0.9520\n",
      "Epoch 538/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0740 - accuracy: 0.9691\n",
      "Epoch 538: val_loss improved from 0.11710 to 0.11708, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0740 - accuracy: 0.9690 - val_loss: 0.1171 - val_accuracy: 0.9520\n",
      "Epoch 539/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9690\n",
      "Epoch 539: val_loss improved from 0.11708 to 0.11706, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0740 - accuracy: 0.9691 - val_loss: 0.1171 - val_accuracy: 0.9520\n",
      "Epoch 540/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9691\n",
      "Epoch 540: val_loss improved from 0.11706 to 0.11704, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0740 - accuracy: 0.9690 - val_loss: 0.1170 - val_accuracy: 0.9522\n",
      "Epoch 541/1000\n",
      "984/995 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9690\n",
      "Epoch 541: val_loss improved from 0.11704 to 0.11703, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0740 - accuracy: 0.9690 - val_loss: 0.1170 - val_accuracy: 0.9522\n",
      "Epoch 542/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9691\n",
      "Epoch 542: val_loss improved from 0.11703 to 0.11701, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0740 - accuracy: 0.9690 - val_loss: 0.1170 - val_accuracy: 0.9522\n",
      "Epoch 543/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9689\n",
      "Epoch 543: val_loss improved from 0.11701 to 0.11700, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0740 - accuracy: 0.9690 - val_loss: 0.1170 - val_accuracy: 0.9522\n",
      "Epoch 544/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0740 - accuracy: 0.9690\n",
      "Epoch 544: val_loss improved from 0.11700 to 0.11698, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0740 - accuracy: 0.9690 - val_loss: 0.1170 - val_accuracy: 0.9522\n",
      "Epoch 545/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9691\n",
      "Epoch 545: val_loss improved from 0.11698 to 0.11697, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0740 - accuracy: 0.9690 - val_loss: 0.1170 - val_accuracy: 0.9522\n",
      "Epoch 546/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9690\n",
      "Epoch 546: val_loss improved from 0.11697 to 0.11696, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0739 - accuracy: 0.9690 - val_loss: 0.1170 - val_accuracy: 0.9522\n",
      "Epoch 547/1000\n",
      "983/995 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9689\n",
      "Epoch 547: val_loss improved from 0.11696 to 0.11694, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0739 - accuracy: 0.9690 - val_loss: 0.1169 - val_accuracy: 0.9522\n",
      "Epoch 548/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9690\n",
      "Epoch 548: val_loss improved from 0.11694 to 0.11693, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0739 - accuracy: 0.9690 - val_loss: 0.1169 - val_accuracy: 0.9522\n",
      "Epoch 549/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9690\n",
      "Epoch 549: val_loss improved from 0.11693 to 0.11692, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0739 - accuracy: 0.9690 - val_loss: 0.1169 - val_accuracy: 0.9522\n",
      "Epoch 550/1000\n",
      "984/995 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9691\n",
      "Epoch 550: val_loss improved from 0.11692 to 0.11691, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0739 - accuracy: 0.9690 - val_loss: 0.1169 - val_accuracy: 0.9522\n",
      "Epoch 551/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9690\n",
      "Epoch 551: val_loss improved from 0.11691 to 0.11690, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0739 - accuracy: 0.9690 - val_loss: 0.1169 - val_accuracy: 0.9522\n",
      "Epoch 552/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9690\n",
      "Epoch 552: val_loss improved from 0.11690 to 0.11689, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0739 - accuracy: 0.9690 - val_loss: 0.1169 - val_accuracy: 0.9521\n",
      "Epoch 553/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9691\n",
      "Epoch 553: val_loss improved from 0.11689 to 0.11688, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0739 - accuracy: 0.9691 - val_loss: 0.1169 - val_accuracy: 0.9521\n",
      "Epoch 554/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9690\n",
      "Epoch 554: val_loss improved from 0.11688 to 0.11687, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0739 - accuracy: 0.9690 - val_loss: 0.1169 - val_accuracy: 0.9522\n",
      "Epoch 555/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0740 - accuracy: 0.9691\n",
      "Epoch 555: val_loss improved from 0.11687 to 0.11686, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0739 - accuracy: 0.9691 - val_loss: 0.1169 - val_accuracy: 0.9522\n",
      "Epoch 556/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9691\n",
      "Epoch 556: val_loss improved from 0.11686 to 0.11684, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0738 - accuracy: 0.9691 - val_loss: 0.1168 - val_accuracy: 0.9522\n",
      "Epoch 557/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9691\n",
      "Epoch 557: val_loss improved from 0.11684 to 0.11683, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0738 - accuracy: 0.9691 - val_loss: 0.1168 - val_accuracy: 0.9522\n",
      "Epoch 558/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9692\n",
      "Epoch 558: val_loss improved from 0.11683 to 0.11682, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0738 - accuracy: 0.9690 - val_loss: 0.1168 - val_accuracy: 0.9522\n",
      "Epoch 559/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9692\n",
      "Epoch 559: val_loss improved from 0.11682 to 0.11681, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0738 - accuracy: 0.9691 - val_loss: 0.1168 - val_accuracy: 0.9522\n",
      "Epoch 560/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0740 - accuracy: 0.9691\n",
      "Epoch 560: val_loss improved from 0.11681 to 0.11680, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0738 - accuracy: 0.9691 - val_loss: 0.1168 - val_accuracy: 0.9522\n",
      "Epoch 561/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9691\n",
      "Epoch 561: val_loss improved from 0.11680 to 0.11679, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0738 - accuracy: 0.9691 - val_loss: 0.1168 - val_accuracy: 0.9522\n",
      "Epoch 562/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9692\n",
      "Epoch 562: val_loss improved from 0.11679 to 0.11678, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0738 - accuracy: 0.9691 - val_loss: 0.1168 - val_accuracy: 0.9522\n",
      "Epoch 563/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9692\n",
      "Epoch 563: val_loss improved from 0.11678 to 0.11677, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0738 - accuracy: 0.9691 - val_loss: 0.1168 - val_accuracy: 0.9523\n",
      "Epoch 564/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9691\n",
      "Epoch 564: val_loss improved from 0.11677 to 0.11675, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0738 - accuracy: 0.9691 - val_loss: 0.1168 - val_accuracy: 0.9523\n",
      "Epoch 565/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9691\n",
      "Epoch 565: val_loss improved from 0.11675 to 0.11674, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0738 - accuracy: 0.9691 - val_loss: 0.1167 - val_accuracy: 0.9523\n",
      "Epoch 566/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9692\n",
      "Epoch 566: val_loss improved from 0.11674 to 0.11673, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0738 - accuracy: 0.9691 - val_loss: 0.1167 - val_accuracy: 0.9524\n",
      "Epoch 567/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9691\n",
      "Epoch 567: val_loss improved from 0.11673 to 0.11672, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0737 - accuracy: 0.9692 - val_loss: 0.1167 - val_accuracy: 0.9524\n",
      "Epoch 568/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9691\n",
      "Epoch 568: val_loss improved from 0.11672 to 0.11671, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0737 - accuracy: 0.9692 - val_loss: 0.1167 - val_accuracy: 0.9524\n",
      "Epoch 569/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9692\n",
      "Epoch 569: val_loss improved from 0.11671 to 0.11669, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0737 - accuracy: 0.9692 - val_loss: 0.1167 - val_accuracy: 0.9524\n",
      "Epoch 570/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9692\n",
      "Epoch 570: val_loss improved from 0.11669 to 0.11668, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0737 - accuracy: 0.9692 - val_loss: 0.1167 - val_accuracy: 0.9524\n",
      "Epoch 571/1000\n",
      "982/995 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9691\n",
      "Epoch 571: val_loss improved from 0.11668 to 0.11667, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0737 - accuracy: 0.9692 - val_loss: 0.1167 - val_accuracy: 0.9523\n",
      "Epoch 572/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9692\n",
      "Epoch 572: val_loss improved from 0.11667 to 0.11667, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0737 - accuracy: 0.9692 - val_loss: 0.1167 - val_accuracy: 0.9524\n",
      "Epoch 573/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9693\n",
      "Epoch 573: val_loss improved from 0.11667 to 0.11666, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0737 - accuracy: 0.9692 - val_loss: 0.1167 - val_accuracy: 0.9524\n",
      "Epoch 574/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9692\n",
      "Epoch 574: val_loss improved from 0.11666 to 0.11665, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0737 - accuracy: 0.9692 - val_loss: 0.1166 - val_accuracy: 0.9524\n",
      "Epoch 575/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9692\n",
      "Epoch 575: val_loss improved from 0.11665 to 0.11664, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0737 - accuracy: 0.9692 - val_loss: 0.1166 - val_accuracy: 0.9524\n",
      "Epoch 576/1000\n",
      "984/995 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9693\n",
      "Epoch 576: val_loss improved from 0.11664 to 0.11662, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0737 - accuracy: 0.9692 - val_loss: 0.1166 - val_accuracy: 0.9524\n",
      "Epoch 577/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9692\n",
      "Epoch 577: val_loss improved from 0.11662 to 0.11661, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0737 - accuracy: 0.9692 - val_loss: 0.1166 - val_accuracy: 0.9524\n",
      "Epoch 578/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9691\n",
      "Epoch 578: val_loss improved from 0.11661 to 0.11661, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0736 - accuracy: 0.9692 - val_loss: 0.1166 - val_accuracy: 0.9524\n",
      "Epoch 579/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9692\n",
      "Epoch 579: val_loss improved from 0.11661 to 0.11660, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0736 - accuracy: 0.9692 - val_loss: 0.1166 - val_accuracy: 0.9524\n",
      "Epoch 580/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9691\n",
      "Epoch 580: val_loss improved from 0.11660 to 0.11659, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 7ms/step - loss: 0.0736 - accuracy: 0.9692 - val_loss: 0.1166 - val_accuracy: 0.9523\n",
      "Epoch 581/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9692\n",
      "Epoch 581: val_loss improved from 0.11659 to 0.11658, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0736 - accuracy: 0.9692 - val_loss: 0.1166 - val_accuracy: 0.9524\n",
      "Epoch 582/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9691\n",
      "Epoch 582: val_loss improved from 0.11658 to 0.11657, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0736 - accuracy: 0.9692 - val_loss: 0.1166 - val_accuracy: 0.9523\n",
      "Epoch 583/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9692\n",
      "Epoch 583: val_loss improved from 0.11657 to 0.11656, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0736 - accuracy: 0.9691 - val_loss: 0.1166 - val_accuracy: 0.9522\n",
      "Epoch 584/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9692\n",
      "Epoch 584: val_loss improved from 0.11656 to 0.11655, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0736 - accuracy: 0.9692 - val_loss: 0.1165 - val_accuracy: 0.9522\n",
      "Epoch 585/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9691\n",
      "Epoch 585: val_loss improved from 0.11655 to 0.11653, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0736 - accuracy: 0.9692 - val_loss: 0.1165 - val_accuracy: 0.9522\n",
      "Epoch 586/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9692\n",
      "Epoch 586: val_loss improved from 0.11653 to 0.11653, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0736 - accuracy: 0.9692 - val_loss: 0.1165 - val_accuracy: 0.9522\n",
      "Epoch 587/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9693\n",
      "Epoch 587: val_loss improved from 0.11653 to 0.11651, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0736 - accuracy: 0.9692 - val_loss: 0.1165 - val_accuracy: 0.9522\n",
      "Epoch 588/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9692\n",
      "Epoch 588: val_loss improved from 0.11651 to 0.11651, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0736 - accuracy: 0.9692 - val_loss: 0.1165 - val_accuracy: 0.9522\n",
      "Epoch 589/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9692\n",
      "Epoch 589: val_loss improved from 0.11651 to 0.11650, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0736 - accuracy: 0.9692 - val_loss: 0.1165 - val_accuracy: 0.9522\n",
      "Epoch 590/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9692\n",
      "Epoch 590: val_loss improved from 0.11650 to 0.11649, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0735 - accuracy: 0.9692 - val_loss: 0.1165 - val_accuracy: 0.9522\n",
      "Epoch 591/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9691\n",
      "Epoch 591: val_loss improved from 0.11649 to 0.11649, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0735 - accuracy: 0.9692 - val_loss: 0.1165 - val_accuracy: 0.9522\n",
      "Epoch 592/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9692\n",
      "Epoch 592: val_loss improved from 0.11649 to 0.11648, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0735 - accuracy: 0.9692 - val_loss: 0.1165 - val_accuracy: 0.9522\n",
      "Epoch 593/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9693\n",
      "Epoch 593: val_loss improved from 0.11648 to 0.11647, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0735 - accuracy: 0.9692 - val_loss: 0.1165 - val_accuracy: 0.9523\n",
      "Epoch 594/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9692\n",
      "Epoch 594: val_loss improved from 0.11647 to 0.11646, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0735 - accuracy: 0.9692 - val_loss: 0.1165 - val_accuracy: 0.9523\n",
      "Epoch 595/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9692\n",
      "Epoch 595: val_loss improved from 0.11646 to 0.11644, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0735 - accuracy: 0.9692 - val_loss: 0.1164 - val_accuracy: 0.9523\n",
      "Epoch 596/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9692\n",
      "Epoch 596: val_loss improved from 0.11644 to 0.11644, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0735 - accuracy: 0.9692 - val_loss: 0.1164 - val_accuracy: 0.9522\n",
      "Epoch 597/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9692\n",
      "Epoch 597: val_loss improved from 0.11644 to 0.11643, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0735 - accuracy: 0.9692 - val_loss: 0.1164 - val_accuracy: 0.9522\n",
      "Epoch 598/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9692\n",
      "Epoch 598: val_loss did not improve from 0.11643\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0735 - accuracy: 0.9692 - val_loss: 0.1164 - val_accuracy: 0.9522\n",
      "Epoch 599/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9692\n",
      "Epoch 599: val_loss improved from 0.11643 to 0.11642, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0735 - accuracy: 0.9692 - val_loss: 0.1164 - val_accuracy: 0.9522\n",
      "Epoch 600/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9692\n",
      "Epoch 600: val_loss improved from 0.11642 to 0.11641, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0735 - accuracy: 0.9692 - val_loss: 0.1164 - val_accuracy: 0.9522\n",
      "Epoch 601/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9692\n",
      "Epoch 601: val_loss improved from 0.11641 to 0.11640, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0735 - accuracy: 0.9692 - val_loss: 0.1164 - val_accuracy: 0.9522\n",
      "Epoch 602/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9692\n",
      "Epoch 602: val_loss improved from 0.11640 to 0.11639, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0734 - accuracy: 0.9692 - val_loss: 0.1164 - val_accuracy: 0.9522\n",
      "Epoch 603/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9693\n",
      "Epoch 603: val_loss improved from 0.11639 to 0.11639, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0734 - accuracy: 0.9692 - val_loss: 0.1164 - val_accuracy: 0.9522\n",
      "Epoch 604/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9692\n",
      "Epoch 604: val_loss improved from 0.11639 to 0.11637, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0734 - accuracy: 0.9692 - val_loss: 0.1164 - val_accuracy: 0.9522\n",
      "Epoch 605/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9692\n",
      "Epoch 605: val_loss improved from 0.11637 to 0.11636, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0734 - accuracy: 0.9692 - val_loss: 0.1164 - val_accuracy: 0.9522\n",
      "Epoch 606/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9692\n",
      "Epoch 606: val_loss improved from 0.11636 to 0.11635, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0734 - accuracy: 0.9692 - val_loss: 0.1164 - val_accuracy: 0.9522\n",
      "Epoch 607/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9693\n",
      "Epoch 607: val_loss improved from 0.11635 to 0.11635, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0734 - accuracy: 0.9692 - val_loss: 0.1163 - val_accuracy: 0.9522\n",
      "Epoch 608/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9693\n",
      "Epoch 608: val_loss improved from 0.11635 to 0.11634, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0734 - accuracy: 0.9692 - val_loss: 0.1163 - val_accuracy: 0.9522\n",
      "Epoch 609/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9692\n",
      "Epoch 609: val_loss improved from 0.11634 to 0.11634, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0734 - accuracy: 0.9692 - val_loss: 0.1163 - val_accuracy: 0.9522\n",
      "Epoch 610/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9693\n",
      "Epoch 610: val_loss improved from 0.11634 to 0.11632, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0734 - accuracy: 0.9692 - val_loss: 0.1163 - val_accuracy: 0.9522\n",
      "Epoch 611/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9692\n",
      "Epoch 611: val_loss improved from 0.11632 to 0.11631, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0734 - accuracy: 0.9692 - val_loss: 0.1163 - val_accuracy: 0.9522\n",
      "Epoch 612/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9691\n",
      "Epoch 612: val_loss improved from 0.11631 to 0.11630, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0734 - accuracy: 0.9692 - val_loss: 0.1163 - val_accuracy: 0.9522\n",
      "Epoch 613/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9693\n",
      "Epoch 613: val_loss improved from 0.11630 to 0.11630, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0734 - accuracy: 0.9693 - val_loss: 0.1163 - val_accuracy: 0.9523\n",
      "Epoch 614/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9692\n",
      "Epoch 614: val_loss improved from 0.11630 to 0.11630, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0733 - accuracy: 0.9692 - val_loss: 0.1163 - val_accuracy: 0.9523\n",
      "Epoch 615/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9692\n",
      "Epoch 615: val_loss improved from 0.11630 to 0.11628, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0733 - accuracy: 0.9692 - val_loss: 0.1163 - val_accuracy: 0.9524\n",
      "Epoch 616/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9693\n",
      "Epoch 616: val_loss improved from 0.11628 to 0.11627, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0733 - accuracy: 0.9692 - val_loss: 0.1163 - val_accuracy: 0.9523\n",
      "Epoch 617/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9693\n",
      "Epoch 617: val_loss improved from 0.11627 to 0.11626, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0733 - accuracy: 0.9693 - val_loss: 0.1163 - val_accuracy: 0.9524\n",
      "Epoch 618/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9694\n",
      "Epoch 618: val_loss improved from 0.11626 to 0.11625, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0733 - accuracy: 0.9693 - val_loss: 0.1162 - val_accuracy: 0.9524\n",
      "Epoch 619/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9692\n",
      "Epoch 619: val_loss improved from 0.11625 to 0.11623, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0733 - accuracy: 0.9692 - val_loss: 0.1162 - val_accuracy: 0.9524\n",
      "Epoch 620/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9694\n",
      "Epoch 620: val_loss improved from 0.11623 to 0.11623, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0733 - accuracy: 0.9693 - val_loss: 0.1162 - val_accuracy: 0.9524\n",
      "Epoch 621/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9693\n",
      "Epoch 621: val_loss improved from 0.11623 to 0.11622, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0733 - accuracy: 0.9693 - val_loss: 0.1162 - val_accuracy: 0.9524\n",
      "Epoch 622/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9693\n",
      "Epoch 622: val_loss improved from 0.11622 to 0.11620, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0733 - accuracy: 0.9693 - val_loss: 0.1162 - val_accuracy: 0.9524\n",
      "Epoch 623/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9693\n",
      "Epoch 623: val_loss improved from 0.11620 to 0.11620, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0733 - accuracy: 0.9693 - val_loss: 0.1162 - val_accuracy: 0.9524\n",
      "Epoch 624/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9693\n",
      "Epoch 624: val_loss improved from 0.11620 to 0.11619, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0733 - accuracy: 0.9693 - val_loss: 0.1162 - val_accuracy: 0.9524\n",
      "Epoch 625/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9693\n",
      "Epoch 625: val_loss improved from 0.11619 to 0.11619, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0733 - accuracy: 0.9693 - val_loss: 0.1162 - val_accuracy: 0.9524\n",
      "Epoch 626/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9694\n",
      "Epoch 626: val_loss improved from 0.11619 to 0.11618, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0733 - accuracy: 0.9693 - val_loss: 0.1162 - val_accuracy: 0.9524\n",
      "Epoch 627/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9693\n",
      "Epoch 627: val_loss improved from 0.11618 to 0.11618, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0733 - accuracy: 0.9693 - val_loss: 0.1162 - val_accuracy: 0.9524\n",
      "Epoch 628/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9693\n",
      "Epoch 628: val_loss improved from 0.11618 to 0.11618, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9693 - val_loss: 0.1162 - val_accuracy: 0.9524\n",
      "Epoch 629/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9693\n",
      "Epoch 629: val_loss improved from 0.11618 to 0.11617, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9693 - val_loss: 0.1162 - val_accuracy: 0.9524\n",
      "Epoch 630/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9693\n",
      "Epoch 630: val_loss improved from 0.11617 to 0.11617, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9693 - val_loss: 0.1162 - val_accuracy: 0.9524\n",
      "Epoch 631/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9693\n",
      "Epoch 631: val_loss improved from 0.11617 to 0.11615, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9693 - val_loss: 0.1162 - val_accuracy: 0.9524\n",
      "Epoch 632/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9693\n",
      "Epoch 632: val_loss improved from 0.11615 to 0.11615, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9692 - val_loss: 0.1161 - val_accuracy: 0.9524\n",
      "Epoch 633/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9693\n",
      "Epoch 633: val_loss improved from 0.11615 to 0.11614, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9693 - val_loss: 0.1161 - val_accuracy: 0.9525\n",
      "Epoch 634/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9692\n",
      "Epoch 634: val_loss improved from 0.11614 to 0.11613, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9692 - val_loss: 0.1161 - val_accuracy: 0.9525\n",
      "Epoch 635/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9693\n",
      "Epoch 635: val_loss improved from 0.11613 to 0.11612, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9693 - val_loss: 0.1161 - val_accuracy: 0.9525\n",
      "Epoch 636/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9692\n",
      "Epoch 636: val_loss improved from 0.11612 to 0.11611, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9692 - val_loss: 0.1161 - val_accuracy: 0.9525\n",
      "Epoch 637/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9692\n",
      "Epoch 637: val_loss improved from 0.11611 to 0.11611, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9692 - val_loss: 0.1161 - val_accuracy: 0.9525\n",
      "Epoch 638/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9692\n",
      "Epoch 638: val_loss improved from 0.11611 to 0.11610, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9693 - val_loss: 0.1161 - val_accuracy: 0.9525\n",
      "Epoch 639/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9692\n",
      "Epoch 639: val_loss improved from 0.11610 to 0.11608, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9692 - val_loss: 0.1161 - val_accuracy: 0.9526\n",
      "Epoch 640/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9692\n",
      "Epoch 640: val_loss improved from 0.11608 to 0.11608, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9692 - val_loss: 0.1161 - val_accuracy: 0.9526\n",
      "Epoch 641/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9691\n",
      "Epoch 641: val_loss improved from 0.11608 to 0.11607, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9692 - val_loss: 0.1161 - val_accuracy: 0.9526\n",
      "Epoch 642/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9692\n",
      "Epoch 642: val_loss improved from 0.11607 to 0.11607, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0732 - accuracy: 0.9692 - val_loss: 0.1161 - val_accuracy: 0.9526\n",
      "Epoch 643/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9691\n",
      "Epoch 643: val_loss improved from 0.11607 to 0.11606, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1161 - val_accuracy: 0.9526\n",
      "Epoch 644/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9692\n",
      "Epoch 644: val_loss improved from 0.11606 to 0.11605, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1161 - val_accuracy: 0.9526\n",
      "Epoch 645/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9692\n",
      "Epoch 645: val_loss improved from 0.11605 to 0.11604, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1160 - val_accuracy: 0.9526\n",
      "Epoch 646/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9692\n",
      "Epoch 646: val_loss improved from 0.11604 to 0.11604, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1160 - val_accuracy: 0.9526\n",
      "Epoch 647/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9693\n",
      "Epoch 647: val_loss did not improve from 0.11604\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1160 - val_accuracy: 0.9526\n",
      "Epoch 648/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9692\n",
      "Epoch 648: val_loss improved from 0.11604 to 0.11603, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1160 - val_accuracy: 0.9526\n",
      "Epoch 649/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9691\n",
      "Epoch 649: val_loss improved from 0.11603 to 0.11602, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1160 - val_accuracy: 0.9526\n",
      "Epoch 650/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9692\n",
      "Epoch 650: val_loss improved from 0.11602 to 0.11601, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1160 - val_accuracy: 0.9526\n",
      "Epoch 651/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9692\n",
      "Epoch 651: val_loss improved from 0.11601 to 0.11600, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1160 - val_accuracy: 0.9526\n",
      "Epoch 652/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9692\n",
      "Epoch 652: val_loss improved from 0.11600 to 0.11600, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1160 - val_accuracy: 0.9526\n",
      "Epoch 653/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9692\n",
      "Epoch 653: val_loss improved from 0.11600 to 0.11599, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1160 - val_accuracy: 0.9526\n",
      "Epoch 654/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9691\n",
      "Epoch 654: val_loss improved from 0.11599 to 0.11598, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1160 - val_accuracy: 0.9526\n",
      "Epoch 655/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9692\n",
      "Epoch 655: val_loss improved from 0.11598 to 0.11598, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1160 - val_accuracy: 0.9526\n",
      "Epoch 656/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9692\n",
      "Epoch 656: val_loss improved from 0.11598 to 0.11597, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1160 - val_accuracy: 0.9526\n",
      "Epoch 657/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9692\n",
      "Epoch 657: val_loss improved from 0.11597 to 0.11596, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1160 - val_accuracy: 0.9526\n",
      "Epoch 658/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9691\n",
      "Epoch 658: val_loss improved from 0.11596 to 0.11595, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.1160 - val_accuracy: 0.9526\n",
      "Epoch 659/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9692\n",
      "Epoch 659: val_loss improved from 0.11595 to 0.11594, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 5s 5ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9526\n",
      "Epoch 660/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9693\n",
      "Epoch 660: val_loss did not improve from 0.11594\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9526\n",
      "Epoch 661/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9692\n",
      "Epoch 661: val_loss improved from 0.11594 to 0.11594, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9526\n",
      "Epoch 662/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9693\n",
      "Epoch 662: val_loss improved from 0.11594 to 0.11593, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9526\n",
      "Epoch 663/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9693\n",
      "Epoch 663: val_loss improved from 0.11593 to 0.11593, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9526\n",
      "Epoch 664/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9692\n",
      "Epoch 664: val_loss improved from 0.11593 to 0.11592, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9526\n",
      "Epoch 665/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9691\n",
      "Epoch 665: val_loss improved from 0.11592 to 0.11591, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9526\n",
      "Epoch 666/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9693\n",
      "Epoch 666: val_loss improved from 0.11591 to 0.11591, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9527\n",
      "Epoch 667/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9693\n",
      "Epoch 667: val_loss did not improve from 0.11591\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9527\n",
      "Epoch 668/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9692\n",
      "Epoch 668: val_loss improved from 0.11591 to 0.11590, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9527\n",
      "Epoch 669/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9693\n",
      "Epoch 669: val_loss improved from 0.11590 to 0.11590, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9527\n",
      "Epoch 670/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9692\n",
      "Epoch 670: val_loss improved from 0.11590 to 0.11590, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9527\n",
      "Epoch 671/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9692\n",
      "Epoch 671: val_loss improved from 0.11590 to 0.11589, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9527\n",
      "Epoch 672/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9693\n",
      "Epoch 672: val_loss improved from 0.11589 to 0.11589, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 7ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9527\n",
      "Epoch 673/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9693\n",
      "Epoch 673: val_loss improved from 0.11589 to 0.11587, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9527\n",
      "Epoch 674/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9693\n",
      "Epoch 674: val_loss improved from 0.11587 to 0.11587, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9527\n",
      "Epoch 675/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9692\n",
      "Epoch 675: val_loss improved from 0.11587 to 0.11586, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9527\n",
      "Epoch 676/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9692\n",
      "Epoch 676: val_loss improved from 0.11586 to 0.11585, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0730 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9527\n",
      "Epoch 677/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9693\n",
      "Epoch 677: val_loss improved from 0.11585 to 0.11585, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 5s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9527\n",
      "Epoch 678/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9692\n",
      "Epoch 678: val_loss improved from 0.11585 to 0.11584, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9527\n",
      "Epoch 679/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9693\n",
      "Epoch 679: val_loss improved from 0.11584 to 0.11583, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9527\n",
      "Epoch 680/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9693\n",
      "Epoch 680: val_loss improved from 0.11583 to 0.11582, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 7ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9527\n",
      "Epoch 681/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9692\n",
      "Epoch 681: val_loss improved from 0.11582 to 0.11582, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9527\n",
      "Epoch 682/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9692\n",
      "Epoch 682: val_loss improved from 0.11582 to 0.11581, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9527\n",
      "Epoch 683/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9694\n",
      "Epoch 683: val_loss did not improve from 0.11581\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9527\n",
      "Epoch 684/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9692\n",
      "Epoch 684: val_loss improved from 0.11581 to 0.11580, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9527\n",
      "Epoch 685/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9692\n",
      "Epoch 685: val_loss improved from 0.11580 to 0.11579, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9527\n",
      "Epoch 686/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9692\n",
      "Epoch 686: val_loss improved from 0.11579 to 0.11579, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9527\n",
      "Epoch 687/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9691\n",
      "Epoch 687: val_loss did not improve from 0.11579\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9526\n",
      "Epoch 688/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9691\n",
      "Epoch 688: val_loss improved from 0.11579 to 0.11578, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9526\n",
      "Epoch 689/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9693\n",
      "Epoch 689: val_loss did not improve from 0.11578\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9526\n",
      "Epoch 690/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9692\n",
      "Epoch 690: val_loss improved from 0.11578 to 0.11577, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9526\n",
      "Epoch 691/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9692\n",
      "Epoch 691: val_loss improved from 0.11577 to 0.11577, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9526\n",
      "Epoch 692/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9692\n",
      "Epoch 692: val_loss improved from 0.11577 to 0.11576, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9526\n",
      "Epoch 693/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9692\n",
      "Epoch 693: val_loss improved from 0.11576 to 0.11576, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9526\n",
      "Epoch 694/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9693\n",
      "Epoch 694: val_loss improved from 0.11576 to 0.11575, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9527\n",
      "Epoch 695/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9692\n",
      "Epoch 695: val_loss did not improve from 0.11575\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9527\n",
      "Epoch 696/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9692\n",
      "Epoch 696: val_loss improved from 0.11575 to 0.11574, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 697/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9692\n",
      "Epoch 697: val_loss improved from 0.11574 to 0.11573, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 698/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9692\n",
      "Epoch 698: val_loss improved from 0.11573 to 0.11573, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 699/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9692\n",
      "Epoch 699: val_loss improved from 0.11573 to 0.11573, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 700/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 700: val_loss improved from 0.11573 to 0.11573, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 701/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9692\n",
      "Epoch 701: val_loss improved from 0.11573 to 0.11572, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 5s 5ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 702/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 702: val_loss improved from 0.11572 to 0.11571, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 703/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9693\n",
      "Epoch 703: val_loss improved from 0.11571 to 0.11570, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 704/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9692\n",
      "Epoch 704: val_loss improved from 0.11570 to 0.11569, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 705/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9693\n",
      "Epoch 705: val_loss improved from 0.11569 to 0.11569, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 706/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9692\n",
      "Epoch 706: val_loss improved from 0.11569 to 0.11568, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 707/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9692\n",
      "Epoch 707: val_loss did not improve from 0.11568\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 708/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9692\n",
      "Epoch 708: val_loss improved from 0.11568 to 0.11567, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 709/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9692\n",
      "Epoch 709: val_loss improved from 0.11567 to 0.11566, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 710/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9692\n",
      "Epoch 710: val_loss did not improve from 0.11566\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 711/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9692\n",
      "Epoch 711: val_loss improved from 0.11566 to 0.11566, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 712/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9693\n",
      "Epoch 712: val_loss did not improve from 0.11566\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 713/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9692\n",
      "Epoch 713: val_loss did not improve from 0.11566\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 714/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9691\n",
      "Epoch 714: val_loss improved from 0.11566 to 0.11565, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9527\n",
      "Epoch 715/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9692\n",
      "Epoch 715: val_loss improved from 0.11565 to 0.11564, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 716/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9692\n",
      "Epoch 716: val_loss improved from 0.11564 to 0.11563, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9692 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 717/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9692\n",
      "Epoch 717: val_loss improved from 0.11563 to 0.11563, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 718/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 718: val_loss improved from 0.11563 to 0.11562, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9692 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 719/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9694\n",
      "Epoch 719: val_loss improved from 0.11562 to 0.11561, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 720/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 720: val_loss did not improve from 0.11561\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 721/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 721: val_loss did not improve from 0.11561\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9692 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 722/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 722: val_loss improved from 0.11561 to 0.11561, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9692 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 723/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9692\n",
      "Epoch 723: val_loss improved from 0.11561 to 0.11560, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 724/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 724: val_loss improved from 0.11560 to 0.11559, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 725/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 725: val_loss improved from 0.11559 to 0.11559, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 5s 5ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 726/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 726: val_loss improved from 0.11559 to 0.11559, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 727/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 727: val_loss improved from 0.11559 to 0.11558, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 728/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 728: val_loss improved from 0.11558 to 0.11557, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 729/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 729: val_loss improved from 0.11557 to 0.11557, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 730/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9693\n",
      "Epoch 730: val_loss improved from 0.11557 to 0.11556, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 731/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 731: val_loss improved from 0.11556 to 0.11556, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 732/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 732: val_loss improved from 0.11556 to 0.11555, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1156 - val_accuracy: 0.9527\n",
      "Epoch 733/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 733: val_loss improved from 0.11555 to 0.11554, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9527\n",
      "Epoch 734/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 734: val_loss improved from 0.11554 to 0.11552, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9527\n",
      "Epoch 735/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 735: val_loss improved from 0.11552 to 0.11552, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9527\n",
      "Epoch 736/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 736: val_loss improved from 0.11552 to 0.11551, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9526\n",
      "Epoch 737/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 737: val_loss improved from 0.11551 to 0.11551, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0727 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9526\n",
      "Epoch 738/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 738: val_loss did not improve from 0.11551\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9526\n",
      "Epoch 739/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 739: val_loss improved from 0.11551 to 0.11551, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9526\n",
      "Epoch 740/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9694\n",
      "Epoch 740: val_loss improved from 0.11551 to 0.11550, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9526\n",
      "Epoch 741/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 741: val_loss improved from 0.11550 to 0.11549, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9525\n",
      "Epoch 742/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 742: val_loss improved from 0.11549 to 0.11548, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9524\n",
      "Epoch 743/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 743: val_loss did not improve from 0.11548\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9525\n",
      "Epoch 744/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9693\n",
      "Epoch 744: val_loss improved from 0.11548 to 0.11547, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9524\n",
      "Epoch 745/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 745: val_loss improved from 0.11547 to 0.11547, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9524\n",
      "Epoch 746/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 746: val_loss did not improve from 0.11547\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9524\n",
      "Epoch 747/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 747: val_loss improved from 0.11547 to 0.11546, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9524\n",
      "Epoch 748/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 748: val_loss improved from 0.11546 to 0.11546, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9524\n",
      "Epoch 749/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9694\n",
      "Epoch 749: val_loss improved from 0.11546 to 0.11546, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9524\n",
      "Epoch 750/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9693\n",
      "Epoch 750: val_loss did not improve from 0.11546\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9524\n",
      "Epoch 751/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 751: val_loss improved from 0.11546 to 0.11545, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1155 - val_accuracy: 0.9524\n",
      "Epoch 752/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9694\n",
      "Epoch 752: val_loss improved from 0.11545 to 0.11545, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 753/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 753: val_loss improved from 0.11545 to 0.11544, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 754/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 754: val_loss improved from 0.11544 to 0.11544, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 755/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 755: val_loss improved from 0.11544 to 0.11544, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 756/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 756: val_loss improved from 0.11544 to 0.11543, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 757/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9694\n",
      "Epoch 757: val_loss improved from 0.11543 to 0.11543, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 758/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 758: val_loss improved from 0.11543 to 0.11543, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 759/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9694\n",
      "Epoch 759: val_loss improved from 0.11543 to 0.11542, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 760/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9693\n",
      "Epoch 760: val_loss improved from 0.11542 to 0.11542, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 761/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 761: val_loss did not improve from 0.11542\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 762/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9694\n",
      "Epoch 762: val_loss improved from 0.11542 to 0.11541, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0726 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 763/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 763: val_loss did not improve from 0.11541\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9525\n",
      "Epoch 764/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9694\n",
      "Epoch 764: val_loss improved from 0.11541 to 0.11541, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 765/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 765: val_loss improved from 0.11541 to 0.11540, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 766/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 766: val_loss improved from 0.11540 to 0.11540, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 767/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 767: val_loss improved from 0.11540 to 0.11539, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 768/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 768: val_loss improved from 0.11539 to 0.11539, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 769/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9693\n",
      "Epoch 769: val_loss improved from 0.11539 to 0.11539, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 770/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9693\n",
      "Epoch 770: val_loss improved from 0.11539 to 0.11537, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9524\n",
      "Epoch 771/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 771: val_loss did not improve from 0.11537\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9525\n",
      "Epoch 772/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9693\n",
      "Epoch 772: val_loss did not improve from 0.11537\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9525\n",
      "Epoch 773/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 773: val_loss improved from 0.11537 to 0.11537, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9525\n",
      "Epoch 774/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 774: val_loss improved from 0.11537 to 0.11537, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9525\n",
      "Epoch 775/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9693\n",
      "Epoch 775: val_loss improved from 0.11537 to 0.11536, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9525\n",
      "Epoch 776/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9693\n",
      "Epoch 776: val_loss did not improve from 0.11536\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9525\n",
      "Epoch 777/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9693\n",
      "Epoch 777: val_loss improved from 0.11536 to 0.11535, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9525\n",
      "Epoch 778/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9692\n",
      "Epoch 778: val_loss did not improve from 0.11535\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9692 - val_loss: 0.1154 - val_accuracy: 0.9525\n",
      "Epoch 779/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 779: val_loss improved from 0.11535 to 0.11535, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1154 - val_accuracy: 0.9525\n",
      "Epoch 780/1000\n",
      "984/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 780: val_loss improved from 0.11535 to 0.11534, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9525\n",
      "Epoch 781/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9693\n",
      "Epoch 781: val_loss improved from 0.11534 to 0.11534, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9525\n",
      "Epoch 782/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 782: val_loss did not improve from 0.11534\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9526\n",
      "Epoch 783/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 783: val_loss did not improve from 0.11534\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9526\n",
      "Epoch 784/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9693\n",
      "Epoch 784: val_loss improved from 0.11534 to 0.11534, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9526\n",
      "Epoch 785/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9692\n",
      "Epoch 785: val_loss improved from 0.11534 to 0.11534, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9526\n",
      "Epoch 786/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9694\n",
      "Epoch 786: val_loss improved from 0.11534 to 0.11533, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9525\n",
      "Epoch 787/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9692\n",
      "Epoch 787: val_loss did not improve from 0.11533\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9526\n",
      "Epoch 788/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 788: val_loss improved from 0.11533 to 0.11532, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9526\n",
      "Epoch 789/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9692\n",
      "Epoch 789: val_loss did not improve from 0.11532\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 790/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9692\n",
      "Epoch 790: val_loss did not improve from 0.11532\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 791/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 791: val_loss improved from 0.11532 to 0.11532, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9526\n",
      "Epoch 792/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9692\n",
      "Epoch 792: val_loss improved from 0.11532 to 0.11531, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9526\n",
      "Epoch 793/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 793: val_loss improved from 0.11531 to 0.11531, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9526\n",
      "Epoch 794/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 794: val_loss did not improve from 0.11531\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 795/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9693\n",
      "Epoch 795: val_loss improved from 0.11531 to 0.11531, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9526\n",
      "Epoch 796/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 796: val_loss improved from 0.11531 to 0.11530, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1153 - val_accuracy: 0.9526\n",
      "Epoch 797/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 797: val_loss did not improve from 0.11530\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 798/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 798: val_loss improved from 0.11530 to 0.11530, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 799/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9692\n",
      "Epoch 799: val_loss improved from 0.11530 to 0.11530, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 800/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 800: val_loss improved from 0.11530 to 0.11530, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 801/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 801: val_loss improved from 0.11530 to 0.11530, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 802/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 802: val_loss improved from 0.11530 to 0.11529, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 803/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 803: val_loss improved from 0.11529 to 0.11529, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 804/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 804: val_loss improved from 0.11529 to 0.11528, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 805/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9692\n",
      "Epoch 805: val_loss improved from 0.11528 to 0.11528, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 806/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9692\n",
      "Epoch 806: val_loss improved from 0.11528 to 0.11527, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 807/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9692\n",
      "Epoch 807: val_loss improved from 0.11527 to 0.11527, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 808/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 808: val_loss improved from 0.11527 to 0.11526, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9693 - val_loss: 0.1153 - val_accuracy: 0.9527\n",
      "Epoch 809/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 809: val_loss improved from 0.11526 to 0.11525, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9526\n",
      "Epoch 810/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9692\n",
      "Epoch 810: val_loss improved from 0.11525 to 0.11525, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1152 - val_accuracy: 0.9526\n",
      "Epoch 811/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9692\n",
      "Epoch 811: val_loss improved from 0.11525 to 0.11525, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1152 - val_accuracy: 0.9526\n",
      "Epoch 812/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9692\n",
      "Epoch 812: val_loss improved from 0.11525 to 0.11524, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 813/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 813: val_loss improved from 0.11524 to 0.11523, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1152 - val_accuracy: 0.9526\n",
      "Epoch 814/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9692\n",
      "Epoch 814: val_loss improved from 0.11523 to 0.11523, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1152 - val_accuracy: 0.9526\n",
      "Epoch 815/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9692\n",
      "Epoch 815: val_loss improved from 0.11523 to 0.11523, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 816/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9693\n",
      "Epoch 816: val_loss improved from 0.11523 to 0.11522, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 817/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 817: val_loss improved from 0.11522 to 0.11522, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 818/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9692\n",
      "Epoch 818: val_loss did not improve from 0.11522\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 819/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9692\n",
      "Epoch 819: val_loss did not improve from 0.11522\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 820/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 820: val_loss did not improve from 0.11522\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 821/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 821: val_loss did not improve from 0.11522\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 822/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9694\n",
      "Epoch 822: val_loss improved from 0.11522 to 0.11522, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 823/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 823: val_loss improved from 0.11522 to 0.11522, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 824/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 824: val_loss improved from 0.11522 to 0.11521, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 825/1000\n",
      "984/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9694\n",
      "Epoch 825: val_loss improved from 0.11521 to 0.11520, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 826/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 826: val_loss improved from 0.11520 to 0.11520, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 827/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 827: val_loss did not improve from 0.11520\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 828/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9695\n",
      "Epoch 828: val_loss improved from 0.11520 to 0.11519, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 829/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 829: val_loss improved from 0.11519 to 0.11519, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 830/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 830: val_loss improved from 0.11519 to 0.11518, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 831/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 831: val_loss improved from 0.11518 to 0.11518, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 832/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 832: val_loss did not improve from 0.11518\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 833/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 833: val_loss improved from 0.11518 to 0.11518, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 834/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9692\n",
      "Epoch 834: val_loss did not improve from 0.11518\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 835/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 835: val_loss did not improve from 0.11518\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 836/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9692\n",
      "Epoch 836: val_loss did not improve from 0.11518\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 837/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 837: val_loss did not improve from 0.11518\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 838/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9692\n",
      "Epoch 838: val_loss improved from 0.11518 to 0.11517, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 839/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 839: val_loss improved from 0.11517 to 0.11517, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 840/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9693\n",
      "Epoch 840: val_loss did not improve from 0.11517\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 841/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 841: val_loss did not improve from 0.11517\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 842/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9692\n",
      "Epoch 842: val_loss improved from 0.11517 to 0.11517, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 843/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 843: val_loss improved from 0.11517 to 0.11517, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 844/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 844: val_loss improved from 0.11517 to 0.11516, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 845/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 845: val_loss did not improve from 0.11516\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 846/1000\n",
      "984/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 846: val_loss did not improve from 0.11516\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 847/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 847: val_loss improved from 0.11516 to 0.11515, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1152 - val_accuracy: 0.9527\n",
      "Epoch 848/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9692\n",
      "Epoch 848: val_loss improved from 0.11515 to 0.11515, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9527\n",
      "Epoch 849/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 849: val_loss improved from 0.11515 to 0.11514, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0723 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9527\n",
      "Epoch 850/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 850: val_loss improved from 0.11514 to 0.11513, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9527\n",
      "Epoch 851/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 851: val_loss did not improve from 0.11513\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9527\n",
      "Epoch 852/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 852: val_loss did not improve from 0.11513\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9527\n",
      "Epoch 853/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 853: val_loss improved from 0.11513 to 0.11513, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9527\n",
      "Epoch 854/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 854: val_loss did not improve from 0.11513\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9527\n",
      "Epoch 855/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 855: val_loss did not improve from 0.11513\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9527\n",
      "Epoch 856/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 856: val_loss did not improve from 0.11513\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9527\n",
      "Epoch 857/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9693\n",
      "Epoch 857: val_loss improved from 0.11513 to 0.11512, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9527\n",
      "Epoch 858/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 858: val_loss improved from 0.11512 to 0.11512, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9527\n",
      "Epoch 859/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 859: val_loss did not improve from 0.11512\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9527\n",
      "Epoch 860/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 860: val_loss improved from 0.11512 to 0.11512, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9528\n",
      "Epoch 861/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 861: val_loss improved from 0.11512 to 0.11512, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9528\n",
      "Epoch 862/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9694\n",
      "Epoch 862: val_loss improved from 0.11512 to 0.11511, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9528\n",
      "Epoch 863/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 863: val_loss improved from 0.11511 to 0.11510, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9528\n",
      "Epoch 864/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 864: val_loss improved from 0.11510 to 0.11510, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9528\n",
      "Epoch 865/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9693\n",
      "Epoch 865: val_loss improved from 0.11510 to 0.11510, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9528\n",
      "Epoch 866/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 866: val_loss did not improve from 0.11510\n",
      "995/995 [==============================] - 5s 5ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9528\n",
      "Epoch 867/1000\n",
      "983/995 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9693\n",
      "Epoch 867: val_loss did not improve from 0.11510\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9528\n",
      "Epoch 868/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 868: val_loss did not improve from 0.11510\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9528\n",
      "Epoch 869/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9692\n",
      "Epoch 869: val_loss improved from 0.11510 to 0.11509, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9529\n",
      "Epoch 870/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9694\n",
      "Epoch 870: val_loss did not improve from 0.11509\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9529\n",
      "Epoch 871/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 871: val_loss improved from 0.11509 to 0.11508, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9529\n",
      "Epoch 872/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 872: val_loss improved from 0.11508 to 0.11508, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9529\n",
      "Epoch 873/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 873: val_loss improved from 0.11508 to 0.11508, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9529\n",
      "Epoch 874/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 874: val_loss did not improve from 0.11508\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9529\n",
      "Epoch 875/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9693\n",
      "Epoch 875: val_loss improved from 0.11508 to 0.11508, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9529\n",
      "Epoch 876/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 876: val_loss did not improve from 0.11508\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9529\n",
      "Epoch 877/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 877: val_loss improved from 0.11508 to 0.11508, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9530\n",
      "Epoch 878/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 878: val_loss did not improve from 0.11508\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9530\n",
      "Epoch 879/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 879: val_loss improved from 0.11508 to 0.11508, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9530\n",
      "Epoch 880/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 880: val_loss did not improve from 0.11508\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9530\n",
      "Epoch 881/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9693\n",
      "Epoch 881: val_loss did not improve from 0.11508\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9530\n",
      "Epoch 882/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 882: val_loss improved from 0.11508 to 0.11507, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9530\n",
      "Epoch 883/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9694\n",
      "Epoch 883: val_loss improved from 0.11507 to 0.11507, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9530\n",
      "Epoch 884/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 884: val_loss improved from 0.11507 to 0.11506, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1151 - val_accuracy: 0.9530\n",
      "Epoch 885/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 885: val_loss did not improve from 0.11506\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1151 - val_accuracy: 0.9530\n",
      "Epoch 886/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9693\n",
      "Epoch 886: val_loss improved from 0.11506 to 0.11506, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9530\n",
      "Epoch 887/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9693\n",
      "Epoch 887: val_loss did not improve from 0.11506\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9530\n",
      "Epoch 888/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 888: val_loss improved from 0.11506 to 0.11505, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9530\n",
      "Epoch 889/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 889: val_loss improved from 0.11505 to 0.11505, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1151 - val_accuracy: 0.9530\n",
      "Epoch 890/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 890: val_loss improved from 0.11505 to 0.11505, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9530\n",
      "Epoch 891/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 891: val_loss improved from 0.11505 to 0.11504, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9693 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 892/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9693\n",
      "Epoch 892: val_loss did not improve from 0.11504\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 893/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9694\n",
      "Epoch 893: val_loss improved from 0.11504 to 0.11503, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9693 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 894/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 894: val_loss did not improve from 0.11503\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 895/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9692\n",
      "Epoch 895: val_loss did not improve from 0.11503\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 896/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 896: val_loss improved from 0.11503 to 0.11503, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9693 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 897/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9693\n",
      "Epoch 897: val_loss improved from 0.11503 to 0.11503, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9531\n",
      "Epoch 898/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9694\n",
      "Epoch 898: val_loss improved from 0.11503 to 0.11503, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9531\n",
      "Epoch 899/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 899: val_loss improved from 0.11503 to 0.11502, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9531\n",
      "Epoch 900/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 900: val_loss improved from 0.11502 to 0.11502, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9531\n",
      "Epoch 901/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9693\n",
      "Epoch 901: val_loss did not improve from 0.11502\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9693 - val_loss: 0.1150 - val_accuracy: 0.9531\n",
      "Epoch 902/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9693\n",
      "Epoch 902: val_loss did not improve from 0.11502\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9693 - val_loss: 0.1150 - val_accuracy: 0.9531\n",
      "Epoch 903/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9695\n",
      "Epoch 903: val_loss improved from 0.11502 to 0.11502, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9531\n",
      "Epoch 904/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 904: val_loss improved from 0.11502 to 0.11502, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9531\n",
      "Epoch 905/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 905: val_loss improved from 0.11502 to 0.11501, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9531\n",
      "Epoch 906/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 906: val_loss did not improve from 0.11501\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9531\n",
      "Epoch 907/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 907: val_loss improved from 0.11501 to 0.11501, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 908/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9694\n",
      "Epoch 908: val_loss improved from 0.11501 to 0.11501, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 909/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9693\n",
      "Epoch 909: val_loss improved from 0.11501 to 0.11501, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 910/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 910: val_loss improved from 0.11501 to 0.11500, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 911/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9694\n",
      "Epoch 911: val_loss improved from 0.11500 to 0.11499, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 912/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9694\n",
      "Epoch 912: val_loss improved from 0.11499 to 0.11499, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 913/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9692\n",
      "Epoch 913: val_loss improved from 0.11499 to 0.11499, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9693 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 914/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 914: val_loss did not improve from 0.11499\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 915/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 915: val_loss did not improve from 0.11499\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 916/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 916: val_loss improved from 0.11499 to 0.11499, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 917/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9694\n",
      "Epoch 917: val_loss improved from 0.11499 to 0.11498, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 918/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9694\n",
      "Epoch 918: val_loss improved from 0.11498 to 0.11498, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0721 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 919/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 919: val_loss improved from 0.11498 to 0.11498, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 920/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 920: val_loss improved from 0.11498 to 0.11498, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 921/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9694\n",
      "Epoch 921: val_loss improved from 0.11498 to 0.11497, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 922/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9694\n",
      "Epoch 922: val_loss improved from 0.11497 to 0.11497, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 923/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9694\n",
      "Epoch 923: val_loss improved from 0.11497 to 0.11497, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 924/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 924: val_loss did not improve from 0.11497\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 925/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9694\n",
      "Epoch 925: val_loss did not improve from 0.11497\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 926/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 926: val_loss did not improve from 0.11497\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 927/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9694\n",
      "Epoch 927: val_loss improved from 0.11497 to 0.11497, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 928/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9694\n",
      "Epoch 928: val_loss improved from 0.11497 to 0.11496, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 929/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 929: val_loss improved from 0.11496 to 0.11496, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 930/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 930: val_loss did not improve from 0.11496\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 931/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9696\n",
      "Epoch 931: val_loss did not improve from 0.11496\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 932/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 932: val_loss did not improve from 0.11496\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 933/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 933: val_loss improved from 0.11496 to 0.11495, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9530\n",
      "Epoch 934/1000\n",
      "984/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 934: val_loss did not improve from 0.11495\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9531\n",
      "Epoch 935/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9694\n",
      "Epoch 935: val_loss did not improve from 0.11495\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9694 - val_loss: 0.1150 - val_accuracy: 0.9531\n",
      "Epoch 936/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 936: val_loss did not improve from 0.11495\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9531\n",
      "Epoch 937/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 937: val_loss improved from 0.11495 to 0.11495, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9531\n",
      "Epoch 938/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 938: val_loss did not improve from 0.11495\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9532\n",
      "Epoch 939/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9694\n",
      "Epoch 939: val_loss did not improve from 0.11495\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9532\n",
      "Epoch 940/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 940: val_loss improved from 0.11495 to 0.11495, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 941/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 941: val_loss improved from 0.11495 to 0.11495, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 942/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 942: val_loss improved from 0.11495 to 0.11494, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 943/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9694\n",
      "Epoch 943: val_loss did not improve from 0.11494\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9694 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 944/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9696\n",
      "Epoch 944: val_loss did not improve from 0.11494\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 945/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 945: val_loss did not improve from 0.11494\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 946/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 946: val_loss did not improve from 0.11494\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 947/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 947: val_loss improved from 0.11494 to 0.11494, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 948/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9696\n",
      "Epoch 948: val_loss improved from 0.11494 to 0.11493, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 949/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 949: val_loss improved from 0.11493 to 0.11493, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 950/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9696\n",
      "Epoch 950: val_loss improved from 0.11493 to 0.11493, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 951/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9696\n",
      "Epoch 951: val_loss did not improve from 0.11493\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 952/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 952: val_loss improved from 0.11493 to 0.11493, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 953/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 953: val_loss improved from 0.11493 to 0.11492, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 954/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9696\n",
      "Epoch 954: val_loss improved from 0.11492 to 0.11492, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0720 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 955/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 955: val_loss improved from 0.11492 to 0.11491, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
      "Epoch 956/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9696\n",
      "Epoch 956: val_loss improved from 0.11491 to 0.11491, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9533\n",
      "Epoch 957/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 957: val_loss did not improve from 0.11491\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9533\n",
      "Epoch 958/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 958: val_loss improved from 0.11491 to 0.11490, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9533\n",
      "Epoch 959/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9696\n",
      "Epoch 959: val_loss improved from 0.11490 to 0.11490, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9696 - val_loss: 0.1149 - val_accuracy: 0.9533\n",
      "Epoch 960/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9696\n",
      "Epoch 960: val_loss improved from 0.11490 to 0.11490, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9533\n",
      "Epoch 961/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 961: val_loss improved from 0.11490 to 0.11489, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 962/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9696\n",
      "Epoch 962: val_loss improved from 0.11489 to 0.11489, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9696 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 963/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9696\n",
      "Epoch 963: val_loss improved from 0.11489 to 0.11489, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9696 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 964/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 964: val_loss improved from 0.11489 to 0.11489, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 965/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 965: val_loss did not improve from 0.11489\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9696 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 966/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9696\n",
      "Epoch 966: val_loss improved from 0.11489 to 0.11488, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9696 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 967/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 967: val_loss improved from 0.11488 to 0.11488, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9696 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 968/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9696\n",
      "Epoch 968: val_loss improved from 0.11488 to 0.11487, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9696 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 969/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 969: val_loss improved from 0.11487 to 0.11486, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9696 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 970/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 970: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 971/1000\n",
      "989/995 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9696\n",
      "Epoch 971: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 972/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 972: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 973/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9695\n",
      "Epoch 973: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 974/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9696\n",
      "Epoch 974: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 975/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9696\n",
      "Epoch 975: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9696 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 976/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 976: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 5s 5ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 977/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 977: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 978/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9696\n",
      "Epoch 978: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 979/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 979: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 980/1000\n",
      "992/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 980: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 981/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9696\n",
      "Epoch 981: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 982/1000\n",
      "991/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 982: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 983/1000\n",
      "985/995 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9695\n",
      "Epoch 983: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 984/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 984: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 985/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9695\n",
      "Epoch 985: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 986/1000\n",
      "983/995 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9696\n",
      "Epoch 986: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9696 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 987/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9696\n",
      "Epoch 987: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9696 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 988/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9696\n",
      "Epoch 988: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 989/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9694\n",
      "Epoch 989: val_loss did not improve from 0.11486\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 990/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 990: val_loss improved from 0.11486 to 0.11486, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 991/1000\n",
      "995/995 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 991: val_loss improved from 0.11486 to 0.11485, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 7s 7ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 992/1000\n",
      "990/995 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9696\n",
      "Epoch 992: val_loss improved from 0.11485 to 0.11484, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1148 - val_accuracy: 0.9534\n",
      "Epoch 993/1000\n",
      "994/995 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9695\n",
      "Epoch 993: val_loss improved from 0.11484 to 0.11484, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 0.1148 - val_accuracy: 0.9534\n",
      "Epoch 994/1000\n",
      "993/995 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9696\n",
      "Epoch 994: val_loss did not improve from 0.11484\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9696 - val_loss: 0.1148 - val_accuracy: 0.9534\n",
      "Epoch 995/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 995: val_loss improved from 0.11484 to 0.11484, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0718 - accuracy: 0.9696 - val_loss: 0.1148 - val_accuracy: 0.9533\n",
      "Epoch 996/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9696\n",
      "Epoch 996: val_loss improved from 0.11484 to 0.11484, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0718 - accuracy: 0.9696 - val_loss: 0.1148 - val_accuracy: 0.9533\n",
      "Epoch 997/1000\n",
      "988/995 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9695\n",
      "Epoch 997: val_loss improved from 0.11484 to 0.11483, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0718 - accuracy: 0.9696 - val_loss: 0.1148 - val_accuracy: 0.9533\n",
      "Epoch 998/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9696\n",
      "Epoch 998: val_loss improved from 0.11483 to 0.11483, saving model to training/best_params.h5\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0718 - accuracy: 0.9696 - val_loss: 0.1148 - val_accuracy: 0.9533\n",
      "Epoch 999/1000\n",
      "987/995 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9697\n",
      "Epoch 999: val_loss did not improve from 0.11483\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0718 - accuracy: 0.9696 - val_loss: 0.1148 - val_accuracy: 0.9533\n",
      "Epoch 1000/1000\n",
      "986/995 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9695\n",
      "Epoch 1000: val_loss did not improve from 0.11483\n",
      "995/995 [==============================] - 6s 6ms/step - loss: 0.0718 - accuracy: 0.9696 - val_loss: 0.1148 - val_accuracy: 0.9533\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1000\n",
    "BATCH_SIZE = 64\n",
    "checkpointer = ModelCheckpoint(filepath = \"training/best_params.h5\", verbose = 1, save_best_only = True)\n",
    "history = model.fit([xi_train, xj_train], pij_train,\n",
    "                     epochs = NUM_EPOCHS, batch_size = BATCH_SIZE, validation_data=([xi_test, xj_test], pij_test),\n",
    "                     callbacks = [checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"training/best_params.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"store\": \"thesis-ltr\",\n",
      "  \"class\": \"org.apache.solr.ltr.model.NeuralNetworkModel\",\n",
      "  \"name\": \"ranknet-e1000-b64-l128-l64-l32-adadelta\",\n",
      "  \"features\": [\n",
      "    {\n",
      "      \"name\": \"title_coveredQueryTerms\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"22.0\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"headings_coveredQueryTerms\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"31.0\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"body_coveredQueryTerms\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"34.0\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"document_coveredQueryTerms\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"34.0\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"title_coveredQueryTermsRatio\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"1.0\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"headings_coveredQueryTermsRatio\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"1.0\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"body_coveredQueryTermsRatio\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"1.0\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"document_coveredQueryTermsRatio\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"1.0\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"title_tf\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"25.217281\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"headings_tf\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"71.3568\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"body_tf\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"529.04224\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"document_tf\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"530.54956\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"title_idf\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"8.369181\",\n",
      "          \"max\": \"238.08615\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"headings_idf\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"6.34288\",\n",
      "          \"max\": \"189.11295\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"body_idf\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"2.5495176\",\n",
      "          \"max\": \"123.85025\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"document_idf\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"2.5495176\",\n",
      "          \"max\": \"123.85024\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"title_tfidf\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"176.37538\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"headings_tfidf\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"354.528\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"body_tfidf\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"2197.0798\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"document_tfidf\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"2197.0798\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"title_bm25\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"38.983177\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"headings_bm25\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"68.29706\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"body_bm25\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"57.78668\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"document_bm25\",\n",
      "      \"norm\": {\n",
      "        \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
      "        \"params\": {\n",
      "          \"min\": \"0.0\",\n",
      "          \"max\": \"63.318443\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"params\": {\n",
      "    \"layers\": [\n",
      "      {\n",
      "        \"matrix\": [\n",
      "          [\n",
      "            0.07900212705135345,\n",
      "            -0.020203085616230965,\n",
      "            0.09546172618865967,\n",
      "            0.11539208889007568,\n",
      "            0.05472585931420326,\n",
      "            -0.08320226520299911,\n",
      "            -0.12150757014751434,\n",
      "            0.19093206524848938,\n",
      "            -0.006347529590129852,\n",
      "            0.09988147765398026,\n",
      "            0.15962181985378265,\n",
      "            -0.12598811089992523,\n",
      "            0.18917831778526306,\n",
      "            -0.07171177119016647,\n",
      "            -0.007298736833035946,\n",
      "            0.08176542818546295,\n",
      "            0.13607452809810638,\n",
      "            -0.0699777603149414,\n",
      "            -0.17132441699504852,\n",
      "            0.07826688885688782,\n",
      "            -0.159000426530838,\n",
      "            0.09477482736110687,\n",
      "            -0.09572070091962814,\n",
      "            -0.11680138856172562\n",
      "          ],\n",
      "          [\n",
      "            0.14608719944953918,\n",
      "            0.15369004011154175,\n",
      "            0.0752224326133728,\n",
      "            0.08463156223297119,\n",
      "            0.04923717677593231,\n",
      "            0.18098480999469757,\n",
      "            -0.023711560294032097,\n",
      "            0.11042478680610657,\n",
      "            -0.14115335047245026,\n",
      "            -0.023859499022364616,\n",
      "            0.08283369988203049,\n",
      "            -0.02708655595779419,\n",
      "            -0.06417899578809738,\n",
      "            0.1500428318977356,\n",
      "            0.053521350026130676,\n",
      "            -0.10732106864452362,\n",
      "            -0.16985218226909637,\n",
      "            0.18889205157756805,\n",
      "            0.16429229080677032,\n",
      "            0.1112261638045311,\n",
      "            0.06031704321503639,\n",
      "            0.14890196919441223,\n",
      "            0.13479644060134888,\n",
      "            0.10052741318941116\n",
      "          ],\n",
      "          [\n",
      "            0.03168322145938873,\n",
      "            0.036528330296278,\n",
      "            0.06310780346393585,\n",
      "            -0.010981349274516106,\n",
      "            0.026471227407455444,\n",
      "            0.1732388138771057,\n",
      "            -0.03823992237448692,\n",
      "            0.05576833710074425,\n",
      "            -0.05049978196620941,\n",
      "            -0.19069230556488037,\n",
      "            0.10757304728031158,\n",
      "            -0.08551084250211716,\n",
      "            0.004033846780657768,\n",
      "            0.04791352152824402,\n",
      "            -0.1265251487493515,\n",
      "            -0.02631043642759323,\n",
      "            -0.12117639929056168,\n",
      "            0.1032700315117836,\n",
      "            -0.09563842415809631,\n",
      "            0.2001560926437378,\n",
      "            0.22712531685829163,\n",
      "            0.15116161108016968,\n",
      "            0.22895778715610504,\n",
      "            0.49268171191215515\n",
      "          ],\n",
      "          [\n",
      "            0.14390398561954498,\n",
      "            -0.12696073949337006,\n",
      "            -0.07577642798423767,\n",
      "            0.08611781895160675,\n",
      "            0.1477736532688141,\n",
      "            0.13799498975276947,\n",
      "            -0.037709496915340424,\n",
      "            0.06413180381059647,\n",
      "            0.06892698258161545,\n",
      "            0.10635486990213394,\n",
      "            0.18220284581184387,\n",
      "            0.1646374762058258,\n",
      "            0.02974359132349491,\n",
      "            -0.052483830600976944,\n",
      "            0.02004518359899521,\n",
      "            -0.030566483736038208,\n",
      "            -0.08804681152105331,\n",
      "            -0.11534016579389572,\n",
      "            -0.025727130472660065,\n",
      "            -0.0667678564786911,\n",
      "            0.0909121111035347,\n",
      "            0.18550831079483032,\n",
      "            0.004355791490525007,\n",
      "            -0.0717814639210701\n",
      "          ],\n",
      "          [\n",
      "            0.07583726942539215,\n",
      "            0.1850363165140152,\n",
      "            -0.09632769227027893,\n",
      "            0.19573363661766052,\n",
      "            0.003720212960615754,\n",
      "            -0.029391206800937653,\n",
      "            -0.18924598395824432,\n",
      "            -0.10025268793106079,\n",
      "            0.14054448902606964,\n",
      "            -0.01034886296838522,\n",
      "            -0.013073891401290894,\n",
      "            -0.019494319334626198,\n",
      "            0.10093770176172256,\n",
      "            0.04372134804725647,\n",
      "            0.21585892140865326,\n",
      "            0.09158914536237717,\n",
      "            -0.10364272445440292,\n",
      "            -0.09853304922580719,\n",
      "            0.03220511972904205,\n",
      "            -0.009211447089910507,\n",
      "            0.008639346808195114,\n",
      "            0.09186755120754242,\n",
      "            -0.09284988045692444,\n",
      "            -0.019542422145605087\n",
      "          ],\n",
      "          [\n",
      "            0.008212230168282986,\n",
      "            -0.02145382948219776,\n",
      "            -0.09815800935029984,\n",
      "            0.15592722594738007,\n",
      "            0.13867144286632538,\n",
      "            0.04316357895731926,\n",
      "            0.0514521449804306,\n",
      "            0.08541131019592285,\n",
      "            -0.06607314199209213,\n",
      "            0.03671020269393921,\n",
      "            -0.17502905428409576,\n",
      "            0.044687818735837936,\n",
      "            -0.20088337361812592,\n",
      "            -0.1472480297088623,\n",
      "            -0.19364957511425018,\n",
      "            -0.15145570039749146,\n",
      "            0.03399699181318283,\n",
      "            -0.0722358375787735,\n",
      "            0.12811729311943054,\n",
      "            0.03562267869710922,\n",
      "            -0.005437908228486776,\n",
      "            -0.1058635413646698,\n",
      "            0.15230172872543335,\n",
      "            0.17806699872016907\n",
      "          ],\n",
      "          [\n",
      "            0.07417692989110947,\n",
      "            -0.18634863197803497,\n",
      "            0.17184150218963623,\n",
      "            0.2076941728591919,\n",
      "            0.09259278327226639,\n",
      "            0.0416802316904068,\n",
      "            0.14179222285747528,\n",
      "            0.06296873837709427,\n",
      "            -0.2054928094148636,\n",
      "            -0.0343552865087986,\n",
      "            -0.055347591638565063,\n",
      "            -0.13781490921974182,\n",
      "            -0.14253486692905426,\n",
      "            0.0785682275891304,\n",
      "            0.06352975964546204,\n",
      "            0.18685002624988556,\n",
      "            -0.12446475774049759,\n",
      "            -0.07972695678472519,\n",
      "            -0.07218662649393082,\n",
      "            0.09825123101472855,\n",
      "            -0.06172840669751167,\n",
      "            0.1732972115278244,\n",
      "            0.06169167533516884,\n",
      "            0.08455553650856018\n",
      "          ],\n",
      "          [\n",
      "            0.21094930171966553,\n",
      "            -0.10377819091081619,\n",
      "            0.2204136997461319,\n",
      "            0.16090576350688934,\n",
      "            0.10617920011281967,\n",
      "            0.14273697137832642,\n",
      "            0.09579100459814072,\n",
      "            0.2477007657289505,\n",
      "            0.14277245104312897,\n",
      "            -0.03393000364303589,\n",
      "            -0.10099723190069199,\n",
      "            -0.10115891695022583,\n",
      "            -0.11002715677022934,\n",
      "            -0.1641838401556015,\n",
      "            -0.08458446711301804,\n",
      "            0.16034473478794098,\n",
      "            -0.08796022087335587,\n",
      "            -0.040832746773958206,\n",
      "            -0.19325612485408783,\n",
      "            0.11075963079929352,\n",
      "            -0.09392263740301132,\n",
      "            0.0254762414842844,\n",
      "            -0.2904057204723358,\n",
      "            -0.03457789868116379\n",
      "          ],\n",
      "          [\n",
      "            -0.01690373755991459,\n",
      "            -0.011162796057760715,\n",
      "            0.15790604054927826,\n",
      "            -0.08968941867351532,\n",
      "            -0.09078091382980347,\n",
      "            0.15893800556659698,\n",
      "            -0.04220349341630936,\n",
      "            0.09021203964948654,\n",
      "            0.18442261219024658,\n",
      "            0.019271325320005417,\n",
      "            0.12484368681907654,\n",
      "            -0.04560026898980141,\n",
      "            -0.045856621116399765,\n",
      "            0.017287911847233772,\n",
      "            0.19035300612449646,\n",
      "            -0.03649581968784332,\n",
      "            -0.09730960428714752,\n",
      "            -0.11007726192474365,\n",
      "            0.11226138472557068,\n",
      "            0.10610654950141907,\n",
      "            -0.06473781913518906,\n",
      "            0.006933161988854408,\n",
      "            0.00777019327506423,\n",
      "            -0.18032464385032654\n",
      "          ],\n",
      "          [\n",
      "            -0.1926136612892151,\n",
      "            -0.03607906773686409,\n",
      "            0.10323446244001389,\n",
      "            0.15419425070285797,\n",
      "            -0.08989690989255905,\n",
      "            0.08851677179336548,\n",
      "            0.14297272264957428,\n",
      "            0.05699615180492401,\n",
      "            -0.036528587341308594,\n",
      "            0.10968945175409317,\n",
      "            -0.14259156584739685,\n",
      "            -0.07703201472759247,\n",
      "            0.11107683926820755,\n",
      "            0.0032249295618385077,\n",
      "            -0.17027068138122559,\n",
      "            -0.10568767786026001,\n",
      "            0.027643611654639244,\n",
      "            -0.031570084393024445,\n",
      "            -0.1698206216096878,\n",
      "            -0.06804652512073517,\n",
      "            0.0941673219203949,\n",
      "            -0.17274713516235352,\n",
      "            0.15690788626670837,\n",
      "            0.09759814292192459\n",
      "          ],\n",
      "          [\n",
      "            0.0482649952173233,\n",
      "            0.12779752910137177,\n",
      "            -0.1339542269706726,\n",
      "            -0.032511547207832336,\n",
      "            -0.06979745626449585,\n",
      "            -0.05431880056858063,\n",
      "            -0.15668915212154388,\n",
      "            -0.13931748270988464,\n",
      "            -0.10275480151176453,\n",
      "            0.0035698264837265015,\n",
      "            -0.12166956067085266,\n",
      "            0.11839337646961212,\n",
      "            -0.10607851296663284,\n",
      "            0.049145739525556564,\n",
      "            0.035607583820819855,\n",
      "            -0.02106470800936222,\n",
      "            0.0504780113697052,\n",
      "            -0.08087640255689621,\n",
      "            0.1467718929052353,\n",
      "            0.02146737277507782,\n",
      "            0.16439495980739594,\n",
      "            -0.18356043100357056,\n",
      "            -0.054608915001153946,\n",
      "            0.15546093881130219\n",
      "          ],\n",
      "          [\n",
      "            0.16492988169193268,\n",
      "            0.0644342452287674,\n",
      "            -0.15549643337726593,\n",
      "            0.04548681154847145,\n",
      "            0.004905658308416605,\n",
      "            0.06595149636268616,\n",
      "            0.016533026471734047,\n",
      "            -0.136488676071167,\n",
      "            -0.022903483361005783,\n",
      "            -0.08750732243061066,\n",
      "            -0.1945025771856308,\n",
      "            -0.10262557864189148,\n",
      "            0.06235599145293236,\n",
      "            0.08421381562948227,\n",
      "            -0.01799984648823738,\n",
      "            0.006184480153024197,\n",
      "            -0.015908772125840187,\n",
      "            -0.0006573356804437935,\n",
      "            0.14836004376411438,\n",
      "            -0.1742151826620102,\n",
      "            -0.04973486810922623,\n",
      "            0.17274895310401917,\n",
      "            -0.19335755705833435,\n",
      "            0.11412227898836136\n",
      "          ],\n",
      "          [\n",
      "            0.017629902809858322,\n",
      "            0.04476625472307205,\n",
      "            -0.21052956581115723,\n",
      "            0.14541655778884888,\n",
      "            -0.1182859018445015,\n",
      "            0.15656976401805878,\n",
      "            0.19183222949504852,\n",
      "            0.0667724758386612,\n",
      "            0.1767563819885254,\n",
      "            0.06312365084886551,\n",
      "            -0.07752760499715805,\n",
      "            0.03366627171635628,\n",
      "            -0.007114836480468512,\n",
      "            0.01555244717746973,\n",
      "            -0.17457923293113708,\n",
      "            -0.20682618021965027,\n",
      "            0.09100556373596191,\n",
      "            -0.011826406233012676,\n",
      "            -0.03613251447677612,\n",
      "            -0.15285292267799377,\n",
      "            0.11551392823457718,\n",
      "            0.07859393209218979,\n",
      "            0.4315940737724304,\n",
      "            0.31118538975715637\n",
      "          ],\n",
      "          [\n",
      "            0.086126409471035,\n",
      "            -0.19568341970443726,\n",
      "            -0.0942937433719635,\n",
      "            -0.11591511964797974,\n",
      "            0.13047410547733307,\n",
      "            0.024760769680142403,\n",
      "            0.06084972992539406,\n",
      "            -0.030593786388635635,\n",
      "            -0.15854805707931519,\n",
      "            0.015303426422178745,\n",
      "            0.054834574460983276,\n",
      "            0.06685439497232437,\n",
      "            -0.05653666332364082,\n",
      "            -0.10028817504644394,\n",
      "            0.05914997681975365,\n",
      "            -0.16794393956661224,\n",
      "            -0.0003474604163784534,\n",
      "            -0.11772625148296356,\n",
      "            0.17951378226280212,\n",
      "            -0.035961881279945374,\n",
      "            0.15408244729042053,\n",
      "            -0.12555809319019318,\n",
      "            -0.045031823217868805,\n",
      "            -0.15936188399791718\n",
      "          ],\n",
      "          [\n",
      "            0.14548976719379425,\n",
      "            -0.15497809648513794,\n",
      "            0.048149190843105316,\n",
      "            0.12585890293121338,\n",
      "            -0.16227956116199493,\n",
      "            0.1559542715549469,\n",
      "            -0.13012196123600006,\n",
      "            0.09527895599603653,\n",
      "            0.1307957023382187,\n",
      "            -0.06568735092878342,\n",
      "            0.01173084881156683,\n",
      "            -0.08202875405550003,\n",
      "            -0.07973512262105942,\n",
      "            -0.18104372918605804,\n",
      "            0.004436433780938387,\n",
      "            0.10151275247335434,\n",
      "            -0.13429337739944458,\n",
      "            0.044334862381219864,\n",
      "            -0.18499872088432312,\n",
      "            -0.017022183164954185,\n",
      "            0.12428345531225204,\n",
      "            -0.0811728835105896,\n",
      "            0.04504808783531189,\n",
      "            -0.08353959023952484\n",
      "          ],\n",
      "          [\n",
      "            0.03842897713184357,\n",
      "            0.19224052131175995,\n",
      "            -0.18367880582809448,\n",
      "            0.09343363344669342,\n",
      "            -0.01856735348701477,\n",
      "            -0.19820493459701538,\n",
      "            0.012103572487831116,\n",
      "            -0.14449496567249298,\n",
      "            -0.058493807911872864,\n",
      "            0.09167556464672089,\n",
      "            -0.1460924744606018,\n",
      "            -0.080246202647686,\n",
      "            0.0784984678030014,\n",
      "            -0.09124106168746948,\n",
      "            -0.18698649108409882,\n",
      "            -0.10133643448352814,\n",
      "            0.09522028267383575,\n",
      "            0.11987800896167755,\n",
      "            -0.19697973132133484,\n",
      "            -0.19788727164268494,\n",
      "            0.09643034636974335,\n",
      "            -0.08419828861951828,\n",
      "            -0.09526112675666809,\n",
      "            0.057275041937828064\n",
      "          ],\n",
      "          [\n",
      "            0.023810094222426414,\n",
      "            -0.015707440674304962,\n",
      "            0.19494125247001648,\n",
      "            0.08209262788295746,\n",
      "            0.11144117265939713,\n",
      "            -0.07450573146343231,\n",
      "            0.023941630497574806,\n",
      "            -0.005633438937366009,\n",
      "            0.15460167825222015,\n",
      "            -0.14622680842876434,\n",
      "            -0.09983406960964203,\n",
      "            -0.07521472871303558,\n",
      "            0.167644664645195,\n",
      "            0.12013822048902512,\n",
      "            0.0964583083987236,\n",
      "            -0.014771277084946632,\n",
      "            -0.1770554482936859,\n",
      "            -0.03645574674010277,\n",
      "            0.1297629028558731,\n",
      "            -0.04318397492170334,\n",
      "            0.06429900228977203,\n",
      "            -0.07998180389404297,\n",
      "            -0.18650558590888977,\n",
      "            -0.12968648970127106\n",
      "          ],\n",
      "          [\n",
      "            -0.08433772623538971,\n",
      "            0.19139321148395538,\n",
      "            -0.09407387673854828,\n",
      "            -0.15157338976860046,\n",
      "            -0.1587308645248413,\n",
      "            -0.1930689811706543,\n",
      "            0.239716574549675,\n",
      "            0.06536541879177094,\n",
      "            -0.10178332030773163,\n",
      "            -0.07157167792320251,\n",
      "            -0.03668607026338577,\n",
      "            -0.06910651922225952,\n",
      "            0.012358459644019604,\n",
      "            -0.15008090436458588,\n",
      "            -0.1831737905740738,\n",
      "            0.13415689766407013,\n",
      "            0.041470203548669815,\n",
      "            -0.17289607226848602,\n",
      "            -0.07432670146226883,\n",
      "            0.004972182214260101,\n",
      "            0.11277041584253311,\n",
      "            -0.05832711234688759,\n",
      "            0.04991742596030235,\n",
      "            -0.16751612722873688\n",
      "          ],\n",
      "          [\n",
      "            -0.14170575141906738,\n",
      "            -0.03856077045202255,\n",
      "            -0.05641308054327965,\n",
      "            -0.1638171225786209,\n",
      "            -0.139900803565979,\n",
      "            -0.18371416628360748,\n",
      "            -0.03286423906683922,\n",
      "            0.03943517431616783,\n",
      "            0.026781905442476273,\n",
      "            0.14905276894569397,\n",
      "            -0.030705256387591362,\n",
      "            0.14218191802501678,\n",
      "            -0.11645082384347916,\n",
      "            0.09560666233301163,\n",
      "            -0.16040243208408356,\n",
      "            0.11036467552185059,\n",
      "            0.09038929641246796,\n",
      "            0.08576071262359619,\n",
      "            0.10339425504207611,\n",
      "            -0.18691641092300415,\n",
      "            -0.16158942878246307,\n",
      "            -0.024506598711013794,\n",
      "            0.1340707391500473,\n",
      "            0.04926098510622978\n",
      "          ],\n",
      "          [\n",
      "            0.06068652868270874,\n",
      "            -0.13879866898059845,\n",
      "            0.13698558509349823,\n",
      "            -0.17758722603321075,\n",
      "            -0.13683061301708221,\n",
      "            -0.054588817059993744,\n",
      "            0.14766854047775269,\n",
      "            -0.13884708285331726,\n",
      "            -0.13897483050823212,\n",
      "            0.11619684845209122,\n",
      "            -0.07290148735046387,\n",
      "            0.10702411830425262,\n",
      "            0.03672110661864281,\n",
      "            -0.14612580835819244,\n",
      "            -0.09021782875061035,\n",
      "            0.2304735779762268,\n",
      "            -0.1368129551410675,\n",
      "            0.097342848777771,\n",
      "            -0.12103894352912903,\n",
      "            0.10935325920581818,\n",
      "            0.10274551063776016,\n",
      "            -0.21714037656784058,\n",
      "            0.2301253080368042,\n",
      "            0.014995942823588848\n",
      "          ],\n",
      "          [\n",
      "            0.042965807020664215,\n",
      "            0.10761342942714691,\n",
      "            -0.2224385142326355,\n",
      "            -0.1915920376777649,\n",
      "            -0.1838400512933731,\n",
      "            0.06432149559259415,\n",
      "            0.01613898202776909,\n",
      "            0.11174260824918747,\n",
      "            -0.17233377695083618,\n",
      "            -0.09809339046478271,\n",
      "            0.09237124770879745,\n",
      "            -0.14853376150131226,\n",
      "            -0.16874505579471588,\n",
      "            0.06420272588729858,\n",
      "            0.019835853949189186,\n",
      "            0.09051252156496048,\n",
      "            0.07589330524206161,\n",
      "            0.16297714412212372,\n",
      "            0.05233883112668991,\n",
      "            0.060018058866262436,\n",
      "            -0.1683546006679535,\n",
      "            0.053296636790037155,\n",
      "            0.1368384063243866,\n",
      "            0.12685133516788483\n",
      "          ],\n",
      "          [\n",
      "            -0.015079065226018429,\n",
      "            0.04972831904888153,\n",
      "            0.111191026866436,\n",
      "            0.024299584329128265,\n",
      "            0.193557471036911,\n",
      "            -0.01611853763461113,\n",
      "            -0.029562747105956078,\n",
      "            -0.10256291925907135,\n",
      "            0.17281721532344818,\n",
      "            -0.13421107828617096,\n",
      "            0.1327824592590332,\n",
      "            0.044953852891922,\n",
      "            -0.09674905240535736,\n",
      "            -0.02992582507431507,\n",
      "            0.009095276705920696,\n",
      "            -0.08840174973011017,\n",
      "            0.15504498779773712,\n",
      "            -0.061417412012815475,\n",
      "            0.03625299036502838,\n",
      "            -0.17361707985401154,\n",
      "            -0.21242523193359375,\n",
      "            0.05641745403409004,\n",
      "            0.12464234232902527,\n",
      "            0.005032530520111322\n",
      "          ],\n",
      "          [\n",
      "            0.09495764970779419,\n",
      "            -0.10416386276483536,\n",
      "            -0.11101584136486053,\n",
      "            -0.0023876805789768696,\n",
      "            -0.11715925484895706,\n",
      "            0.1258503496646881,\n",
      "            -0.17799443006515503,\n",
      "            -0.11473803222179413,\n",
      "            0.15569806098937988,\n",
      "            0.16902942955493927,\n",
      "            -0.012342747300863266,\n",
      "            0.18273498117923737,\n",
      "            0.010699319653213024,\n",
      "            0.10912801325321198,\n",
      "            0.1045500859618187,\n",
      "            0.10786779969930649,\n",
      "            -0.1538207083940506,\n",
      "            0.022579992190003395,\n",
      "            0.035221751779317856,\n",
      "            -0.009512826800346375,\n",
      "            -0.007122837007045746,\n",
      "            0.15085704624652863,\n",
      "            -0.11543814092874527,\n",
      "            -0.03328457847237587\n",
      "          ],\n",
      "          [\n",
      "            -0.06051773577928543,\n",
      "            -0.18146276473999023,\n",
      "            -0.04007727652788162,\n",
      "            0.04200436919927597,\n",
      "            0.0024750293232500553,\n",
      "            -0.10861698538064957,\n",
      "            -0.02021113596856594,\n",
      "            -0.015659311786293983,\n",
      "            0.12607574462890625,\n",
      "            -0.15319140255451202,\n",
      "            -0.16496136784553528,\n",
      "            -0.17295411229133606,\n",
      "            -0.11397714912891388,\n",
      "            0.1813378632068634,\n",
      "            -0.04621858522295952,\n",
      "            -0.0658835619688034,\n",
      "            0.011219751089811325,\n",
      "            -0.022970274090766907,\n",
      "            0.05216575041413307,\n",
      "            0.008344200439751148,\n",
      "            -0.10905241966247559,\n",
      "            -0.052971914410591125,\n",
      "            -0.03885418549180031,\n",
      "            0.19384796917438507\n",
      "          ],\n",
      "          [\n",
      "            0.13588762283325195,\n",
      "            -0.10741642117500305,\n",
      "            0.03126031905412674,\n",
      "            -0.22257225215435028,\n",
      "            -0.0339135080575943,\n",
      "            -0.032782863825559616,\n",
      "            0.07329973578453064,\n",
      "            0.08545058965682983,\n",
      "            -0.181405171751976,\n",
      "            0.13536740839481354,\n",
      "            0.12795153260231018,\n",
      "            -0.086742103099823,\n",
      "            0.05379459261894226,\n",
      "            -0.195906400680542,\n",
      "            -0.20746253430843353,\n",
      "            0.11015096306800842,\n",
      "            -0.011350267566740513,\n",
      "            0.1778530329465866,\n",
      "            0.244838148355484,\n",
      "            0.09113471955060959,\n",
      "            0.10721561312675476,\n",
      "            0.1958225816488266,\n",
      "            0.2576124370098114,\n",
      "            0.4383556842803955\n",
      "          ],\n",
      "          [\n",
      "            0.007489231880754232,\n",
      "            -0.02866668812930584,\n",
      "            -0.1173422709107399,\n",
      "            -0.009892711415886879,\n",
      "            0.01911761797964573,\n",
      "            -0.12308984994888306,\n",
      "            0.108956478536129,\n",
      "            0.23317071795463562,\n",
      "            0.18593829870224,\n",
      "            -0.036428093910217285,\n",
      "            -0.11091246455907822,\n",
      "            -0.0917695164680481,\n",
      "            -0.03016537055373192,\n",
      "            -0.031004877761006355,\n",
      "            0.15081793069839478,\n",
      "            0.07943643629550934,\n",
      "            0.09975414723157883,\n",
      "            -0.03597194701433182,\n",
      "            -0.10113310813903809,\n",
      "            0.16641660034656525,\n",
      "            -0.017233330756425858,\n",
      "            0.15629549324512482,\n",
      "            0.4551834762096405,\n",
      "            0.45401135087013245\n",
      "          ],\n",
      "          [\n",
      "            0.12507697939872742,\n",
      "            -0.12610380351543427,\n",
      "            0.09036951512098312,\n",
      "            -0.17324590682983398,\n",
      "            -0.13782362639904022,\n",
      "            0.10569188743829727,\n",
      "            0.14505180716514587,\n",
      "            -0.07235570251941681,\n",
      "            -0.17162564396858215,\n",
      "            -0.12708140909671783,\n",
      "            -0.03376607224345207,\n",
      "            0.124407097697258,\n",
      "            0.07200123369693756,\n",
      "            -0.12808804214000702,\n",
      "            -0.07727319002151489,\n",
      "            -0.18658681213855743,\n",
      "            0.127546489238739,\n",
      "            0.10789186507463455,\n",
      "            -0.07929286360740662,\n",
      "            0.08056806772947311,\n",
      "            0.2280711978673935,\n",
      "            0.14025209844112396,\n",
      "            0.05726230517029762,\n",
      "            0.16250520944595337\n",
      "          ],\n",
      "          [\n",
      "            0.045940034091472626,\n",
      "            0.03575192764401436,\n",
      "            0.016673097386956215,\n",
      "            -0.019500529393553734,\n",
      "            0.1467701941728592,\n",
      "            0.16186057031154633,\n",
      "            -0.09792719036340714,\n",
      "            -0.07800853997468948,\n",
      "            0.11749164760112762,\n",
      "            -0.008159535005688667,\n",
      "            0.033988505601882935,\n",
      "            0.12716849148273468,\n",
      "            0.004410336259752512,\n",
      "            -0.2129465788602829,\n",
      "            0.01764816604554653,\n",
      "            -0.09153854846954346,\n",
      "            -0.05933746322989464,\n",
      "            -0.10930269211530685,\n",
      "            0.03564177826046944,\n",
      "            0.12587986886501312,\n",
      "            -0.02452975884079933,\n",
      "            0.034804292023181915,\n",
      "            0.12150556594133377,\n",
      "            0.04850903898477554\n",
      "          ],\n",
      "          [\n",
      "            -0.04405830428004265,\n",
      "            0.13917656242847443,\n",
      "            0.14198720455169678,\n",
      "            0.17621323466300964,\n",
      "            0.1997649222612381,\n",
      "            0.20988719165325165,\n",
      "            -0.003411287209019065,\n",
      "            -0.012256004847586155,\n",
      "            0.1857651174068451,\n",
      "            0.00800960324704647,\n",
      "            0.09326846152544022,\n",
      "            0.03371389955282211,\n",
      "            0.1361512392759323,\n",
      "            0.19651050865650177,\n",
      "            0.04361668601632118,\n",
      "            0.007129828445613384,\n",
      "            0.1392708271741867,\n",
      "            0.16021943092346191,\n",
      "            -0.01554984413087368,\n",
      "            -0.06253921240568161,\n",
      "            -0.02539348416030407,\n",
      "            -0.14238101243972778,\n",
      "            -0.0025510352570563555,\n",
      "            -0.10714522004127502\n",
      "          ],\n",
      "          [\n",
      "            0.013044118881225586,\n",
      "            0.07875051349401474,\n",
      "            -0.1946924775838852,\n",
      "            0.14973828196525574,\n",
      "            -0.12942352890968323,\n",
      "            -0.10930003225803375,\n",
      "            0.04201917722821236,\n",
      "            -0.1692790985107422,\n",
      "            0.07431501150131226,\n",
      "            -0.14222440123558044,\n",
      "            -0.06652440875768661,\n",
      "            0.15600845217704773,\n",
      "            0.21979478001594543,\n",
      "            0.1474825143814087,\n",
      "            0.03128638118505478,\n",
      "            -0.007234727963805199,\n",
      "            0.18918968737125397,\n",
      "            0.12812834978103638,\n",
      "            0.04457793012261391,\n",
      "            0.15596197545528412,\n",
      "            -0.08690901100635529,\n",
      "            0.13598233461380005,\n",
      "            0.22961704432964325,\n",
      "            -0.15758328139781952\n",
      "          ],\n",
      "          [\n",
      "            0.07189172506332397,\n",
      "            -0.22028616070747375,\n",
      "            0.14161013066768646,\n",
      "            0.058618638664484024,\n",
      "            0.037653714418411255,\n",
      "            -0.2051943689584732,\n",
      "            -0.014547513797879219,\n",
      "            0.0720135048031807,\n",
      "            0.106952965259552,\n",
      "            0.11546745896339417,\n",
      "            0.1106882318854332,\n",
      "            -0.1904737800359726,\n",
      "            0.08858207613229752,\n",
      "            -0.0051713488064706326,\n",
      "            0.09390829503536224,\n",
      "            0.07273762673139572,\n",
      "            -0.0020095582585781813,\n",
      "            -0.1025255024433136,\n",
      "            0.017944416031241417,\n",
      "            0.02625671587884426,\n",
      "            0.03858301043510437,\n",
      "            0.046427324414253235,\n",
      "            0.09973833709955215,\n",
      "            0.04539861902594566\n",
      "          ],\n",
      "          [\n",
      "            -0.0928860679268837,\n",
      "            -0.2142859697341919,\n",
      "            -0.013467425480484962,\n",
      "            0.17284712195396423,\n",
      "            0.06597156077623367,\n",
      "            0.15923921763896942,\n",
      "            0.17149284482002258,\n",
      "            -0.021689673885703087,\n",
      "            -0.18971897661685944,\n",
      "            0.06949474662542343,\n",
      "            0.0843116044998169,\n",
      "            0.1320040225982666,\n",
      "            -0.20934736728668213,\n",
      "            -0.07352351397275925,\n",
      "            -0.13600154221057892,\n",
      "            -0.10243235528469086,\n",
      "            -0.09322194755077362,\n",
      "            0.06573005020618439,\n",
      "            0.1118725910782814,\n",
      "            0.07008471339941025,\n",
      "            -0.01395108550786972,\n",
      "            0.05036060884594917,\n",
      "            0.1856960654258728,\n",
      "            0.42994922399520874\n",
      "          ],\n",
      "          [\n",
      "            -0.16704082489013672,\n",
      "            0.16380523145198822,\n",
      "            0.09643848240375519,\n",
      "            -0.1373821496963501,\n",
      "            0.1915966272354126,\n",
      "            0.17629598081111908,\n",
      "            -0.07494733482599258,\n",
      "            0.09306187182664871,\n",
      "            0.15266793966293335,\n",
      "            0.10657976567745209,\n",
      "            0.13148276507854462,\n",
      "            -0.0017041043611243367,\n",
      "            -0.08847540616989136,\n",
      "            0.1798105686903,\n",
      "            0.012132078409194946,\n",
      "            0.08709101378917694,\n",
      "            -0.02280385233461857,\n",
      "            0.1353101283311844,\n",
      "            -0.015476384200155735,\n",
      "            0.06340868771076202,\n",
      "            0.21836747229099274,\n",
      "            0.11817492544651031,\n",
      "            0.48859700560569763,\n",
      "            0.3406383991241455\n",
      "          ],\n",
      "          [\n",
      "            0.019979655742645264,\n",
      "            0.08454056084156036,\n",
      "            -0.12274828553199768,\n",
      "            0.014005848206579685,\n",
      "            0.03807946294546127,\n",
      "            0.17086708545684814,\n",
      "            0.11191949248313904,\n",
      "            0.08012329041957855,\n",
      "            -0.10012193024158478,\n",
      "            -0.018710631877183914,\n",
      "            -0.1697896122932434,\n",
      "            -0.06316341459751129,\n",
      "            0.03741297125816345,\n",
      "            0.04292716458439827,\n",
      "            -0.040259525179862976,\n",
      "            -0.014740265905857086,\n",
      "            0.11236203461885452,\n",
      "            -0.15343211591243744,\n",
      "            -0.030349498614668846,\n",
      "            0.03149471431970596,\n",
      "            -0.17522844672203064,\n",
      "            0.14574596285820007,\n",
      "            -0.2474394589662552,\n",
      "            -0.08822476118803024\n",
      "          ],\n",
      "          [\n",
      "            0.1338476985692978,\n",
      "            -0.1372915655374527,\n",
      "            0.16480772197246552,\n",
      "            0.16393674910068512,\n",
      "            -0.024565160274505615,\n",
      "            0.06084837019443512,\n",
      "            -0.059839800000190735,\n",
      "            -0.11517172306776047,\n",
      "            -0.04200439155101776,\n",
      "            -0.04315492510795593,\n",
      "            -0.10159170627593994,\n",
      "            0.01790390908718109,\n",
      "            -0.1359993815422058,\n",
      "            -0.17869645357131958,\n",
      "            -0.055983200669288635,\n",
      "            -0.1405901461839676,\n",
      "            -0.0212363600730896,\n",
      "            -0.11648701876401901,\n",
      "            -0.16459256410598755,\n",
      "            -0.11181464046239853,\n",
      "            0.10780130326747894,\n",
      "            -0.14268575608730316,\n",
      "            0.02937231957912445,\n",
      "            -0.13749633729457855\n",
      "          ],\n",
      "          [\n",
      "            0.11712297797203064,\n",
      "            -0.11896859109401703,\n",
      "            -0.022689511999487877,\n",
      "            0.03883683308959007,\n",
      "            -0.001264450023882091,\n",
      "            0.09573405236005783,\n",
      "            0.11544059216976166,\n",
      "            0.17316989600658417,\n",
      "            -0.0711393803358078,\n",
      "            0.1678566038608551,\n",
      "            -0.18330775201320648,\n",
      "            -0.18398131430149078,\n",
      "            -0.21099428832530975,\n",
      "            0.048178914934396744,\n",
      "            0.12049377709627151,\n",
      "            0.16480323672294617,\n",
      "            0.0493355318903923,\n",
      "            0.10215810686349869,\n",
      "            0.060220856219530106,\n",
      "            -0.02675781212747097,\n",
      "            -0.01181034930050373,\n",
      "            0.1543601006269455,\n",
      "            0.32579636573791504,\n",
      "            0.40961742401123047\n",
      "          ],\n",
      "          [\n",
      "            0.16156317293643951,\n",
      "            -0.1491621732711792,\n",
      "            0.16762606799602509,\n",
      "            0.02863014116883278,\n",
      "            -0.11816764622926712,\n",
      "            0.17247459292411804,\n",
      "            0.17257176339626312,\n",
      "            -0.12695328891277313,\n",
      "            -0.024828514084219933,\n",
      "            0.093289814889431,\n",
      "            0.14039146900177002,\n",
      "            -0.013544888235628605,\n",
      "            -0.1300315111875534,\n",
      "            -0.19286024570465088,\n",
      "            -0.04406186565756798,\n",
      "            -0.10796940326690674,\n",
      "            0.16965046525001526,\n",
      "            0.09275953471660614,\n",
      "            -0.10536403954029083,\n",
      "            0.017827440053224564,\n",
      "            -0.08525611460208893,\n",
      "            -0.14382508397102356,\n",
      "            0.28793221712112427,\n",
      "            0.06725707650184631\n",
      "          ],\n",
      "          [\n",
      "            0.08170050382614136,\n",
      "            0.13205084204673767,\n",
      "            0.006352814380079508,\n",
      "            -0.04732493311166763,\n",
      "            -0.1985226571559906,\n",
      "            -0.012273411266505718,\n",
      "            -0.12173958122730255,\n",
      "            -0.1157296746969223,\n",
      "            -0.044202275574207306,\n",
      "            0.0844196304678917,\n",
      "            -0.19590641558170319,\n",
      "            -0.05360209569334984,\n",
      "            -0.03666476532816887,\n",
      "            0.21376220881938934,\n",
      "            0.14695030450820923,\n",
      "            0.02715882658958435,\n",
      "            0.17427180707454681,\n",
      "            0.019046854227781296,\n",
      "            -0.05954434722661972,\n",
      "            -0.14979460835456848,\n",
      "            0.10226274281740189,\n",
      "            0.1046210303902626,\n",
      "            -0.15152442455291748,\n",
      "            0.09206564724445343\n",
      "          ],\n",
      "          [\n",
      "            0.17179766297340393,\n",
      "            -0.15612974762916565,\n",
      "            -0.18983647227287292,\n",
      "            -0.14753621816635132,\n",
      "            -0.11732167750597,\n",
      "            0.03983337804675102,\n",
      "            -0.1127103641629219,\n",
      "            -0.0637434795498848,\n",
      "            -0.10635821521282196,\n",
      "            0.18045258522033691,\n",
      "            -0.0036169900558888912,\n",
      "            0.07376815378665924,\n",
      "            -0.04072732478380203,\n",
      "            0.25843721628189087,\n",
      "            0.13929332792758942,\n",
      "            -0.08502353727817535,\n",
      "            -0.11939806491136551,\n",
      "            -0.12847846746444702,\n",
      "            -0.028368378058075905,\n",
      "            0.11258917301893234,\n",
      "            -0.12811686098575592,\n",
      "            0.1778673529624939,\n",
      "            0.03544771671295166,\n",
      "            -0.13448382914066315\n",
      "          ],\n",
      "          [\n",
      "            0.11907649040222168,\n",
      "            0.07802855968475342,\n",
      "            0.12931214272975922,\n",
      "            0.03212583437561989,\n",
      "            0.1461130976676941,\n",
      "            0.021931085735559464,\n",
      "            0.16012437641620636,\n",
      "            0.04311390593647957,\n",
      "            0.1036204993724823,\n",
      "            -0.05319995433092117,\n",
      "            0.01339486613869667,\n",
      "            0.05412435904145241,\n",
      "            0.0639355406165123,\n",
      "            -0.1727854609489441,\n",
      "            0.1352698653936386,\n",
      "            -0.10662536323070526,\n",
      "            -0.022730335593223572,\n",
      "            -0.09993760287761688,\n",
      "            0.13378603756427765,\n",
      "            0.12468228489160538,\n",
      "            0.2539111077785492,\n",
      "            0.1882966011762619,\n",
      "            0.13156893849372864,\n",
      "            0.3297082185745239\n",
      "          ],\n",
      "          [\n",
      "            0.15234999358654022,\n",
      "            -0.16995005309581757,\n",
      "            -0.18559855222702026,\n",
      "            -0.1487921178340912,\n",
      "            0.12950073182582855,\n",
      "            -0.15497994422912598,\n",
      "            -0.059966880828142166,\n",
      "            0.19776365160942078,\n",
      "            -0.13273245096206665,\n",
      "            -0.03389057517051697,\n",
      "            0.13132759928703308,\n",
      "            -0.12303940951824188,\n",
      "            -0.054367028176784515,\n",
      "            -0.11080005019903183,\n",
      "            -0.11851640790700912,\n",
      "            0.03653772920370102,\n",
      "            -0.0566437803208828,\n",
      "            0.11357590556144714,\n",
      "            0.04802911728620529,\n",
      "            0.011020135134458542,\n",
      "            0.09189650416374207,\n",
      "            -0.17874914407730103,\n",
      "            0.2361622154712677,\n",
      "            0.3495618402957916\n",
      "          ],\n",
      "          [\n",
      "            -0.06106790155172348,\n",
      "            0.0897253230214119,\n",
      "            0.127078115940094,\n",
      "            -0.1989063024520874,\n",
      "            -0.2283177673816681,\n",
      "            -0.15066559612751007,\n",
      "            -0.055876605212688446,\n",
      "            0.16267752647399902,\n",
      "            -0.05703709274530411,\n",
      "            -0.0894106775522232,\n",
      "            0.0031609900761395693,\n",
      "            -0.19626584649085999,\n",
      "            0.03660060465335846,\n",
      "            -0.07388599961996078,\n",
      "            0.02912748232483864,\n",
      "            0.1066683903336525,\n",
      "            -0.0503503680229187,\n",
      "            -0.10813196748495102,\n",
      "            0.05681381747126579,\n",
      "            -0.04243481904268265,\n",
      "            -0.09403955191373825,\n",
      "            -0.17285703122615814,\n",
      "            0.2862962782382965,\n",
      "            0.09205600619316101\n",
      "          ],\n",
      "          [\n",
      "            0.12076825648546219,\n",
      "            0.15761813521385193,\n",
      "            -0.10922390967607498,\n",
      "            -0.02069796249270439,\n",
      "            0.13955055177211761,\n",
      "            -0.10699475556612015,\n",
      "            -0.03758487477898598,\n",
      "            -0.034123677760362625,\n",
      "            0.15241961181163788,\n",
      "            0.12373040616512299,\n",
      "            -0.1968695968389511,\n",
      "            -0.0910244807600975,\n",
      "            0.13431942462921143,\n",
      "            -0.12126480042934418,\n",
      "            -0.13863293826580048,\n",
      "            -0.06101609766483307,\n",
      "            0.18488429486751556,\n",
      "            -0.05571229010820389,\n",
      "            -0.12659336626529694,\n",
      "            0.18856459856033325,\n",
      "            -0.06415954977273941,\n",
      "            -0.15726782381534576,\n",
      "            -0.03632725030183792,\n",
      "            -0.059857361018657684\n",
      "          ],\n",
      "          [\n",
      "            0.020544305443763733,\n",
      "            -0.1516348272562027,\n",
      "            -0.022141853347420692,\n",
      "            -0.10702567547559738,\n",
      "            -0.18341434001922607,\n",
      "            -0.08515442162752151,\n",
      "            -0.21391132473945618,\n",
      "            0.020257793366909027,\n",
      "            0.06348884850740433,\n",
      "            -0.11129879206418991,\n",
      "            -0.19336366653442383,\n",
      "            -0.15749245882034302,\n",
      "            0.22306376695632935,\n",
      "            0.008624161593616009,\n",
      "            0.1472466140985489,\n",
      "            0.03804432973265648,\n",
      "            -0.006225612945854664,\n",
      "            0.1604129523038864,\n",
      "            0.09954937547445297,\n",
      "            -0.010229521431028843,\n",
      "            0.026169274002313614,\n",
      "            -0.08485817164182663,\n",
      "            -0.05314488708972931,\n",
      "            0.07245425134897232\n",
      "          ],\n",
      "          [\n",
      "            0.1595887392759323,\n",
      "            0.18188557028770447,\n",
      "            -0.08195414394140244,\n",
      "            -0.06493522971868515,\n",
      "            0.01284905057400465,\n",
      "            -0.08201219886541367,\n",
      "            -0.07057049125432968,\n",
      "            0.10152420401573181,\n",
      "            -0.12023754417896271,\n",
      "            -0.09182184189558029,\n",
      "            -0.017110221087932587,\n",
      "            0.09521792829036713,\n",
      "            -0.15831832587718964,\n",
      "            -0.055266138166189194,\n",
      "            -0.0011117745889350772,\n",
      "            -0.0047026099637150764,\n",
      "            0.048137497156858444,\n",
      "            -0.019853314384818077,\n",
      "            0.0043465993367135525,\n",
      "            -0.17120717465877533,\n",
      "            -0.08954847604036331,\n",
      "            0.024320406839251518,\n",
      "            -0.08628296107053757,\n",
      "            -0.09766063094139099\n",
      "          ],\n",
      "          [\n",
      "            -0.060379497706890106,\n",
      "            -0.05949048325419426,\n",
      "            -0.024617131799459457,\n",
      "            0.1996266394853592,\n",
      "            0.006705392152070999,\n",
      "            -0.10045818984508514,\n",
      "            -0.03846566379070282,\n",
      "            -0.09864729642868042,\n",
      "            -0.06192133575677872,\n",
      "            -0.1111343652009964,\n",
      "            0.07160133123397827,\n",
      "            -0.11549875885248184,\n",
      "            -0.04911899194121361,\n",
      "            -0.04700881615281105,\n",
      "            0.11216897517442703,\n",
      "            0.12278519570827484,\n",
      "            -0.08792771399021149,\n",
      "            0.005183259956538677,\n",
      "            0.19573475420475006,\n",
      "            0.013441123999655247,\n",
      "            -0.009060949087142944,\n",
      "            -0.17786423861980438,\n",
      "            0.19962859153747559,\n",
      "            -0.007774837780743837\n",
      "          ],\n",
      "          [\n",
      "            -0.031697239726781845,\n",
      "            -0.11454228311777115,\n",
      "            -0.058809563517570496,\n",
      "            -0.10241281986236572,\n",
      "            0.0471242219209671,\n",
      "            0.0004474442102946341,\n",
      "            0.08057666569948196,\n",
      "            0.17396968603134155,\n",
      "            0.14474311470985413,\n",
      "            0.15170830488204956,\n",
      "            -0.04352003335952759,\n",
      "            0.061495501548051834,\n",
      "            0.08908919990062714,\n",
      "            0.16075602173805237,\n",
      "            0.01703592762351036,\n",
      "            0.0531698539853096,\n",
      "            -0.1892557144165039,\n",
      "            0.20456534624099731,\n",
      "            -0.011804912239313126,\n",
      "            -0.1810493767261505,\n",
      "            0.0728016346693039,\n",
      "            -0.20433340966701508,\n",
      "            -0.4493285119533539,\n",
      "            -0.33942875266075134\n",
      "          ],\n",
      "          [\n",
      "            0.11679276078939438,\n",
      "            0.11729781329631805,\n",
      "            0.1415623128414154,\n",
      "            0.03695716708898544,\n",
      "            0.1753322184085846,\n",
      "            -0.19496358931064606,\n",
      "            -0.044541917741298676,\n",
      "            0.15624085068702698,\n",
      "            0.13839197158813477,\n",
      "            0.13577745854854584,\n",
      "            -0.16044852137565613,\n",
      "            -0.15863101184368134,\n",
      "            -0.019828427582979202,\n",
      "            -0.09592727571725845,\n",
      "            0.005353347398340702,\n",
      "            0.15209496021270752,\n",
      "            -0.05517254397273064,\n",
      "            0.14889943599700928,\n",
      "            0.14233706891536713,\n",
      "            -0.15438418090343475,\n",
      "            -0.03248250111937523,\n",
      "            0.13174858689308167,\n",
      "            -0.022250954061746597,\n",
      "            -0.19665905833244324\n",
      "          ],\n",
      "          [\n",
      "            -0.13226763904094696,\n",
      "            -0.17281240224838257,\n",
      "            0.20986877381801605,\n",
      "            0.16776235401630402,\n",
      "            0.015721697360277176,\n",
      "            0.03316975012421608,\n",
      "            -0.03744981065392494,\n",
      "            -0.10430087894201279,\n",
      "            0.17252425849437714,\n",
      "            0.1626550704240799,\n",
      "            0.08835913240909576,\n",
      "            -0.013371424749493599,\n",
      "            -0.022184934467077255,\n",
      "            0.18084582686424255,\n",
      "            0.09476420283317566,\n",
      "            0.15508030354976654,\n",
      "            0.14946189522743225,\n",
      "            0.1087765172123909,\n",
      "            -0.11507051438093185,\n",
      "            -0.046315815299749374,\n",
      "            -0.032148104161024094,\n",
      "            -0.05495879799127579,\n",
      "            -0.10436803102493286,\n",
      "            0.14623025059700012\n",
      "          ],\n",
      "          [\n",
      "            0.17362214624881744,\n",
      "            0.004426788538694382,\n",
      "            0.15964682400226593,\n",
      "            -0.08969603478908539,\n",
      "            0.18105287849903107,\n",
      "            -0.15648677945137024,\n",
      "            -0.0549008771777153,\n",
      "            -0.1456710398197174,\n",
      "            -0.11145882308483124,\n",
      "            0.008760501630604267,\n",
      "            0.1592905968427658,\n",
      "            0.018199149519205093,\n",
      "            0.037856556475162506,\n",
      "            0.19875198602676392,\n",
      "            -0.12040027976036072,\n",
      "            0.18997806310653687,\n",
      "            0.09670296311378479,\n",
      "            0.10383931547403336,\n",
      "            -0.038467299193143845,\n",
      "            0.12130086869001389,\n",
      "            -0.08030004054307938,\n",
      "            0.08697821199893951,\n",
      "            0.06612805277109146,\n",
      "            -0.107856385409832\n",
      "          ],\n",
      "          [\n",
      "            -0.19042563438415527,\n",
      "            -0.18513447046279907,\n",
      "            -0.07203971594572067,\n",
      "            0.03805477172136307,\n",
      "            0.18701349198818207,\n",
      "            -0.10602045059204102,\n",
      "            -0.07879249006509781,\n",
      "            0.06134199723601341,\n",
      "            -0.11793874949216843,\n",
      "            -0.10609552264213562,\n",
      "            -0.1223294734954834,\n",
      "            -0.04389631375670433,\n",
      "            -0.17653211951255798,\n",
      "            -0.11801585555076599,\n",
      "            -0.053392235189676285,\n",
      "            -0.02185620740056038,\n",
      "            0.10852500051259995,\n",
      "            -0.13171254098415375,\n",
      "            0.20488695800304413,\n",
      "            0.14191477000713348,\n",
      "            0.18422718346118927,\n",
      "            -0.11032429337501526,\n",
      "            0.2685968279838562,\n",
      "            0.15184146165847778\n",
      "          ],\n",
      "          [\n",
      "            -0.11756621301174164,\n",
      "            0.02176961489021778,\n",
      "            -0.12907010316848755,\n",
      "            -0.017343372106552124,\n",
      "            0.15007910132408142,\n",
      "            0.11198032647371292,\n",
      "            -0.17380909621715546,\n",
      "            0.04251628741621971,\n",
      "            -0.2098936140537262,\n",
      "            -0.029665624722838402,\n",
      "            0.0002832164173014462,\n",
      "            -0.14556211233139038,\n",
      "            0.04929308593273163,\n",
      "            0.13601642847061157,\n",
      "            0.08798792213201523,\n",
      "            -0.19622233510017395,\n",
      "            -0.0848224088549614,\n",
      "            0.17443977296352386,\n",
      "            -0.12109881639480591,\n",
      "            -0.06013975664973259,\n",
      "            0.2528785765171051,\n",
      "            0.2077215015888214,\n",
      "            0.07404457032680511,\n",
      "            0.02026924304664135\n",
      "          ],\n",
      "          [\n",
      "            0.10068107396364212,\n",
      "            -0.121749147772789,\n",
      "            -0.06635600328445435,\n",
      "            -0.20332591235637665,\n",
      "            -0.14300470054149628,\n",
      "            0.17197133600711823,\n",
      "            -0.1690756380558014,\n",
      "            0.1932821273803711,\n",
      "            -0.08237544447183609,\n",
      "            -0.19974841177463531,\n",
      "            -0.11445816606283188,\n",
      "            -0.06568948179483414,\n",
      "            0.09335550665855408,\n",
      "            -0.053405433893203735,\n",
      "            0.04179463908076286,\n",
      "            0.04978351294994354,\n",
      "            -0.1779918670654297,\n",
      "            0.17076869308948517,\n",
      "            -0.10632088035345078,\n",
      "            0.12787507474422455,\n",
      "            -0.14806130528450012,\n",
      "            0.12359222024679184,\n",
      "            0.04513910040259361,\n",
      "            0.3255164325237274\n",
      "          ],\n",
      "          [\n",
      "            0.21672362089157104,\n",
      "            -0.13507048785686493,\n",
      "            0.0684535875916481,\n",
      "            -0.007273842580616474,\n",
      "            -0.020739814266562462,\n",
      "            -0.07676205039024353,\n",
      "            0.10984433442354202,\n",
      "            -0.06942673772573471,\n",
      "            0.2115968018770218,\n",
      "            0.11559443175792694,\n",
      "            0.11986915022134781,\n",
      "            -0.11978117376565933,\n",
      "            0.018253900110721588,\n",
      "            -0.009940890595316887,\n",
      "            0.17019915580749512,\n",
      "            0.12267143279314041,\n",
      "            0.10014620423316956,\n",
      "            0.016319429501891136,\n",
      "            0.17681558430194855,\n",
      "            -0.0955897644162178,\n",
      "            -0.10981886833906174,\n",
      "            -0.08944646269083023,\n",
      "            -0.1602175086736679,\n",
      "            -0.10487440973520279\n",
      "          ],\n",
      "          [\n",
      "            -0.007218522485345602,\n",
      "            -0.11002849787473679,\n",
      "            -0.05665705353021622,\n",
      "            -0.037639468908309937,\n",
      "            0.021582258865237236,\n",
      "            -0.12642332911491394,\n",
      "            0.189483180642128,\n",
      "            0.09603659063577652,\n",
      "            0.04215731844305992,\n",
      "            -0.12385326623916626,\n",
      "            -0.19032634794712067,\n",
      "            0.14107349514961243,\n",
      "            0.04194052889943123,\n",
      "            -0.021534264087677002,\n",
      "            -0.05874078348278999,\n",
      "            -0.0964662954211235,\n",
      "            -0.01032806932926178,\n",
      "            -0.00912644062191248,\n",
      "            0.03261054679751396,\n",
      "            0.18556348979473114,\n",
      "            0.12592874467372894,\n",
      "            0.05844537541270256,\n",
      "            0.13580094277858734,\n",
      "            0.22742901742458344\n",
      "          ],\n",
      "          [\n",
      "            -0.18779852986335754,\n",
      "            -0.13586197793483734,\n",
      "            -0.2598170340061188,\n",
      "            -0.22531762719154358,\n",
      "            0.03129060938954353,\n",
      "            -0.11076898872852325,\n",
      "            -0.08042562752962112,\n",
      "            0.14154687523841858,\n",
      "            -0.14362500607967377,\n",
      "            -0.2142462432384491,\n",
      "            -0.19160149991512299,\n",
      "            -0.03651486709713936,\n",
      "            0.07555752992630005,\n",
      "            0.1552589237689972,\n",
      "            0.013006651774048805,\n",
      "            0.13418827950954437,\n",
      "            0.016803110018372536,\n",
      "            -0.18733282387256622,\n",
      "            -0.1653645634651184,\n",
      "            -0.16887104511260986,\n",
      "            -0.12980994582176208,\n",
      "            0.014912251383066177,\n",
      "            0.2268257886171341,\n",
      "            0.17839375138282776\n",
      "          ],\n",
      "          [\n",
      "            -0.06180274114012718,\n",
      "            0.09563547372817993,\n",
      "            -0.006381035316735506,\n",
      "            -0.1919831484556198,\n",
      "            -0.025152724236249924,\n",
      "            -0.1859913021326065,\n",
      "            0.201398566365242,\n",
      "            0.2043537050485611,\n",
      "            0.18285779654979706,\n",
      "            -0.18206055462360382,\n",
      "            0.03401436284184456,\n",
      "            0.12859387695789337,\n",
      "            0.17900992929935455,\n",
      "            -0.1685774326324463,\n",
      "            -0.11460111290216446,\n",
      "            -0.18108247220516205,\n",
      "            0.05647823587059975,\n",
      "            -0.032533176243305206,\n",
      "            0.18571029603481293,\n",
      "            0.1466943621635437,\n",
      "            0.24525263905525208,\n",
      "            -0.09892559051513672,\n",
      "            0.053232353180646896,\n",
      "            0.12124387919902802\n",
      "          ],\n",
      "          [\n",
      "            -0.18423311412334442,\n",
      "            0.021682968363165855,\n",
      "            -0.027773134410381317,\n",
      "            0.14821332693099976,\n",
      "            0.1718495786190033,\n",
      "            0.11849834024906158,\n",
      "            0.14011874794960022,\n",
      "            0.0028799648862332106,\n",
      "            0.1344216763973236,\n",
      "            -0.12870126962661743,\n",
      "            -0.19757437705993652,\n",
      "            -0.06523879617452621,\n",
      "            0.14205673336982727,\n",
      "            0.0009411732899025083,\n",
      "            0.05478045344352722,\n",
      "            -0.17031443119049072,\n",
      "            0.07463707774877548,\n",
      "            -0.04696464166045189,\n",
      "            -0.02769833244383335,\n",
      "            0.040233954787254333,\n",
      "            0.13418956100940704,\n",
      "            -0.0419161394238472,\n",
      "            0.07936801761388779,\n",
      "            0.33737778663635254\n",
      "          ],\n",
      "          [\n",
      "            0.127005472779274,\n",
      "            -0.07903087884187698,\n",
      "            0.04399866983294487,\n",
      "            -0.09833431988954544,\n",
      "            0.09832782298326492,\n",
      "            0.04998747259378433,\n",
      "            -0.049363959580659866,\n",
      "            -0.007421200629323721,\n",
      "            -0.07018352299928665,\n",
      "            -0.12865754961967468,\n",
      "            0.16243034601211548,\n",
      "            0.05073985829949379,\n",
      "            -0.11391063034534454,\n",
      "            -0.06948523223400116,\n",
      "            -0.18481947481632233,\n",
      "            0.15731343626976013,\n",
      "            -0.1693790853023529,\n",
      "            -0.05037222430109978,\n",
      "            -0.1041550263762474,\n",
      "            0.16962076723575592,\n",
      "            0.05342191457748413,\n",
      "            0.16584835946559906,\n",
      "            -0.15460877120494843,\n",
      "            -0.11255203187465668\n",
      "          ],\n",
      "          [\n",
      "            0.014455683529376984,\n",
      "            -0.19115833938121796,\n",
      "            -0.11983151733875275,\n",
      "            0.07364924252033234,\n",
      "            0.1852840930223465,\n",
      "            0.121039479970932,\n",
      "            0.12459181994199753,\n",
      "            -0.09122654050588608,\n",
      "            -0.018286563456058502,\n",
      "            -0.17627187073230743,\n",
      "            -0.07096099853515625,\n",
      "            -0.06511827558279037,\n",
      "            0.0646800845861435,\n",
      "            -0.1611184924840927,\n",
      "            0.1093933954834938,\n",
      "            0.10540207475423813,\n",
      "            0.04966874420642853,\n",
      "            -0.09990707039833069,\n",
      "            0.021360650658607483,\n",
      "            -0.04959667846560478,\n",
      "            -0.04968097433447838,\n",
      "            0.17962829768657684,\n",
      "            0.3363719582557678,\n",
      "            0.3264405131340027\n",
      "          ],\n",
      "          [\n",
      "            -0.035997286438941956,\n",
      "            -0.017046088352799416,\n",
      "            0.06230399012565613,\n",
      "            -0.003050730098038912,\n",
      "            0.07996845245361328,\n",
      "            -0.0823582336306572,\n",
      "            0.012658835388720036,\n",
      "            0.19021303951740265,\n",
      "            -0.022051537409424782,\n",
      "            0.13016287982463837,\n",
      "            0.04459653049707413,\n",
      "            -0.1117817834019661,\n",
      "            0.16912871599197388,\n",
      "            -0.011911537498235703,\n",
      "            -0.0892820805311203,\n",
      "            -0.08322146534919739,\n",
      "            0.08812044560909271,\n",
      "            0.0019236153457313776,\n",
      "            -0.04848479479551315,\n",
      "            0.0967266783118248,\n",
      "            0.07921453565359116,\n",
      "            0.03825262561440468,\n",
      "            0.26876336336135864,\n",
      "            0.12987631559371948\n",
      "          ],\n",
      "          [\n",
      "            0.1712227314710617,\n",
      "            -0.15619242191314697,\n",
      "            -0.06474296003580093,\n",
      "            -0.14803893864154816,\n",
      "            0.06327081471681595,\n",
      "            0.1600686013698578,\n",
      "            0.05847552791237831,\n",
      "            -0.05172080919146538,\n",
      "            -0.012328669428825378,\n",
      "            0.006059663370251656,\n",
      "            0.08134356141090393,\n",
      "            -0.044053081423044205,\n",
      "            -0.1634569615125656,\n",
      "            -0.10241345316171646,\n",
      "            0.03448488935828209,\n",
      "            -0.19440428912639618,\n",
      "            -0.1483854055404663,\n",
      "            -0.07141798734664917,\n",
      "            -0.19750294089317322,\n",
      "            -0.19052638113498688,\n",
      "            0.06972125172615051,\n",
      "            0.11484858393669128,\n",
      "            -0.19023442268371582,\n",
      "            0.07977000623941422\n",
      "          ],\n",
      "          [\n",
      "            -0.13245061039924622,\n",
      "            0.09398006647825241,\n",
      "            -0.10630935430526733,\n",
      "            0.24052511155605316,\n",
      "            -0.06479942798614502,\n",
      "            0.00442518200725317,\n",
      "            0.20441149175167084,\n",
      "            -0.0031061158515512943,\n",
      "            0.142901211977005,\n",
      "            0.22053861618041992,\n",
      "            0.026712113991379738,\n",
      "            0.1916479617357254,\n",
      "            -0.09138260781764984,\n",
      "            -0.0834922194480896,\n",
      "            -0.17824995517730713,\n",
      "            0.21156428754329681,\n",
      "            0.16462691128253937,\n",
      "            0.03967098146677017,\n",
      "            0.05742489919066429,\n",
      "            -0.025417577475309372,\n",
      "            -0.18802592158317566,\n",
      "            0.09325609356164932,\n",
      "            -0.3548246920108795,\n",
      "            -0.29947054386138916\n",
      "          ],\n",
      "          [\n",
      "            0.025214385241270065,\n",
      "            -0.051634982228279114,\n",
      "            -0.07632336765527725,\n",
      "            -0.048881854861974716,\n",
      "            -0.11072557419538498,\n",
      "            -0.08141446858644485,\n",
      "            0.0696912333369255,\n",
      "            -0.14798487722873688,\n",
      "            0.029235772788524628,\n",
      "            -0.09813421964645386,\n",
      "            0.049220822751522064,\n",
      "            0.15479111671447754,\n",
      "            0.04424920678138733,\n",
      "            0.18057069182395935,\n",
      "            -0.07839073985815048,\n",
      "            0.14311662316322327,\n",
      "            0.020574523136019707,\n",
      "            0.16806113719940186,\n",
      "            -0.027695517987012863,\n",
      "            -0.18180109560489655,\n",
      "            -0.086046501994133,\n",
      "            -0.04899056255817413,\n",
      "            0.007941389456391335,\n",
      "            -0.14107856154441833\n",
      "          ],\n",
      "          [\n",
      "            -0.14600537717342377,\n",
      "            0.12257340550422668,\n",
      "            -0.09804364293813705,\n",
      "            0.13876788318157196,\n",
      "            -0.12842898070812225,\n",
      "            -0.12313386052846909,\n",
      "            0.15356501936912537,\n",
      "            0.12761658430099487,\n",
      "            0.0916069820523262,\n",
      "            0.17037682235240936,\n",
      "            -0.027214914560317993,\n",
      "            -0.10691174864768982,\n",
      "            -0.02216614969074726,\n",
      "            -0.14248405396938324,\n",
      "            0.01981695368885994,\n",
      "            -0.06343553215265274,\n",
      "            -0.12755273282527924,\n",
      "            0.051472168415784836,\n",
      "            -0.22525472939014435,\n",
      "            -0.02396777644753456,\n",
      "            -0.15167255699634552,\n",
      "            -0.14261828362941742,\n",
      "            -0.3529444932937622,\n",
      "            -0.2828100621700287\n",
      "          ],\n",
      "          [\n",
      "            0.14705230295658112,\n",
      "            0.07365132123231888,\n",
      "            0.07608085870742798,\n",
      "            -0.02002633549273014,\n",
      "            -0.10216106474399567,\n",
      "            0.03311719745397568,\n",
      "            -0.14559365808963776,\n",
      "            0.039463117718696594,\n",
      "            -0.0855400487780571,\n",
      "            0.005202931817620993,\n",
      "            0.1485927551984787,\n",
      "            -0.1257498860359192,\n",
      "            0.019630249589681625,\n",
      "            0.16088755428791046,\n",
      "            -0.09093538671731949,\n",
      "            0.03134952113032341,\n",
      "            0.07174655795097351,\n",
      "            0.035502560436725616,\n",
      "            0.15145927667617798,\n",
      "            -0.16506275534629822,\n",
      "            -0.18201129138469696,\n",
      "            -0.020897511392831802,\n",
      "            0.09144052863121033,\n",
      "            0.1824408769607544\n",
      "          ],\n",
      "          [\n",
      "            0.008582677692174911,\n",
      "            -0.1387297511100769,\n",
      "            0.11673733592033386,\n",
      "            0.038827668875455856,\n",
      "            -0.18645286560058594,\n",
      "            -0.15600237250328064,\n",
      "            -0.03581351786851883,\n",
      "            -0.15912333130836487,\n",
      "            -0.09411725401878357,\n",
      "            0.15286466479301453,\n",
      "            -0.06895973533391953,\n",
      "            -0.10583440959453583,\n",
      "            0.1361732929944992,\n",
      "            0.1623028963804245,\n",
      "            -0.09971822053194046,\n",
      "            -0.12295911461114883,\n",
      "            0.08774880319833755,\n",
      "            0.09608728438615799,\n",
      "            -0.1333494335412979,\n",
      "            -0.12786617875099182,\n",
      "            0.19255560636520386,\n",
      "            0.10096748173236847,\n",
      "            0.0972585529088974,\n",
      "            0.0011318916222080588\n",
      "          ],\n",
      "          [\n",
      "            0.0934339091181755,\n",
      "            0.16407009959220886,\n",
      "            0.1704035848379135,\n",
      "            -0.09594693779945374,\n",
      "            0.13252995908260345,\n",
      "            -0.1396358460187912,\n",
      "            0.13042709231376648,\n",
      "            -0.03381233662366867,\n",
      "            -0.11648567765951157,\n",
      "            -0.07046199589967728,\n",
      "            0.15753215551376343,\n",
      "            -0.12034749239683151,\n",
      "            -0.09522545337677002,\n",
      "            -0.12602367997169495,\n",
      "            0.0604904443025589,\n",
      "            0.03356660529971123,\n",
      "            -0.04279307648539543,\n",
      "            0.10720037668943405,\n",
      "            -0.09134087711572647,\n",
      "            0.05160027742385864,\n",
      "            0.10568477213382721,\n",
      "            0.1188717857003212,\n",
      "            0.43548548221588135,\n",
      "            0.3085375726222992\n",
      "          ],\n",
      "          [\n",
      "            -0.10629472881555557,\n",
      "            -0.114255890250206,\n",
      "            0.09477587044239044,\n",
      "            -0.11870123445987701,\n",
      "            -0.1702844202518463,\n",
      "            -0.06430532038211823,\n",
      "            -0.1406908631324768,\n",
      "            -0.09236110001802444,\n",
      "            0.06158237159252167,\n",
      "            0.04625195264816284,\n",
      "            -0.04202285408973694,\n",
      "            0.1288188248872757,\n",
      "            0.14032845199108124,\n",
      "            -0.16223523020744324,\n",
      "            0.17202939093112946,\n",
      "            -0.17838197946548462,\n",
      "            -0.10965754836797714,\n",
      "            0.11912940442562103,\n",
      "            0.14457468688488007,\n",
      "            -0.02658635377883911,\n",
      "            0.1948925107717514,\n",
      "            -0.10528524965047836,\n",
      "            -0.057934194803237915,\n",
      "            -0.10639278590679169\n",
      "          ],\n",
      "          [\n",
      "            -0.0025575237814337015,\n",
      "            -0.1646260768175125,\n",
      "            0.058694906532764435,\n",
      "            0.07205744832754135,\n",
      "            -0.06771359592676163,\n",
      "            -0.057039737701416016,\n",
      "            0.018149375915527344,\n",
      "            -0.1817234456539154,\n",
      "            0.007431133650243282,\n",
      "            0.0786675438284874,\n",
      "            0.0038774835411459208,\n",
      "            -0.11717282235622406,\n",
      "            0.014153883792459965,\n",
      "            0.017055174335837364,\n",
      "            0.12828192114830017,\n",
      "            -0.08050113171339035,\n",
      "            0.038515496999025345,\n",
      "            -0.057650111615657806,\n",
      "            0.19558413326740265,\n",
      "            -0.13796022534370422,\n",
      "            -0.11914390325546265,\n",
      "            0.03422922268509865,\n",
      "            -0.03323052451014519,\n",
      "            0.17149078845977783\n",
      "          ],\n",
      "          [\n",
      "            -0.12201551347970963,\n",
      "            0.10162429511547089,\n",
      "            0.02196449227631092,\n",
      "            -0.13438177108764648,\n",
      "            -0.1255829930305481,\n",
      "            0.04832221195101738,\n",
      "            -9.118206071434543e-05,\n",
      "            -0.06272834539413452,\n",
      "            -0.13995404541492462,\n",
      "            -0.07255090773105621,\n",
      "            -0.11855809390544891,\n",
      "            -0.1612970232963562,\n",
      "            -0.13966262340545654,\n",
      "            0.1468580663204193,\n",
      "            0.12996315956115723,\n",
      "            0.1097337007522583,\n",
      "            -0.09130687266588211,\n",
      "            0.06467549502849579,\n",
      "            0.07533520460128784,\n",
      "            -0.15127842128276825,\n",
      "            -0.04062667861580849,\n",
      "            -0.029387883841991425,\n",
      "            0.12398476153612137,\n",
      "            -0.1017746850848198\n",
      "          ],\n",
      "          [\n",
      "            -0.16688144207000732,\n",
      "            0.1327020674943924,\n",
      "            0.16484566032886505,\n",
      "            -0.1661890596151352,\n",
      "            0.08274480700492859,\n",
      "            -0.15740551054477692,\n",
      "            0.04628241807222366,\n",
      "            0.09904215484857559,\n",
      "            -0.12912532687187195,\n",
      "            -0.171242818236351,\n",
      "            0.11954519152641296,\n",
      "            -0.1630234718322754,\n",
      "            -0.11464769393205643,\n",
      "            0.07985920459032059,\n",
      "            0.14524799585342407,\n",
      "            0.074386365711689,\n",
      "            0.0757463201880455,\n",
      "            0.07703851163387299,\n",
      "            0.05816153064370155,\n",
      "            -0.14930664002895355,\n",
      "            0.20969128608703613,\n",
      "            -0.16110390424728394,\n",
      "            0.045583274215459824,\n",
      "            0.2028474360704422\n",
      "          ],\n",
      "          [\n",
      "            0.055393196642398834,\n",
      "            -0.058964863419532776,\n",
      "            -0.01826517842710018,\n",
      "            0.08043090254068375,\n",
      "            0.002214855281636119,\n",
      "            -0.17136049270629883,\n",
      "            -0.02695568837225437,\n",
      "            0.1432366818189621,\n",
      "            0.017865221947431564,\n",
      "            -0.002671486232429743,\n",
      "            -0.09071095287799835,\n",
      "            0.013594996184110641,\n",
      "            -0.14139622449874878,\n",
      "            -0.020871303975582123,\n",
      "            0.13797153532505035,\n",
      "            0.15788045525550842,\n",
      "            0.14089946448802948,\n",
      "            -0.09898089617490768,\n",
      "            -0.07516924291849136,\n",
      "            0.13520096242427826,\n",
      "            -0.03125283867120743,\n",
      "            -0.12513162195682526,\n",
      "            0.04446566849946976,\n",
      "            -0.253002405166626\n",
      "          ],\n",
      "          [\n",
      "            -0.153616264462471,\n",
      "            -0.1375395655632019,\n",
      "            -0.17372189462184906,\n",
      "            -0.1225351095199585,\n",
      "            0.022347798570990562,\n",
      "            -0.026103833690285683,\n",
      "            0.04366742819547653,\n",
      "            0.13470108807086945,\n",
      "            -0.0006164615042507648,\n",
      "            -0.056585315614938736,\n",
      "            -0.13618141412734985,\n",
      "            -0.16304120421409607,\n",
      "            0.04784092679619789,\n",
      "            -0.06221794709563255,\n",
      "            0.0976836085319519,\n",
      "            0.1281934380531311,\n",
      "            -0.1697092354297638,\n",
      "            -0.0784880518913269,\n",
      "            -0.10820148885250092,\n",
      "            0.11799397319555283,\n",
      "            0.023388022556900978,\n",
      "            0.04927535355091095,\n",
      "            0.41540178656578064,\n",
      "            0.4441283345222473\n",
      "          ],\n",
      "          [\n",
      "            -0.16887277364730835,\n",
      "            -0.15331044793128967,\n",
      "            -0.06095127761363983,\n",
      "            0.09267125278711319,\n",
      "            0.12722352147102356,\n",
      "            -0.040475983172655106,\n",
      "            0.11490938812494278,\n",
      "            0.15978777408599854,\n",
      "            0.03350992873311043,\n",
      "            0.07557976990938187,\n",
      "            -0.020474301651120186,\n",
      "            0.1211414560675621,\n",
      "            0.019163403660058975,\n",
      "            -0.09694413840770721,\n",
      "            0.16698946058750153,\n",
      "            -0.05267774313688278,\n",
      "            0.04406416043639183,\n",
      "            -0.1590566486120224,\n",
      "            0.09204159677028656,\n",
      "            -0.008245062083005905,\n",
      "            0.06795606017112732,\n",
      "            0.010477692820131779,\n",
      "            0.15034735202789307,\n",
      "            0.285147488117218\n",
      "          ],\n",
      "          [\n",
      "            -0.1148519366979599,\n",
      "            -0.13958367705345154,\n",
      "            0.04345988109707832,\n",
      "            -0.14567263424396515,\n",
      "            0.03738487511873245,\n",
      "            0.14657829701900482,\n",
      "            -0.08948998153209686,\n",
      "            0.008007077500224113,\n",
      "            -0.16093945503234863,\n",
      "            -0.047531262040138245,\n",
      "            -0.10943736135959625,\n",
      "            0.11706969887018204,\n",
      "            -0.09005605429410934,\n",
      "            0.207625612616539,\n",
      "            0.06855040788650513,\n",
      "            -0.07885764539241791,\n",
      "            -0.10951405763626099,\n",
      "            0.07396382838487625,\n",
      "            0.1979968100786209,\n",
      "            0.1404128074645996,\n",
      "            0.18685540556907654,\n",
      "            -0.08590617030858994,\n",
      "            0.021209439262747765,\n",
      "            0.12534569203853607\n",
      "          ],\n",
      "          [\n",
      "            0.004264111630618572,\n",
      "            -0.19554732739925385,\n",
      "            0.032606080174446106,\n",
      "            -0.10559530556201935,\n",
      "            -0.11326508224010468,\n",
      "            -0.15650159120559692,\n",
      "            -0.19749724864959717,\n",
      "            -0.022652089595794678,\n",
      "            0.07432088255882263,\n",
      "            0.027750540524721146,\n",
      "            0.09873144328594208,\n",
      "            0.07051407545804977,\n",
      "            0.17942456901073456,\n",
      "            0.21513645350933075,\n",
      "            -0.010145938955247402,\n",
      "            0.025709228590130806,\n",
      "            0.008375690318644047,\n",
      "            0.1826307624578476,\n",
      "            0.10591030865907669,\n",
      "            0.17949989438056946,\n",
      "            -0.14302390813827515,\n",
      "            -0.08181560039520264,\n",
      "            -0.19835245609283447,\n",
      "            -0.18186691403388977\n",
      "          ],\n",
      "          [\n",
      "            0.0977291613817215,\n",
      "            0.15377086400985718,\n",
      "            0.13619297742843628,\n",
      "            0.1767914891242981,\n",
      "            -0.08478313684463501,\n",
      "            -0.1905221939086914,\n",
      "            0.05921107158064842,\n",
      "            0.16583198308944702,\n",
      "            -0.1957595944404602,\n",
      "            0.1417415291070938,\n",
      "            0.04704512283205986,\n",
      "            -0.041839875280857086,\n",
      "            -0.026554005220532417,\n",
      "            0.11161869764328003,\n",
      "            0.051566097885370255,\n",
      "            0.03442453220486641,\n",
      "            0.15554003417491913,\n",
      "            0.06177652254700661,\n",
      "            -0.14672258496284485,\n",
      "            -0.13280299305915833,\n",
      "            -0.027895234525203705,\n",
      "            0.2284512221813202,\n",
      "            0.000850935815833509,\n",
      "            -0.0510367788374424\n",
      "          ],\n",
      "          [\n",
      "            -0.191293865442276,\n",
      "            -0.12262073159217834,\n",
      "            -0.05762970820069313,\n",
      "            -0.12300976365804672,\n",
      "            -0.18210282921791077,\n",
      "            0.10707247257232666,\n",
      "            0.0759926289319992,\n",
      "            -0.15296682715415955,\n",
      "            0.10246041417121887,\n",
      "            0.04709016904234886,\n",
      "            0.07790226489305496,\n",
      "            0.1474916934967041,\n",
      "            0.18851037323474884,\n",
      "            0.057088252156972885,\n",
      "            0.03717704862356186,\n",
      "            -0.05763239786028862,\n",
      "            0.1902536153793335,\n",
      "            -0.021588243544101715,\n",
      "            -0.031139133498072624,\n",
      "            0.09683064371347427,\n",
      "            0.0598822720348835,\n",
      "            -0.18870675563812256,\n",
      "            -0.02080739289522171,\n",
      "            -0.10770564526319504\n",
      "          ],\n",
      "          [\n",
      "            0.08469607681035995,\n",
      "            0.17595896124839783,\n",
      "            -0.029792804270982742,\n",
      "            -0.1090564951300621,\n",
      "            0.18282471597194672,\n",
      "            0.0867830216884613,\n",
      "            0.11893899738788605,\n",
      "            -0.11373867094516754,\n",
      "            0.1066056415438652,\n",
      "            0.021632809191942215,\n",
      "            -0.09438508749008179,\n",
      "            -0.02674047090113163,\n",
      "            -0.1357470154762268,\n",
      "            0.0518161803483963,\n",
      "            -0.07318060100078583,\n",
      "            0.10615520924329758,\n",
      "            -0.16536128520965576,\n",
      "            0.18048147857189178,\n",
      "            0.059203024953603745,\n",
      "            -0.1301264762878418,\n",
      "            0.017358683049678802,\n",
      "            0.13762731850147247,\n",
      "            0.3144122064113617,\n",
      "            0.4552971422672272\n",
      "          ],\n",
      "          [\n",
      "            0.03494776785373688,\n",
      "            0.10811665654182434,\n",
      "            -0.18385736644268036,\n",
      "            -0.21463195979595184,\n",
      "            0.11196298897266388,\n",
      "            -0.1676369160413742,\n",
      "            0.20198751986026764,\n",
      "            0.12156576663255692,\n",
      "            -0.08675115555524826,\n",
      "            -0.17287223041057587,\n",
      "            -0.17369593679904938,\n",
      "            0.14554442465305328,\n",
      "            -0.005519811064004898,\n",
      "            -0.06689867377281189,\n",
      "            -0.14083132147789001,\n",
      "            0.09105673432350159,\n",
      "            -0.11386896669864655,\n",
      "            -0.0722220242023468,\n",
      "            0.07231999188661575,\n",
      "            0.23191723227500916,\n",
      "            0.2864282727241516,\n",
      "            0.12172546982765198,\n",
      "            0.17604349553585052,\n",
      "            0.38387489318847656\n",
      "          ],\n",
      "          [\n",
      "            0.14018750190734863,\n",
      "            -0.039426885545253754,\n",
      "            0.08649402111768723,\n",
      "            0.06190356984734535,\n",
      "            0.10475955158472061,\n",
      "            0.0014880336821079254,\n",
      "            -0.09075970202684402,\n",
      "            -0.006510391365736723,\n",
      "            -0.10834135860204697,\n",
      "            -0.1616857498884201,\n",
      "            0.013276655226945877,\n",
      "            -0.13016346096992493,\n",
      "            0.10066050291061401,\n",
      "            -0.12179052829742432,\n",
      "            -0.03235464543104172,\n",
      "            0.16861650347709656,\n",
      "            0.10501112788915634,\n",
      "            0.006369896698743105,\n",
      "            0.08603276312351227,\n",
      "            0.08555915951728821,\n",
      "            -0.17773577570915222,\n",
      "            -0.1357560008764267,\n",
      "            -0.09246344119310379,\n",
      "            -0.14681747555732727\n",
      "          ],\n",
      "          [\n",
      "            0.19192564487457275,\n",
      "            -0.12214168161153793,\n",
      "            0.1051124781370163,\n",
      "            0.10564000904560089,\n",
      "            -0.1816210299730301,\n",
      "            0.10220503807067871,\n",
      "            -0.03637777641415596,\n",
      "            -0.1752026230096817,\n",
      "            -0.05453238636255264,\n",
      "            0.18283380568027496,\n",
      "            -0.054227229207754135,\n",
      "            -0.06681587547063828,\n",
      "            0.16449829936027527,\n",
      "            0.12598814070224762,\n",
      "            -0.09170567244291306,\n",
      "            -0.0605054646730423,\n",
      "            0.00147022376768291,\n",
      "            -0.09612581878900528,\n",
      "            -0.11296559125185013,\n",
      "            -0.1191626489162445,\n",
      "            -0.1662263572216034,\n",
      "            0.006658857222646475,\n",
      "            -0.10539258271455765,\n",
      "            -0.015531858429312706\n",
      "          ],\n",
      "          [\n",
      "            0.18365997076034546,\n",
      "            -0.04568187892436981,\n",
      "            -0.09052286297082901,\n",
      "            0.18189072608947754,\n",
      "            -0.059055447578430176,\n",
      "            -0.16358497738838196,\n",
      "            0.14780761301517487,\n",
      "            -0.08009075373411179,\n",
      "            -0.11051054298877716,\n",
      "            -0.01242885459214449,\n",
      "            0.203174889087677,\n",
      "            -0.10201798379421234,\n",
      "            0.1837754100561142,\n",
      "            0.08001527935266495,\n",
      "            -0.04910562187433243,\n",
      "            0.06891309469938278,\n",
      "            -0.06379636377096176,\n",
      "            -0.0759504958987236,\n",
      "            -0.16172149777412415,\n",
      "            0.1404861956834793,\n",
      "            0.1609651744365692,\n",
      "            -0.19385671615600586,\n",
      "            -0.015208193100988865,\n",
      "            -0.13131296634674072\n",
      "          ],\n",
      "          [\n",
      "            -0.036256760358810425,\n",
      "            -0.12641088664531708,\n",
      "            0.0936925858259201,\n",
      "            0.05804722383618355,\n",
      "            -0.11207990348339081,\n",
      "            0.008900566957890987,\n",
      "            0.08863504230976105,\n",
      "            -0.13453322649002075,\n",
      "            -0.08794340491294861,\n",
      "            0.15402749180793762,\n",
      "            -0.0011991009814664721,\n",
      "            -0.11734245717525482,\n",
      "            -0.12619510293006897,\n",
      "            0.02979029342532158,\n",
      "            0.12701979279518127,\n",
      "            0.07876447588205338,\n",
      "            0.09392305463552475,\n",
      "            -0.019823528826236725,\n",
      "            -0.053852468729019165,\n",
      "            0.10795589536428452,\n",
      "            -0.0003319011302664876,\n",
      "            0.05016369745135307,\n",
      "            -0.10513019561767578,\n",
      "            0.09143292158842087\n",
      "          ],\n",
      "          [\n",
      "            -0.18468354642391205,\n",
      "            -0.08544155210256577,\n",
      "            -0.008726847358047962,\n",
      "            -0.0003052930114790797,\n",
      "            0.12459158897399902,\n",
      "            -0.06341385096311569,\n",
      "            -0.18198208510875702,\n",
      "            0.04977422580122948,\n",
      "            -0.04259364306926727,\n",
      "            -0.19522172212600708,\n",
      "            0.15118172764778137,\n",
      "            -0.056039419025182724,\n",
      "            0.08820943534374237,\n",
      "            -0.036048635840415955,\n",
      "            -0.06931960582733154,\n",
      "            0.19228647649288177,\n",
      "            0.043294355273246765,\n",
      "            0.13494202494621277,\n",
      "            -0.17880794405937195,\n",
      "            0.07264021039009094,\n",
      "            -0.009904194623231888,\n",
      "            0.02673872746527195,\n",
      "            0.2111920714378357,\n",
      "            -0.06853338330984116\n",
      "          ],\n",
      "          [\n",
      "            -0.05788125470280647,\n",
      "            -0.1495351940393448,\n",
      "            0.04108988121151924,\n",
      "            -0.12545867264270782,\n",
      "            0.0030290440190583467,\n",
      "            0.17411945760250092,\n",
      "            0.05027681961655617,\n",
      "            -0.2061237245798111,\n",
      "            -0.0843985453248024,\n",
      "            0.17356839776039124,\n",
      "            -0.16035018861293793,\n",
      "            0.1476215422153473,\n",
      "            0.15691635012626648,\n",
      "            -0.13163776695728302,\n",
      "            0.0032303943298757076,\n",
      "            0.08306615054607391,\n",
      "            0.027761179953813553,\n",
      "            0.027414890006184578,\n",
      "            0.09896006435155869,\n",
      "            0.03792469576001167,\n",
      "            -0.06896723061800003,\n",
      "            -0.11858132481575012,\n",
      "            0.014988381415605545,\n",
      "            -0.07766818255186081\n",
      "          ],\n",
      "          [\n",
      "            0.09046012163162231,\n",
      "            0.09683293104171753,\n",
      "            0.04737699031829834,\n",
      "            0.0947832316160202,\n",
      "            -0.05851298198103905,\n",
      "            -0.16304540634155273,\n",
      "            -0.05650804191827774,\n",
      "            -0.20334762334823608,\n",
      "            0.10054667294025421,\n",
      "            -0.026701616123318672,\n",
      "            0.10617227107286453,\n",
      "            -0.11650878190994263,\n",
      "            0.1061597689986229,\n",
      "            -0.08953937143087387,\n",
      "            0.1953742951154709,\n",
      "            0.03993398696184158,\n",
      "            -0.09737921506166458,\n",
      "            -0.1540301889181137,\n",
      "            0.08583153039216995,\n",
      "            -0.039329104125499725,\n",
      "            0.10534307360649109,\n",
      "            -0.032413966953754425,\n",
      "            0.035808999091386795,\n",
      "            -0.1977406144142151\n",
      "          ],\n",
      "          [\n",
      "            0.17768999934196472,\n",
      "            0.09973183274269104,\n",
      "            -0.17620757222175598,\n",
      "            -0.1345711648464203,\n",
      "            -0.06819555908441544,\n",
      "            -0.1488129049539566,\n",
      "            -0.007757213898003101,\n",
      "            0.1795593500137329,\n",
      "            0.17070238292217255,\n",
      "            0.0507824681699276,\n",
      "            0.15644532442092896,\n",
      "            -0.04179459437727928,\n",
      "            -0.1586787849664688,\n",
      "            0.15047743916511536,\n",
      "            -0.05816912651062012,\n",
      "            0.06187820062041283,\n",
      "            0.05016772821545601,\n",
      "            0.09659221023321152,\n",
      "            0.23435331881046295,\n",
      "            -0.01277164462953806,\n",
      "            0.23770782351493835,\n",
      "            0.21392400562763214,\n",
      "            0.32951250672340393,\n",
      "            0.3719085156917572\n",
      "          ],\n",
      "          [\n",
      "            -0.010868444107472897,\n",
      "            0.2063784897327423,\n",
      "            -0.14864297211170197,\n",
      "            -0.09674187749624252,\n",
      "            -0.011769918724894524,\n",
      "            0.11177614331245422,\n",
      "            0.10518385469913483,\n",
      "            -0.14894184470176697,\n",
      "            0.0390298068523407,\n",
      "            0.1073085367679596,\n",
      "            0.08295958489179611,\n",
      "            -0.0017025748966261744,\n",
      "            -0.014314028434455395,\n",
      "            -0.1236366331577301,\n",
      "            0.08909428864717484,\n",
      "            0.06580990552902222,\n",
      "            -0.10737784206867218,\n",
      "            0.054412681609392166,\n",
      "            0.05749749764800072,\n",
      "            0.058901410549879074,\n",
      "            0.06142725422978401,\n",
      "            -0.08457119762897491,\n",
      "            0.10523450374603271,\n",
      "            -0.007924070581793785\n",
      "          ],\n",
      "          [\n",
      "            0.09746108204126358,\n",
      "            0.20845524966716766,\n",
      "            0.034296948462724686,\n",
      "            -0.08901545405387878,\n",
      "            0.04333801940083504,\n",
      "            0.03669627383351326,\n",
      "            -0.15394894778728485,\n",
      "            0.0625862255692482,\n",
      "            -0.00970433373004198,\n",
      "            0.08243073523044586,\n",
      "            0.10753835737705231,\n",
      "            0.0965203121304512,\n",
      "            -0.0622192919254303,\n",
      "            0.1762089729309082,\n",
      "            0.021503793075680733,\n",
      "            0.11125598847866058,\n",
      "            -0.08499608188867569,\n",
      "            0.0492468923330307,\n",
      "            0.09193695336580276,\n",
      "            -0.14501236379146576,\n",
      "            -0.006428760942071676,\n",
      "            0.052405621856451035,\n",
      "            -0.020004762336611748,\n",
      "            0.06075216457247734\n",
      "          ],\n",
      "          [\n",
      "            0.10685105621814728,\n",
      "            -0.030542610213160515,\n",
      "            0.15296946465969086,\n",
      "            0.1352674514055252,\n",
      "            0.05747721344232559,\n",
      "            -0.16288471221923828,\n",
      "            0.1178218275308609,\n",
      "            0.12947864830493927,\n",
      "            -0.15194320678710938,\n",
      "            -0.20037341117858887,\n",
      "            0.13864605128765106,\n",
      "            0.01655358076095581,\n",
      "            -0.20871132612228394,\n",
      "            -0.21051698923110962,\n",
      "            0.031125523149967194,\n",
      "            -0.19430148601531982,\n",
      "            0.18490466475486755,\n",
      "            0.14358370006084442,\n",
      "            0.02776634506881237,\n",
      "            -0.08496810495853424,\n",
      "            -0.025270486250519753,\n",
      "            0.026277797296643257,\n",
      "            0.17957423627376556,\n",
      "            0.39794060587882996\n",
      "          ],\n",
      "          [\n",
      "            -0.018904080614447594,\n",
      "            0.1783970147371292,\n",
      "            -0.15084876120090485,\n",
      "            0.008229770697653294,\n",
      "            -0.06123626232147217,\n",
      "            0.1310315728187561,\n",
      "            0.02305690199136734,\n",
      "            -0.1629432737827301,\n",
      "            0.03520895913243294,\n",
      "            0.00454472191631794,\n",
      "            -0.17720778286457062,\n",
      "            0.19475413858890533,\n",
      "            0.04410117492079735,\n",
      "            -0.1490202099084854,\n",
      "            0.07820990681648254,\n",
      "            0.0207248255610466,\n",
      "            -0.12473831325769424,\n",
      "            -0.18404582142829895,\n",
      "            0.1489478498697281,\n",
      "            0.05816430598497391,\n",
      "            0.14443276822566986,\n",
      "            0.06498908996582031,\n",
      "            0.02088956907391548,\n",
      "            0.08255103975534439\n",
      "          ],\n",
      "          [\n",
      "            0.07164754718542099,\n",
      "            0.013969934545457363,\n",
      "            -0.09086831659078598,\n",
      "            -0.16088220477104187,\n",
      "            -0.1971898078918457,\n",
      "            0.12386953085660934,\n",
      "            0.03088170289993286,\n",
      "            -0.004570258315652609,\n",
      "            -0.20924623310565948,\n",
      "            -0.1898028552532196,\n",
      "            0.008780844509601593,\n",
      "            -0.10618656128644943,\n",
      "            -0.04768149182200432,\n",
      "            0.14184068143367767,\n",
      "            0.166505828499794,\n",
      "            -0.1081894263625145,\n",
      "            0.031899455934762955,\n",
      "            0.0789850652217865,\n",
      "            -0.04597678408026695,\n",
      "            -0.06392484903335571,\n",
      "            -0.16223779320716858,\n",
      "            0.08465290814638138,\n",
      "            0.27863073348999023,\n",
      "            0.256095290184021\n",
      "          ],\n",
      "          [\n",
      "            -0.004973485600203276,\n",
      "            -0.04774371162056923,\n",
      "            -0.1486538052558899,\n",
      "            0.040316805243492126,\n",
      "            0.05554584413766861,\n",
      "            0.07942855358123779,\n",
      "            -0.017314711585640907,\n",
      "            0.18712221086025238,\n",
      "            0.11108910292387009,\n",
      "            0.13837039470672607,\n",
      "            0.0435107983648777,\n",
      "            -0.1884421557188034,\n",
      "            0.14534690976142883,\n",
      "            0.16167421638965607,\n",
      "            -0.1696116179227829,\n",
      "            -0.06333517283201218,\n",
      "            0.017150605097413063,\n",
      "            0.043383918702602386,\n",
      "            0.20357275009155273,\n",
      "            0.08920766413211823,\n",
      "            -0.02668057382106781,\n",
      "            0.052392054349184036,\n",
      "            0.35474008321762085,\n",
      "            0.4549371898174286\n",
      "          ],\n",
      "          [\n",
      "            -0.13769374787807465,\n",
      "            -0.19522176682949066,\n",
      "            -0.1611686795949936,\n",
      "            0.07787598669528961,\n",
      "            -0.022380102425813675,\n",
      "            0.02412736788392067,\n",
      "            0.06693088263273239,\n",
      "            0.14637993276119232,\n",
      "            0.10560465604066849,\n",
      "            -0.18195170164108276,\n",
      "            0.1430615782737732,\n",
      "            -0.0014877277426421642,\n",
      "            -0.03979751095175743,\n",
      "            -0.17254668474197388,\n",
      "            -0.03547188639640808,\n",
      "            -0.14148254692554474,\n",
      "            0.01585262082517147,\n",
      "            -0.02644520439207554,\n",
      "            0.02392948977649212,\n",
      "            0.01914128102362156,\n",
      "            0.17139650881290436,\n",
      "            -0.10966125875711441,\n",
      "            0.28958749771118164,\n",
      "            0.3433617651462555\n",
      "          ],\n",
      "          [\n",
      "            0.1933661848306656,\n",
      "            -0.14388936758041382,\n",
      "            -0.13836565613746643,\n",
      "            0.09599924087524414,\n",
      "            -0.09680930525064468,\n",
      "            -0.05591908097267151,\n",
      "            0.08255089074373245,\n",
      "            0.13170771300792694,\n",
      "            0.06510753184556961,\n",
      "            0.07019165903329849,\n",
      "            0.09213006496429443,\n",
      "            0.03402360901236534,\n",
      "            0.145376056432724,\n",
      "            0.05377830192446709,\n",
      "            0.09486477822065353,\n",
      "            -0.06317128986120224,\n",
      "            -0.1270267516374588,\n",
      "            0.08639047294855118,\n",
      "            -0.2033984512090683,\n",
      "            0.1289927363395691,\n",
      "            -0.17696844041347504,\n",
      "            0.11622734367847443,\n",
      "            -0.2816190719604492,\n",
      "            -0.31929081678390503\n",
      "          ],\n",
      "          [\n",
      "            0.12919043004512787,\n",
      "            -0.07346395403146744,\n",
      "            0.09725531190633774,\n",
      "            0.07260853797197342,\n",
      "            -0.025131510570645332,\n",
      "            0.10970910638570786,\n",
      "            0.06867041438817978,\n",
      "            -0.05404503270983696,\n",
      "            -0.0769096165895462,\n",
      "            -0.1775171011686325,\n",
      "            -0.10982918739318848,\n",
      "            0.0027412939816713333,\n",
      "            -0.185945525765419,\n",
      "            0.09517913311719894,\n",
      "            -0.19833256304264069,\n",
      "            0.18136444687843323,\n",
      "            -0.05256829783320427,\n",
      "            0.036700718104839325,\n",
      "            0.0852406695485115,\n",
      "            0.23806516826152802,\n",
      "            0.14379705488681793,\n",
      "            0.1304245889186859,\n",
      "            0.47771236300468445,\n",
      "            0.3030966520309448\n",
      "          ],\n",
      "          [\n",
      "            -0.17273367941379547,\n",
      "            -0.16302895545959473,\n",
      "            -0.23017635941505432,\n",
      "            -0.03188499063253403,\n",
      "            -0.13804660737514496,\n",
      "            -0.009309252724051476,\n",
      "            0.03588877245783806,\n",
      "            -0.08159828186035156,\n",
      "            -0.09354814141988754,\n",
      "            -0.19918225705623627,\n",
      "            -0.1073928251862526,\n",
      "            -0.18652194738388062,\n",
      "            0.08342164009809494,\n",
      "            0.20859232544898987,\n",
      "            0.11238260567188263,\n",
      "            0.19877322018146515,\n",
      "            0.000921121216379106,\n",
      "            0.06800457835197449,\n",
      "            0.036325860768556595,\n",
      "            -0.12368349730968475,\n",
      "            -0.1699475347995758,\n",
      "            -0.08715254068374634,\n",
      "            0.19088125228881836,\n",
      "            0.15775848925113678\n",
      "          ],\n",
      "          [\n",
      "            -0.04810648038983345,\n",
      "            0.009079519659280777,\n",
      "            -0.06476093083620071,\n",
      "            -0.13944895565509796,\n",
      "            -0.004105056636035442,\n",
      "            -0.12324319779872894,\n",
      "            0.005956044886261225,\n",
      "            0.056011129170656204,\n",
      "            -0.01618482545018196,\n",
      "            -0.02470979280769825,\n",
      "            -0.06322023272514343,\n",
      "            -0.04510698467493057,\n",
      "            -0.03549279272556305,\n",
      "            0.09450967609882355,\n",
      "            -0.13095317780971527,\n",
      "            0.11039137095212936,\n",
      "            -0.06178773194551468,\n",
      "            0.0024735252372920513,\n",
      "            0.05147935822606087,\n",
      "            -0.09978587925434113,\n",
      "            -0.015626534819602966,\n",
      "            -0.08228310197591782,\n",
      "            0.0671810731291771,\n",
      "            0.21498243510723114\n",
      "          ],\n",
      "          [\n",
      "            0.03896685317158699,\n",
      "            0.12566804885864258,\n",
      "            0.04322076961398125,\n",
      "            -0.13667012751102448,\n",
      "            0.2210043966770172,\n",
      "            -0.11352064460515976,\n",
      "            0.14494389295578003,\n",
      "            0.10929224640130997,\n",
      "            -0.14440171420574188,\n",
      "            -0.019352106377482414,\n",
      "            0.1629488468170166,\n",
      "            0.12299346923828125,\n",
      "            0.14161422848701477,\n",
      "            -0.16791535913944244,\n",
      "            0.11813398450613022,\n",
      "            0.08141680061817169,\n",
      "            -0.16247910261154175,\n",
      "            -0.03173300251364708,\n",
      "            0.02120744064450264,\n",
      "            -0.2065880000591278,\n",
      "            0.10467052459716797,\n",
      "            -0.001816531759686768,\n",
      "            -0.21359306573867798,\n",
      "            -0.020682888105511665\n",
      "          ],\n",
      "          [\n",
      "            -0.15639078617095947,\n",
      "            0.16465502977371216,\n",
      "            -0.07449029386043549,\n",
      "            -0.1053965613245964,\n",
      "            0.1411343365907669,\n",
      "            0.132914736866951,\n",
      "            -0.04366855323314667,\n",
      "            -0.03375197947025299,\n",
      "            -0.16704827547073364,\n",
      "            -0.11374931037425995,\n",
      "            0.0946885272860527,\n",
      "            -0.15619494020938873,\n",
      "            -0.12491566687822342,\n",
      "            0.18244969844818115,\n",
      "            0.04704626649618149,\n",
      "            -0.15347222983837128,\n",
      "            -0.16937319934368134,\n",
      "            0.02101030945777893,\n",
      "            0.07535988092422485,\n",
      "            0.13422566652297974,\n",
      "            -0.07222426682710648,\n",
      "            0.10807311534881592,\n",
      "            -0.021736444905400276,\n",
      "            0.05001085251569748\n",
      "          ],\n",
      "          [\n",
      "            0.1395065039396286,\n",
      "            0.13961316645145416,\n",
      "            -0.05081234127283096,\n",
      "            -0.032917894423007965,\n",
      "            -0.12979620695114136,\n",
      "            0.039230458438396454,\n",
      "            0.029056916013360023,\n",
      "            -0.0473472885787487,\n",
      "            0.19220229983329773,\n",
      "            -0.0650477185845375,\n",
      "            0.11483313143253326,\n",
      "            0.1408672332763672,\n",
      "            0.20234763622283936,\n",
      "            0.19089660048484802,\n",
      "            -0.17242231965065002,\n",
      "            -0.04217245429754257,\n",
      "            0.1855243742465973,\n",
      "            -0.12312032282352448,\n",
      "            -0.028550898656249046,\n",
      "            0.010870824567973614,\n",
      "            0.1375383585691452,\n",
      "            -0.1220388114452362,\n",
      "            -0.049902334809303284,\n",
      "            0.07012247294187546\n",
      "          ],\n",
      "          [\n",
      "            0.1282426416873932,\n",
      "            -0.037276625633239746,\n",
      "            -0.05944548547267914,\n",
      "            0.04460015892982483,\n",
      "            -0.17644505202770233,\n",
      "            -0.05265272408723831,\n",
      "            -0.050080087035894394,\n",
      "            -0.1011599749326706,\n",
      "            0.012788685970008373,\n",
      "            -0.005350034683942795,\n",
      "            -0.008477505296468735,\n",
      "            0.14673277735710144,\n",
      "            0.20897197723388672,\n",
      "            -0.11131679266691208,\n",
      "            0.03255778178572655,\n",
      "            0.137104332447052,\n",
      "            0.09735196828842163,\n",
      "            0.03842959553003311,\n",
      "            0.1287834197282791,\n",
      "            -0.038769736886024475,\n",
      "            -0.08772315829992294,\n",
      "            0.02631511352956295,\n",
      "            0.05378161370754242,\n",
      "            0.08749639987945557\n",
      "          ],\n",
      "          [\n",
      "            -0.01131347008049488,\n",
      "            -0.02335307188332081,\n",
      "            -0.14338341355323792,\n",
      "            -0.1340426802635193,\n",
      "            -0.12686540186405182,\n",
      "            -0.028364969417452812,\n",
      "            -0.17556016147136688,\n",
      "            -0.037099700421094894,\n",
      "            -0.1603119671344757,\n",
      "            0.18284381926059723,\n",
      "            0.16616880893707275,\n",
      "            0.08114323019981384,\n",
      "            0.019220849499106407,\n",
      "            0.06124576926231384,\n",
      "            0.16674727201461792,\n",
      "            -0.1871519684791565,\n",
      "            0.041414931416511536,\n",
      "            0.047341231256723404,\n",
      "            -0.11840634047985077,\n",
      "            -0.1281960904598236,\n",
      "            0.1827087104320526,\n",
      "            -0.16819830238819122,\n",
      "            -0.066133052110672,\n",
      "            -0.14168253540992737\n",
      "          ],\n",
      "          [\n",
      "            -0.0020359158515930176,\n",
      "            0.12196366488933563,\n",
      "            0.004571482539176941,\n",
      "            -0.046948134899139404,\n",
      "            -0.03134812414646149,\n",
      "            -0.08473673462867737,\n",
      "            -0.16217102110385895,\n",
      "            -0.10415966808795929,\n",
      "            0.09798388183116913,\n",
      "            -0.07716026157140732,\n",
      "            -0.018019065260887146,\n",
      "            0.053420498967170715,\n",
      "            -0.00744687020778656,\n",
      "            -0.03691554069519043,\n",
      "            -0.13961856067180634,\n",
      "            0.03687649965286255,\n",
      "            0.06431649625301361,\n",
      "            0.1796538084745407,\n",
      "            0.12169112265110016,\n",
      "            -0.13777776062488556,\n",
      "            0.08299459517002106,\n",
      "            -0.08143128454685211,\n",
      "            -0.1974312961101532,\n",
      "            -0.02785693109035492\n",
      "          ],\n",
      "          [\n",
      "            0.16729970276355743,\n",
      "            0.16071362793445587,\n",
      "            0.12671895325183868,\n",
      "            -0.038679759949445724,\n",
      "            0.09509079903364182,\n",
      "            0.12257838994264603,\n",
      "            0.034446652978658676,\n",
      "            -0.03893265500664711,\n",
      "            0.014550935477018356,\n",
      "            0.18350182473659515,\n",
      "            0.17233142256736755,\n",
      "            0.13799549639225006,\n",
      "            0.042297955602407455,\n",
      "            0.019995836541056633,\n",
      "            0.1489586979150772,\n",
      "            0.20251937210559845,\n",
      "            0.022823648527264595,\n",
      "            0.06512635946273804,\n",
      "            -0.22390446066856384,\n",
      "            0.08588463068008423,\n",
      "            -0.1083875447511673,\n",
      "            -0.0018671848811209202,\n",
      "            -0.27420976758003235,\n",
      "            -0.11049029231071472\n",
      "          ],\n",
      "          [\n",
      "            0.07299529761075974,\n",
      "            -0.13217340409755707,\n",
      "            -0.12341688573360443,\n",
      "            0.16681528091430664,\n",
      "            -0.19220182299613953,\n",
      "            0.013730336911976337,\n",
      "            -0.08889996260404587,\n",
      "            -0.08109346032142639,\n",
      "            0.07818710803985596,\n",
      "            -0.15712258219718933,\n",
      "            -0.020181598141789436,\n",
      "            -0.011628156527876854,\n",
      "            -0.05537252128124237,\n",
      "            -0.13014984130859375,\n",
      "            -0.09378574788570404,\n",
      "            0.19810958206653595,\n",
      "            -0.026420677080750465,\n",
      "            0.17730826139450073,\n",
      "            0.12690287828445435,\n",
      "            0.17307554185390472,\n",
      "            0.07500199973583221,\n",
      "            -0.0732642263174057,\n",
      "            0.008021559566259384,\n",
      "            0.19697785377502441\n",
      "          ],\n",
      "          [\n",
      "            -0.1649748980998993,\n",
      "            0.15827614068984985,\n",
      "            0.035403456538915634,\n",
      "            -0.09434788674116135,\n",
      "            -0.09266839921474457,\n",
      "            0.012088386341929436,\n",
      "            -0.1054387241601944,\n",
      "            0.052879687398672104,\n",
      "            -0.18847671151161194,\n",
      "            -0.10867083817720413,\n",
      "            -0.10900846123695374,\n",
      "            0.07545896619558334,\n",
      "            0.10134243965148926,\n",
      "            -0.1434381753206253,\n",
      "            0.07133882492780685,\n",
      "            0.221314936876297,\n",
      "            0.14797520637512207,\n",
      "            -0.11683978140354156,\n",
      "            -0.13971126079559326,\n",
      "            -0.1485753357410431,\n",
      "            0.16566860675811768,\n",
      "            0.1912546157836914,\n",
      "            -0.19554951786994934,\n",
      "            -0.07706140726804733\n",
      "          ],\n",
      "          [\n",
      "            -0.08185969293117523,\n",
      "            -0.11585155129432678,\n",
      "            0.07667951285839081,\n",
      "            0.09021744132041931,\n",
      "            -0.10681787133216858,\n",
      "            0.17165419459342957,\n",
      "            -0.12214593589305878,\n",
      "            0.15938647091388702,\n",
      "            -0.02489846758544445,\n",
      "            -0.08863614499568939,\n",
      "            0.1566561758518219,\n",
      "            -0.194014310836792,\n",
      "            0.13628524541854858,\n",
      "            0.10498487204313278,\n",
      "            -0.16550010442733765,\n",
      "            0.1253395974636078,\n",
      "            0.16730928421020508,\n",
      "            -0.16048605740070343,\n",
      "            -0.07497002184391022,\n",
      "            0.18815453350543976,\n",
      "            0.2521083354949951,\n",
      "            0.005567614920437336,\n",
      "            0.11377567797899246,\n",
      "            0.21228764951229095\n",
      "          ],\n",
      "          [\n",
      "            0.028694117441773415,\n",
      "            -0.1380215883255005,\n",
      "            0.08440791070461273,\n",
      "            -0.16796621680259705,\n",
      "            0.01998533308506012,\n",
      "            0.11163431406021118,\n",
      "            0.030123675242066383,\n",
      "            -0.10283520817756653,\n",
      "            0.02882823534309864,\n",
      "            0.1580577790737152,\n",
      "            -0.036702051758766174,\n",
      "            -0.08118460327386856,\n",
      "            -0.063262939453125,\n",
      "            -0.09024985134601593,\n",
      "            -0.1333467811346054,\n",
      "            -0.17441348731517792,\n",
      "            -0.1287466287612915,\n",
      "            -0.18421584367752075,\n",
      "            0.006106375250965357,\n",
      "            0.18106861412525177,\n",
      "            -0.1246660053730011,\n",
      "            0.11140479892492294,\n",
      "            0.1567804515361786,\n",
      "            -0.08923965692520142\n",
      "          ],\n",
      "          [\n",
      "            -0.15538345277309418,\n",
      "            -0.08398296684026718,\n",
      "            0.10746195912361145,\n",
      "            0.18910132348537445,\n",
      "            0.11400623619556427,\n",
      "            0.16755922138690948,\n",
      "            0.12937316298484802,\n",
      "            0.2693321704864502,\n",
      "            0.14614711701869965,\n",
      "            -0.18988628685474396,\n",
      "            0.06413683295249939,\n",
      "            -0.0778440460562706,\n",
      "            0.03829646483063698,\n",
      "            0.021177668124437332,\n",
      "            0.05042809247970581,\n",
      "            -0.2030320167541504,\n",
      "            0.1046205684542656,\n",
      "            -0.13837511837482452,\n",
      "            0.11709977686405182,\n",
      "            -0.038445379585027695,\n",
      "            0.12352053821086884,\n",
      "            0.14572502672672272,\n",
      "            -0.00472985440865159,\n",
      "            0.040550995618104935\n",
      "          ],\n",
      "          [\n",
      "            0.16340813040733337,\n",
      "            0.10858683288097382,\n",
      "            0.020390475168824196,\n",
      "            0.005450871307402849,\n",
      "            0.2081909328699112,\n",
      "            0.2094077169895172,\n",
      "            0.15032145380973816,\n",
      "            -0.12144425511360168,\n",
      "            0.05630210041999817,\n",
      "            -0.015359251759946346,\n",
      "            -0.02281632460653782,\n",
      "            0.012138578109443188,\n",
      "            -0.05646850913763046,\n",
      "            0.1265990287065506,\n",
      "            0.14593981206417084,\n",
      "            -0.07808025181293488,\n",
      "            -0.00010987772111548111,\n",
      "            -0.12992335855960846,\n",
      "            -0.1437564194202423,\n",
      "            -0.02509564533829689,\n",
      "            -0.15854661166667938,\n",
      "            -0.023254217579960823,\n",
      "            -0.2525293231010437,\n",
      "            -0.22406421601772308\n",
      "          ],\n",
      "          [\n",
      "            0.06975939124822617,\n",
      "            -0.16941837966442108,\n",
      "            0.17536720633506775,\n",
      "            -0.07513758540153503,\n",
      "            0.009913718327879906,\n",
      "            0.187277689576149,\n",
      "            -0.13690043985843658,\n",
      "            0.22045330703258514,\n",
      "            -0.07075654715299606,\n",
      "            0.1770084649324417,\n",
      "            0.07311932742595673,\n",
      "            0.2037198394536972,\n",
      "            -0.01159573346376419,\n",
      "            0.025626040995121002,\n",
      "            -0.11026546359062195,\n",
      "            0.10508795827627182,\n",
      "            0.0035479897633194923,\n",
      "            -0.03966192156076431,\n",
      "            -0.016231928020715714,\n",
      "            -0.13808812201023102,\n",
      "            0.03130121901631355,\n",
      "            -0.2003350555896759,\n",
      "            -0.07972091436386108,\n",
      "            -0.31337589025497437\n",
      "          ],\n",
      "          [\n",
      "            -0.18450282514095306,\n",
      "            0.0044351015239953995,\n",
      "            -0.12946557998657227,\n",
      "            -0.04213656112551689,\n",
      "            0.1510486751794815,\n",
      "            0.057690780609846115,\n",
      "            -0.10721287876367569,\n",
      "            -0.17366895079612732,\n",
      "            0.10873512178659439,\n",
      "            -0.0612424798309803,\n",
      "            0.1828463077545166,\n",
      "            -0.08492081612348557,\n",
      "            0.15306341648101807,\n",
      "            -0.040157582610845566,\n",
      "            0.2309618592262268,\n",
      "            0.17467869818210602,\n",
      "            -0.03376604989171028,\n",
      "            -0.012654046528041363,\n",
      "            -0.14642900228500366,\n",
      "            0.1436484456062317,\n",
      "            0.18943677842617035,\n",
      "            0.09288674592971802,\n",
      "            -0.140781968832016,\n",
      "            -0.0571017861366272\n",
      "          ],\n",
      "          [\n",
      "            0.07033491134643555,\n",
      "            0.09014776349067688,\n",
      "            0.09102747589349747,\n",
      "            0.012186402454972267,\n",
      "            -0.13603822886943817,\n",
      "            0.08234819769859314,\n",
      "            0.06536301970481873,\n",
      "            -0.023069946095347404,\n",
      "            -0.18623694777488708,\n",
      "            0.10478752851486206,\n",
      "            -0.026186570525169373,\n",
      "            0.10571763664484024,\n",
      "            -0.212550088763237,\n",
      "            0.06739894300699234,\n",
      "            0.09185846149921417,\n",
      "            -0.09603745490312576,\n",
      "            -0.010646666400134563,\n",
      "            0.07006645202636719,\n",
      "            0.0020426432602107525,\n",
      "            0.1502060890197754,\n",
      "            -0.04107416048645973,\n",
      "            0.10452630370855331,\n",
      "            0.1555752456188202,\n",
      "            0.31015297770500183\n",
      "          ],\n",
      "          [\n",
      "            0.1358606368303299,\n",
      "            -0.12522105872631073,\n",
      "            0.013255936093628407,\n",
      "            -0.040890466421842575,\n",
      "            -0.003366751130670309,\n",
      "            0.03814863786101341,\n",
      "            0.16271381080150604,\n",
      "            -0.18905319273471832,\n",
      "            -0.021524783223867416,\n",
      "            -0.027675893157720566,\n",
      "            -0.18403933942317963,\n",
      "            0.0031044762581586838,\n",
      "            -0.11161863058805466,\n",
      "            0.19147266447544098,\n",
      "            -0.15916478633880615,\n",
      "            0.13384652137756348,\n",
      "            0.16583415865898132,\n",
      "            -0.10369149595499039,\n",
      "            -0.15687879920005798,\n",
      "            0.005101673770695925,\n",
      "            -0.032010216265916824,\n",
      "            0.06304522603750229,\n",
      "            0.22667524218559265,\n",
      "            0.05154542997479439\n",
      "          ],\n",
      "          [\n",
      "            -0.0662267729640007,\n",
      "            0.17398926615715027,\n",
      "            -0.15634599328041077,\n",
      "            0.11960114538669586,\n",
      "            -0.07848301529884338,\n",
      "            -0.040637947618961334,\n",
      "            0.09230154752731323,\n",
      "            0.092147096991539,\n",
      "            -0.1339850276708603,\n",
      "            -0.17587526142597198,\n",
      "            -0.11656511574983597,\n",
      "            0.13935504853725433,\n",
      "            0.009930435568094254,\n",
      "            0.15313144028186798,\n",
      "            0.019469555467367172,\n",
      "            0.048259537667036057,\n",
      "            -0.0009581237682141364,\n",
      "            0.1400611698627472,\n",
      "            0.041842736303806305,\n",
      "            0.04863949120044708,\n",
      "            -0.23767893016338348,\n",
      "            0.08868948370218277,\n",
      "            -0.15376481413841248,\n",
      "            -0.16633054614067078\n",
      "          ],\n",
      "          [\n",
      "            0.12447293847799301,\n",
      "            -0.07014047354459763,\n",
      "            0.10216457396745682,\n",
      "            -0.17016461491584778,\n",
      "            0.061372291296720505,\n",
      "            0.12731394171714783,\n",
      "            0.13762077689170837,\n",
      "            -0.07676868140697479,\n",
      "            -0.059428222477436066,\n",
      "            0.0012689345749095082,\n",
      "            -0.1666828691959381,\n",
      "            -0.13069505989551544,\n",
      "            -0.1431817263364792,\n",
      "            -0.09631530940532684,\n",
      "            0.018204182386398315,\n",
      "            0.14480778574943542,\n",
      "            0.1170220896601677,\n",
      "            -0.08450070768594742,\n",
      "            0.004946999717503786,\n",
      "            -0.046478260308504105,\n",
      "            0.032706089317798615,\n",
      "            -0.13664105534553528,\n",
      "            0.3275165259838104,\n",
      "            0.36965692043304443\n",
      "          ],\n",
      "          [\n",
      "            0.17714270949363708,\n",
      "            -0.16070067882537842,\n",
      "            -0.1113295778632164,\n",
      "            0.2281288355588913,\n",
      "            0.18245896697044373,\n",
      "            0.08328083157539368,\n",
      "            0.09220995754003525,\n",
      "            0.037573717534542084,\n",
      "            -0.10719909518957138,\n",
      "            -0.042981911450624466,\n",
      "            0.09363152086734772,\n",
      "            0.08539828658103943,\n",
      "            0.029985425993800163,\n",
      "            -0.16670021414756775,\n",
      "            -0.10002268850803375,\n",
      "            0.20516341924667358,\n",
      "            -0.05693773180246353,\n",
      "            0.09959500283002853,\n",
      "            -0.027075456455349922,\n",
      "            0.10663250833749771,\n",
      "            0.04837549850344658,\n",
      "            0.09217212349176407,\n",
      "            -0.16494013369083405,\n",
      "            -0.1733676791191101\n",
      "          ],\n",
      "          [\n",
      "            -0.17092366516590118,\n",
      "            -0.046156659722328186,\n",
      "            -0.09778991341590881,\n",
      "            -0.0038406262174248695,\n",
      "            -0.11752866208553314,\n",
      "            -0.12234338372945786,\n",
      "            0.017931215465068817,\n",
      "            -0.1921447366476059,\n",
      "            -0.00782407820224762,\n",
      "            -0.11524476855993271,\n",
      "            0.19270770251750946,\n",
      "            -0.07702184468507767,\n",
      "            0.194406658411026,\n",
      "            -0.035896897315979004,\n",
      "            -0.07428935915231705,\n",
      "            -0.10329949110746384,\n",
      "            -0.030776619911193848,\n",
      "            -0.04904739558696747,\n",
      "            -0.04403167963027954,\n",
      "            0.0869966596364975,\n",
      "            0.12709029018878937,\n",
      "            0.155679389834404,\n",
      "            0.04500606656074524,\n",
      "            -0.12752223014831543\n",
      "          ],\n",
      "          [\n",
      "            -0.13686953485012054,\n",
      "            -0.049828533083200455,\n",
      "            0.11057532578706741,\n",
      "            -0.04866417497396469,\n",
      "            0.14800655841827393,\n",
      "            -0.18048860132694244,\n",
      "            0.1250045895576477,\n",
      "            -0.02773238718509674,\n",
      "            0.18120436370372772,\n",
      "            -0.03341681882739067,\n",
      "            0.17693226039409637,\n",
      "            -0.14018061757087708,\n",
      "            0.07486698031425476,\n",
      "            0.11749526858329773,\n",
      "            -0.15799500048160553,\n",
      "            0.09101488441228867,\n",
      "            -0.040881577879190445,\n",
      "            -0.15751349925994873,\n",
      "            0.17736726999282837,\n",
      "            -0.20077306032180786,\n",
      "            -0.13462293148040771,\n",
      "            -0.12904810905456543,\n",
      "            -0.04893503710627556,\n",
      "            -0.03672974184155464\n",
      "          ],\n",
      "          [\n",
      "            0.19781188666820526,\n",
      "            -0.15396255254745483,\n",
      "            -0.13435152173042297,\n",
      "            0.20193925499916077,\n",
      "            0.19174805283546448,\n",
      "            0.15407048165798187,\n",
      "            -0.10862035304307938,\n",
      "            -0.15544520318508148,\n",
      "            -0.05979204922914505,\n",
      "            -0.15760332345962524,\n",
      "            0.18214240670204163,\n",
      "            0.06006263196468353,\n",
      "            -0.10321743041276932,\n",
      "            0.009096191264688969,\n",
      "            -0.02308640256524086,\n",
      "            0.20131424069404602,\n",
      "            0.20157846808433533,\n",
      "            0.11386103183031082,\n",
      "            0.1108635887503624,\n",
      "            -0.16477124392986298,\n",
      "            0.1749701201915741,\n",
      "            0.012246151454746723,\n",
      "            0.0024966869968920946,\n",
      "            0.16473107039928436\n",
      "          ],\n",
      "          [\n",
      "            0.07125741988420486,\n",
      "            -0.05328415706753731,\n",
      "            -0.04207199439406395,\n",
      "            -0.024373481050133705,\n",
      "            0.04808591678738594,\n",
      "            0.01285792700946331,\n",
      "            -0.15504275262355804,\n",
      "            -0.1586512178182602,\n",
      "            -0.14377117156982422,\n",
      "            0.015646371990442276,\n",
      "            -0.12171898037195206,\n",
      "            0.01689147762954235,\n",
      "            0.20613877475261688,\n",
      "            0.12316378951072693,\n",
      "            -0.10540638864040375,\n",
      "            0.1735020875930786,\n",
      "            0.07487840950489044,\n",
      "            -0.006179118528962135,\n",
      "            0.10506285727024078,\n",
      "            -0.13313569128513336,\n",
      "            -0.09601812809705734,\n",
      "            0.09227219223976135,\n",
      "            0.10947102308273315,\n",
      "            0.051060453057289124\n",
      "          ],\n",
      "          [\n",
      "            0.15874487161636353,\n",
      "            -0.11896289139986038,\n",
      "            0.017911596223711967,\n",
      "            -0.008365543559193611,\n",
      "            -0.05048450082540512,\n",
      "            -0.07237359881401062,\n",
      "            0.08351394534111023,\n",
      "            0.2176966816186905,\n",
      "            -0.157298281788826,\n",
      "            -0.01569567620754242,\n",
      "            -0.07057613134384155,\n",
      "            -0.02830539084970951,\n",
      "            0.1620280146598816,\n",
      "            0.20116138458251953,\n",
      "            -0.04943152889609337,\n",
      "            -0.013986772857606411,\n",
      "            -0.16068650782108307,\n",
      "            0.06321252882480621,\n",
      "            -0.08130625635385513,\n",
      "            0.01322879083454609,\n",
      "            -0.20877650380134583,\n",
      "            -0.08272136747837067,\n",
      "            -0.12110069394111633,\n",
      "            -0.3138856887817383\n",
      "          ],\n",
      "          [\n",
      "            -0.01364171039313078,\n",
      "            0.16731218993663788,\n",
      "            0.15385136008262634,\n",
      "            0.1760929822921753,\n",
      "            0.09209329634904861,\n",
      "            -0.16277283430099487,\n",
      "            -0.11051607877016068,\n",
      "            -0.08605001121759415,\n",
      "            -0.07747277617454529,\n",
      "            0.03294280916452408,\n",
      "            -0.15958477556705475,\n",
      "            0.17353419959545135,\n",
      "            -0.1263679713010788,\n",
      "            -0.10915369540452957,\n",
      "            -0.1459643542766571,\n",
      "            0.17832443118095398,\n",
      "            0.17647843062877655,\n",
      "            0.18879607319831848,\n",
      "            -0.08275237679481506,\n",
      "            0.09227566421031952,\n",
      "            -0.062076084315776825,\n",
      "            -0.16988006234169006,\n",
      "            0.057775579392910004,\n",
      "            -0.0612889789044857\n",
      "          ],\n",
      "          [\n",
      "            -0.049233462661504745,\n",
      "            -0.028221571817994118,\n",
      "            -0.03247424215078354,\n",
      "            -0.11410185694694519,\n",
      "            -0.16054362058639526,\n",
      "            0.044070422649383545,\n",
      "            0.20862466096878052,\n",
      "            0.17440316081047058,\n",
      "            -0.07676029205322266,\n",
      "            0.15319614112377167,\n",
      "            0.0230255164206028,\n",
      "            -0.01825365424156189,\n",
      "            0.17914070188999176,\n",
      "            -0.03615874797105789,\n",
      "            0.08235076069831848,\n",
      "            -0.05832995846867561,\n",
      "            0.18169699609279633,\n",
      "            0.11648695170879364,\n",
      "            0.03357119485735893,\n",
      "            0.12265097349882126,\n",
      "            -0.11143460124731064,\n",
      "            0.14264482259750366,\n",
      "            0.0768495723605156,\n",
      "            0.33976155519485474\n",
      "          ],\n",
      "          [\n",
      "            0.20799767971038818,\n",
      "            0.0027761608362197876,\n",
      "            0.06879264861345291,\n",
      "            0.14472657442092896,\n",
      "            -0.0449763722717762,\n",
      "            0.012534301728010178,\n",
      "            0.20139722526073456,\n",
      "            0.05500916391611099,\n",
      "            -0.12604591250419617,\n",
      "            0.11048832535743713,\n",
      "            0.16306176781654358,\n",
      "            0.13800030946731567,\n",
      "            -0.06405868381261826,\n",
      "            -0.0021408575121313334,\n",
      "            -0.02086218073964119,\n",
      "            0.16102732717990875,\n",
      "            -0.1763923019170761,\n",
      "            -0.13672597706317902,\n",
      "            0.002838742220774293,\n",
      "            0.01993854157626629,\n",
      "            0.08891954272985458,\n",
      "            -0.09176959097385406,\n",
      "            -0.09831119328737259,\n",
      "            -0.1455053836107254\n",
      "          ]\n",
      "        ],\n",
      "        \"bias\": [\n",
      "          0.025655575096607208,\n",
      "          -0.0329495444893837,\n",
      "          -0.0016151395393535495,\n",
      "          -0.0027611448895186186,\n",
      "          0.06726748496294022,\n",
      "          -0.013856845907866955,\n",
      "          0.02228819578886032,\n",
      "          0.04609627276659012,\n",
      "          0.02355320192873478,\n",
      "          0.04008561745285988,\n",
      "          0.031848836690187454,\n",
      "          -0.026073496788740158,\n",
      "          0.008847673423588276,\n",
      "          0.02019977755844593,\n",
      "          -0.04076772555708885,\n",
      "          0.0,\n",
      "          0.05122089758515358,\n",
      "          0.04489058256149292,\n",
      "          -0.02059980481863022,\n",
      "          0.02601921558380127,\n",
      "          -0.03358742222189903,\n",
      "          -0.009499277919530869,\n",
      "          0.025779198855161667,\n",
      "          -0.005054572131484747,\n",
      "          -0.003822634695097804,\n",
      "          0.03691690415143967,\n",
      "          0.01782752387225628,\n",
      "          0.005244242027401924,\n",
      "          0.028765659779310226,\n",
      "          0.059945981949567795,\n",
      "          -0.0018279541982337832,\n",
      "          -0.03608735650777817,\n",
      "          -0.005224983207881451,\n",
      "          -0.014872600324451923,\n",
      "          0.0,\n",
      "          -0.0035723731853067875,\n",
      "          0.005493140779435635,\n",
      "          0.01486763171851635,\n",
      "          0.09052934497594833,\n",
      "          0.029293470084667206,\n",
      "          0.01007707417011261,\n",
      "          0.01990419812500477,\n",
      "          -0.004484262317419052,\n",
      "          0.0393281988799572,\n",
      "          -0.012620984576642513,\n",
      "          0.03293609991669655,\n",
      "          0.05305583029985428,\n",
      "          -0.014530682004988194,\n",
      "          0.02324538119137287,\n",
      "          0.04523760452866554,\n",
      "          0.0674666240811348,\n",
      "          -0.002275038044899702,\n",
      "          -0.007231854368001223,\n",
      "          0.03593640774488449,\n",
      "          0.012015079148113728,\n",
      "          0.018907273188233376,\n",
      "          0.05577719211578369,\n",
      "          0.018440185114741325,\n",
      "          0.023999474942684174,\n",
      "          -0.00391743890941143,\n",
      "          0.03373740613460541,\n",
      "          0.006796537898480892,\n",
      "          0.04426213353872299,\n",
      "          0.0038450832944363356,\n",
      "          0.0605543814599514,\n",
      "          0.043936535716056824,\n",
      "          0.01126568391919136,\n",
      "          0.0037977341562509537,\n",
      "          0.0015908785862848163,\n",
      "          -0.021774912253022194,\n",
      "          0.048165932297706604,\n",
      "          0.02994130551815033,\n",
      "          -0.006709792651236057,\n",
      "          -0.00951556209474802,\n",
      "          -0.02526172809302807,\n",
      "          0.08618378639221191,\n",
      "          0.02988778054714203,\n",
      "          0.016124924644827843,\n",
      "          -0.003793245879933238,\n",
      "          0.03871789202094078,\n",
      "          -0.01076123584061861,\n",
      "          -0.02803405374288559,\n",
      "          0.045560404658317566,\n",
      "          0.01654239185154438,\n",
      "          0.013821479864418507,\n",
      "          0.051312271505594254,\n",
      "          -0.022240396589040756,\n",
      "          0.05771113187074661,\n",
      "          -0.005683929659426212,\n",
      "          -0.02067619003355503,\n",
      "          -0.018524324521422386,\n",
      "          -0.05195765569806099,\n",
      "          -0.025212667882442474,\n",
      "          -0.009180369786918163,\n",
      "          0.027471449226140976,\n",
      "          -0.0026152976788580418,\n",
      "          0.032115790992975235,\n",
      "          -0.000925331492908299,\n",
      "          0.03144557774066925,\n",
      "          -0.016637183725833893,\n",
      "          0.05115215480327606,\n",
      "          -0.031598158180713654,\n",
      "          0.011221189983189106,\n",
      "          0.03445878624916077,\n",
      "          -0.025644009932875633,\n",
      "          0.0,\n",
      "          0.023551292717456818,\n",
      "          -0.00045620318269357085,\n",
      "          0.0070912097580730915,\n",
      "          0.01547692809253931,\n",
      "          -0.009618383832275867,\n",
      "          0.016602540388703346,\n",
      "          0.03584083169698715,\n",
      "          0.012106483802199364,\n",
      "          0.0600653737783432,\n",
      "          -0.012891686521470547,\n",
      "          0.023621216416358948,\n",
      "          0.03377731889486313,\n",
      "          0.0019212199840694666,\n",
      "          0.014676903374493122,\n",
      "          -0.004248098470270634,\n",
      "          0.027087289839982986,\n",
      "          0.06470358371734619,\n",
      "          0.02921173721551895,\n",
      "          0.0466478131711483,\n",
      "          -5.665872322424548e-06,\n",
      "          0.06321212649345398,\n",
      "          -0.013803908601403236\n",
      "        ],\n",
      "        \"activation\": \"relu\"\n",
      "      },\n",
      "      {\n",
      "        \"matrix\": [\n",
      "          [\n",
      "            -0.09531625360250473,\n",
      "            0.051058653742074966,\n",
      "            0.23539792001247406,\n",
      "            -0.09433417022228241,\n",
      "            0.07981415092945099,\n",
      "            0.12982256710529327,\n",
      "            0.11049260944128036,\n",
      "            -0.016725998371839523,\n",
      "            -0.028032401576638222,\n",
      "            -0.037233658134937286,\n",
      "            0.17171116173267365,\n",
      "            0.14764554798603058,\n",
      "            0.14541640877723694,\n",
      "            -0.07546564191579819,\n",
      "            -0.039677694439888,\n",
      "            0.11945979297161102,\n",
      "            0.017256686463952065,\n",
      "            -0.003708908101543784,\n",
      "            0.13693612813949585,\n",
      "            0.030729513615369797,\n",
      "            0.021258791908621788,\n",
      "            -0.07647950202226639,\n",
      "            0.051604293286800385,\n",
      "            -0.15007919073104858,\n",
      "            0.21177372336387634,\n",
      "            0.2150149643421173,\n",
      "            -0.13224735856056213,\n",
      "            0.09025637060403824,\n",
      "            -0.12869209051132202,\n",
      "            0.02409357950091362,\n",
      "            0.13150475919246674,\n",
      "            0.03174468129873276,\n",
      "            0.21956050395965576,\n",
      "            -0.07318980246782303,\n",
      "            0.1430141180753708,\n",
      "            0.19045814871788025,\n",
      "            -0.08721964806318283,\n",
      "            -0.022005951032042503,\n",
      "            0.15245848894119263,\n",
      "            -0.09174554795026779,\n",
      "            0.005070151295512915,\n",
      "            0.14188645780086517,\n",
      "            -0.0353894904255867,\n",
      "            -0.11157050728797913,\n",
      "            -0.15624891221523285,\n",
      "            -0.13059526681900024,\n",
      "            -0.16124364733695984,\n",
      "            0.06925699859857559,\n",
      "            0.15386784076690674,\n",
      "            -0.12296119332313538,\n",
      "            -0.020943038165569305,\n",
      "            0.06197376176714897,\n",
      "            0.0588621161878109,\n",
      "            -0.17456035315990448,\n",
      "            -0.0810512825846672,\n",
      "            -0.005427008494734764,\n",
      "            -0.09379757195711136,\n",
      "            0.11956857144832611,\n",
      "            0.1774960607290268,\n",
      "            -0.022419875487685204,\n",
      "            0.1602703481912613,\n",
      "            -0.01677822694182396,\n",
      "            -0.22603575885295868,\n",
      "            -0.07284586876630783,\n",
      "            -0.10463936626911163,\n",
      "            0.05408765748143196,\n",
      "            -0.06679882109165192,\n",
      "            -0.011096993461251259,\n",
      "            0.15856264531612396,\n",
      "            -0.08032342791557312,\n",
      "            0.10612234473228455,\n",
      "            0.022937897592782974,\n",
      "            -0.06695643067359924,\n",
      "            0.13207818567752838,\n",
      "            0.2175528109073639,\n",
      "            -0.10486835986375809,\n",
      "            -0.16564343869686127,\n",
      "            0.0020609525963664055,\n",
      "            0.11678308248519897,\n",
      "            0.15159863233566284,\n",
      "            0.15166449546813965,\n",
      "            -0.01456020213663578,\n",
      "            -0.10142213106155396,\n",
      "            -0.15913914144039154,\n",
      "            -0.05532693862915039,\n",
      "            0.033339567482471466,\n",
      "            0.14584152400493622,\n",
      "            -0.1184321716427803,\n",
      "            0.052934449166059494,\n",
      "            0.12031292170286179,\n",
      "            -0.17832183837890625,\n",
      "            0.020143557339906693,\n",
      "            -0.10307952761650085,\n",
      "            0.16905945539474487,\n",
      "            0.2562773823738098,\n",
      "            0.13982351124286652,\n",
      "            -0.04154149442911148,\n",
      "            0.18434128165245056,\n",
      "            0.21573993563652039,\n",
      "            0.14340873062610626,\n",
      "            -0.17462487518787384,\n",
      "            0.00683782622218132,\n",
      "            -0.1221209168434143,\n",
      "            0.0048091234639286995,\n",
      "            0.1426651030778885,\n",
      "            0.09125010669231415,\n",
      "            -0.22061380743980408,\n",
      "            0.10114192962646484,\n",
      "            -0.08155674487352371,\n",
      "            0.09106579422950745,\n",
      "            0.13985735177993774,\n",
      "            0.17401191592216492,\n",
      "            -0.11668435484170914,\n",
      "            -0.22317175567150116,\n",
      "            0.008076523430645466,\n",
      "            0.011816314421594143,\n",
      "            0.1728122979402542,\n",
      "            -0.13596707582473755,\n",
      "            0.22130665183067322,\n",
      "            0.11569613963365555,\n",
      "            0.13103921711444855,\n",
      "            -0.07163850218057632,\n",
      "            0.19767960906028748,\n",
      "            -0.007216854952275753,\n",
      "            -0.1533186137676239,\n",
      "            -0.10826792567968369,\n",
      "            0.07101187109947205,\n",
      "            0.01924380473792553\n",
      "          ],\n",
      "          [\n",
      "            0.030411316081881523,\n",
      "            -0.002601106883957982,\n",
      "            -0.014041267335414886,\n",
      "            -0.1705826222896576,\n",
      "            0.033062588423490524,\n",
      "            -0.05871529132127762,\n",
      "            -0.10842414200305939,\n",
      "            0.07165701687335968,\n",
      "            0.03123214654624462,\n",
      "            -0.019175682216882706,\n",
      "            -0.10256998986005783,\n",
      "            -0.14221124351024628,\n",
      "            0.09463615715503693,\n",
      "            0.031158655881881714,\n",
      "            -0.12821830809116364,\n",
      "            0.058125272393226624,\n",
      "            -0.17252343893051147,\n",
      "            -0.14211484789848328,\n",
      "            -0.13809023797512054,\n",
      "            0.15021593868732452,\n",
      "            0.013676547445356846,\n",
      "            0.11203012615442276,\n",
      "            -0.03549864515662193,\n",
      "            -0.16785837709903717,\n",
      "            -0.05724899470806122,\n",
      "            -0.12230667471885681,\n",
      "            -0.06422226130962372,\n",
      "            -0.06553076952695847,\n",
      "            0.10935290157794952,\n",
      "            0.04593878611922264,\n",
      "            0.10060659795999527,\n",
      "            0.0042998758144676685,\n",
      "            0.015414593741297722,\n",
      "            -0.02159748785197735,\n",
      "            0.020435988903045654,\n",
      "            -0.033089879900217056,\n",
      "            -0.1543315351009369,\n",
      "            -0.03370554372668266,\n",
      "            -0.09011286497116089,\n",
      "            0.02920296974480152,\n",
      "            0.1663164496421814,\n",
      "            0.0034884249325841665,\n",
      "            0.11656747758388519,\n",
      "            0.030933907255530357,\n",
      "            0.08809159696102142,\n",
      "            0.029923871159553528,\n",
      "            -0.13700033724308014,\n",
      "            -0.055583421140909195,\n",
      "            0.002397442003712058,\n",
      "            0.022617509588599205,\n",
      "            0.10149098932743073,\n",
      "            0.05323825404047966,\n",
      "            -0.17228788137435913,\n",
      "            0.16213922202587128,\n",
      "            0.11602127552032471,\n",
      "            -0.029768865555524826,\n",
      "            -0.03602171316742897,\n",
      "            0.09018082171678543,\n",
      "            0.07456599175930023,\n",
      "            0.17247039079666138,\n",
      "            0.041184619069099426,\n",
      "            -0.0393369123339653,\n",
      "            0.1649601310491562,\n",
      "            0.1731170117855072,\n",
      "            0.11940551549196243,\n",
      "            0.014557473361492157,\n",
      "            -0.1614149808883667,\n",
      "            0.1341569721698761,\n",
      "            -0.10740302503108978,\n",
      "            0.09724358469247818,\n",
      "            0.10528453439474106,\n",
      "            -0.07242552936077118,\n",
      "            -0.0009694249019958079,\n",
      "            -0.021621309220790863,\n",
      "            -0.15864580869674683,\n",
      "            -0.004008717834949493,\n",
      "            -0.05418197810649872,\n",
      "            0.061213571578264236,\n",
      "            0.008404219523072243,\n",
      "            -0.1159074604511261,\n",
      "            -0.05832092836499214,\n",
      "            0.0045235962606966496,\n",
      "            0.1671888679265976,\n",
      "            -0.10380581766366959,\n",
      "            -0.1175931990146637,\n",
      "            0.0011704465141519904,\n",
      "            0.09043446183204651,\n",
      "            -0.15247593820095062,\n",
      "            -0.11271990090608597,\n",
      "            0.010165124200284481,\n",
      "            0.06894217431545258,\n",
      "            0.15506164729595184,\n",
      "            0.03625103458762169,\n",
      "            0.1406443566083908,\n",
      "            0.061980921775102615,\n",
      "            -0.007554240059107542,\n",
      "            -0.09352990239858627,\n",
      "            0.1589093804359436,\n",
      "            -0.07099847495555878,\n",
      "            -0.0635869950056076,\n",
      "            -0.12162638455629349,\n",
      "            -0.007885878905653954,\n",
      "            -0.12355168163776398,\n",
      "            0.05407562851905823,\n",
      "            -0.14426198601722717,\n",
      "            0.059421077370643616,\n",
      "            0.12186302989721298,\n",
      "            -0.1726929247379303,\n",
      "            0.16595688462257385,\n",
      "            0.1660298854112625,\n",
      "            -0.014629105105996132,\n",
      "            -0.16322574019432068,\n",
      "            -0.023173613473773003,\n",
      "            -0.062045201659202576,\n",
      "            0.10535375028848648,\n",
      "            -0.0924636498093605,\n",
      "            0.009014698676764965,\n",
      "            0.07552887499332428,\n",
      "            0.13352441787719727,\n",
      "            0.01664223149418831,\n",
      "            0.003470534225925803,\n",
      "            -0.04759058728814125,\n",
      "            -0.09149296581745148,\n",
      "            0.11965020000934601,\n",
      "            -0.020836148411035538,\n",
      "            0.15993383526802063,\n",
      "            -0.13657355308532715,\n",
      "            0.1176510900259018\n",
      "          ],\n",
      "          [\n",
      "            -0.13662685453891754,\n",
      "            0.16557036340236664,\n",
      "            -0.01586986519396305,\n",
      "            -0.0596436932682991,\n",
      "            0.026809239760041237,\n",
      "            -0.10238897055387497,\n",
      "            0.09057539701461792,\n",
      "            -0.05026216059923172,\n",
      "            -0.15512144565582275,\n",
      "            -0.0039017307572066784,\n",
      "            0.11050370335578918,\n",
      "            0.013371826149523258,\n",
      "            -0.02558363974094391,\n",
      "            0.1412765383720398,\n",
      "            0.04305630922317505,\n",
      "            -0.013677790760993958,\n",
      "            -0.05446256324648857,\n",
      "            0.04203173518180847,\n",
      "            0.004274371545761824,\n",
      "            0.08990626037120819,\n",
      "            0.14104951918125153,\n",
      "            0.06683483719825745,\n",
      "            0.027385396882891655,\n",
      "            0.1293679177761078,\n",
      "            0.043116386979818344,\n",
      "            0.12795457243919373,\n",
      "            -0.022486038506031036,\n",
      "            0.005426452029496431,\n",
      "            0.14432263374328613,\n",
      "            -0.0014471777249127626,\n",
      "            0.05385519191622734,\n",
      "            -0.11234349012374878,\n",
      "            -0.07542282342910767,\n",
      "            0.15036064386367798,\n",
      "            -0.11335828900337219,\n",
      "            -0.035314228385686874,\n",
      "            -0.0909278392791748,\n",
      "            -0.01604950614273548,\n",
      "            0.16010627150535583,\n",
      "            0.026430875062942505,\n",
      "            -0.1059798151254654,\n",
      "            0.021769924089312553,\n",
      "            0.012118741869926453,\n",
      "            0.025868242606520653,\n",
      "            -0.17158275842666626,\n",
      "            -0.04087451472878456,\n",
      "            0.08830779045820236,\n",
      "            0.13187971711158752,\n",
      "            -0.09758574515581131,\n",
      "            0.07317107170820236,\n",
      "            0.0327429361641407,\n",
      "            0.13559910655021667,\n",
      "            -0.04094400256872177,\n",
      "            -0.06906911730766296,\n",
      "            -0.14422117173671722,\n",
      "            0.04399782419204712,\n",
      "            0.08651122450828552,\n",
      "            0.10420824587345123,\n",
      "            0.016001198440790176,\n",
      "            -0.11706764250993729,\n",
      "            0.04466044157743454,\n",
      "            -0.05248959735035896,\n",
      "            -0.06059786677360535,\n",
      "            -0.15020494163036346,\n",
      "            0.034879688173532486,\n",
      "            -0.033532578498125076,\n",
      "            -0.08799609541893005,\n",
      "            -0.0561307929456234,\n",
      "            -0.02927739918231964,\n",
      "            0.11711025983095169,\n",
      "            -0.12836341559886932,\n",
      "            -0.13285617530345917,\n",
      "            0.014293086715042591,\n",
      "            -0.1239614188671112,\n",
      "            -0.03209696337580681,\n",
      "            0.038171835243701935,\n",
      "            -0.13366319239139557,\n",
      "            -0.11598124355077744,\n",
      "            -0.06807819753885269,\n",
      "            0.07891035825014114,\n",
      "            0.10294587165117264,\n",
      "            -0.16764436662197113,\n",
      "            -0.091517373919487,\n",
      "            0.1081216111779213,\n",
      "            0.09916127473115921,\n",
      "            0.009902950376272202,\n",
      "            0.013306399807333946,\n",
      "            -0.15165498852729797,\n",
      "            -0.11309389770030975,\n",
      "            0.0056980992667376995,\n",
      "            0.008078489452600479,\n",
      "            0.04227118566632271,\n",
      "            0.16335424780845642,\n",
      "            0.12741705775260925,\n",
      "            -0.15734288096427917,\n",
      "            -0.12172821909189224,\n",
      "            -0.004254193510860205,\n",
      "            0.0414714589715004,\n",
      "            0.07133157551288605,\n",
      "            -0.048246119171381,\n",
      "            -0.10460007935762405,\n",
      "            0.10668058693408966,\n",
      "            0.02537769079208374,\n",
      "            0.1349790245294571,\n",
      "            -0.045279666781425476,\n",
      "            0.0031233280897140503,\n",
      "            0.15260273218154907,\n",
      "            -0.05005478113889694,\n",
      "            -0.1369556188583374,\n",
      "            0.003477071411907673,\n",
      "            -0.026036830618977547,\n",
      "            -0.08425787836313248,\n",
      "            -0.03826906532049179,\n",
      "            -0.09302164614200592,\n",
      "            0.14811941981315613,\n",
      "            -0.14066721498966217,\n",
      "            -0.13033126294612885,\n",
      "            0.1266295164823532,\n",
      "            0.1328391134738922,\n",
      "            -0.05657416954636574,\n",
      "            -0.12042675167322159,\n",
      "            -0.12906277179718018,\n",
      "            -0.009535874240100384,\n",
      "            0.02118944562971592,\n",
      "            -0.11818405240774155,\n",
      "            0.135095477104187,\n",
      "            0.011790917254984379,\n",
      "            -0.013315603137016296\n",
      "          ],\n",
      "          [\n",
      "            0.06782595813274384,\n",
      "            -0.07661034911870956,\n",
      "            -0.0074799275025725365,\n",
      "            0.03431589901447296,\n",
      "            -0.08253660053014755,\n",
      "            0.05711396038532257,\n",
      "            0.08911485970020294,\n",
      "            0.05546541139483452,\n",
      "            0.10031576454639435,\n",
      "            0.031086590141057968,\n",
      "            -0.026130666956305504,\n",
      "            -0.09650824218988419,\n",
      "            0.14997322857379913,\n",
      "            -0.015695542097091675,\n",
      "            0.11364977061748505,\n",
      "            0.030700862407684326,\n",
      "            0.09478367865085602,\n",
      "            -0.08301656693220139,\n",
      "            -0.10017555952072144,\n",
      "            -0.13456878066062927,\n",
      "            -0.028336096554994583,\n",
      "            -0.08407747000455856,\n",
      "            0.18814215064048767,\n",
      "            0.10709960013628006,\n",
      "            0.1712057739496231,\n",
      "            0.1545172929763794,\n",
      "            0.010319174267351627,\n",
      "            -0.0018806045409291983,\n",
      "            -0.04649368301033974,\n",
      "            0.034764960408210754,\n",
      "            -0.0737360417842865,\n",
      "            0.06376439332962036,\n",
      "            0.1235998123884201,\n",
      "            -0.14870458841323853,\n",
      "            0.0592341423034668,\n",
      "            0.04689571261405945,\n",
      "            0.14662395417690277,\n",
      "            -0.1502602994441986,\n",
      "            0.028905341401696205,\n",
      "            -0.0238969586789608,\n",
      "            0.20714029669761658,\n",
      "            -0.06577731668949127,\n",
      "            -0.06840561330318451,\n",
      "            0.08061888068914413,\n",
      "            0.13646358251571655,\n",
      "            -0.11094506084918976,\n",
      "            0.07999636232852936,\n",
      "            0.028085382655262947,\n",
      "            -0.034217312932014465,\n",
      "            -0.10657589882612228,\n",
      "            0.052988361567258835,\n",
      "            0.17068615555763245,\n",
      "            0.03333215415477753,\n",
      "            -0.16840989887714386,\n",
      "            0.014344993978738785,\n",
      "            0.1618320792913437,\n",
      "            -0.1612730175256729,\n",
      "            0.1717189997434616,\n",
      "            0.014733724296092987,\n",
      "            0.04550566151738167,\n",
      "            0.10874244570732117,\n",
      "            0.006276758853346109,\n",
      "            -0.011990560218691826,\n",
      "            0.009635568596422672,\n",
      "            0.09933307766914368,\n",
      "            0.0790708065032959,\n",
      "            0.03871478512883186,\n",
      "            0.0036715310998260975,\n",
      "            -0.08489041030406952,\n",
      "            -0.021308861672878265,\n",
      "            0.024720018729567528,\n",
      "            0.012924615293741226,\n",
      "            0.11135417222976685,\n",
      "            -0.028870632871985435,\n",
      "            0.0173869077116251,\n",
      "            0.026625562459230423,\n",
      "            0.05608300119638443,\n",
      "            0.11767179518938065,\n",
      "            -0.12276863306760788,\n",
      "            -0.06439932435750961,\n",
      "            0.05532503500580788,\n",
      "            -0.17526894807815552,\n",
      "            0.15206369757652283,\n",
      "            -0.05012013763189316,\n",
      "            -0.11261895298957825,\n",
      "            0.18375329673290253,\n",
      "            -0.07841433584690094,\n",
      "            0.09918001294136047,\n",
      "            -0.009540244936943054,\n",
      "            0.10882007330656052,\n",
      "            0.1485157459974289,\n",
      "            0.17507781088352203,\n",
      "            -0.013296888209879398,\n",
      "            0.18412160873413086,\n",
      "            0.04521004483103752,\n",
      "            0.21183596551418304,\n",
      "            0.08382801711559296,\n",
      "            0.1426842361688614,\n",
      "            -0.014148766174912453,\n",
      "            0.06533397734165192,\n",
      "            0.09859520196914673,\n",
      "            0.02226325124502182,\n",
      "            0.04308011382818222,\n",
      "            -0.1474940925836563,\n",
      "            0.018443232402205467,\n",
      "            0.16138394176959991,\n",
      "            -0.13992951810359955,\n",
      "            0.12160617858171463,\n",
      "            0.06383161246776581,\n",
      "            0.04030344635248184,\n",
      "            -0.035090990364551544,\n",
      "            -0.010894348844885826,\n",
      "            -0.1818072646856308,\n",
      "            0.12825775146484375,\n",
      "            0.12097882479429245,\n",
      "            0.06917701661586761,\n",
      "            0.08145315200090408,\n",
      "            -0.1544937789440155,\n",
      "            -0.12113898992538452,\n",
      "            -0.17890818417072296,\n",
      "            -0.10037621110677719,\n",
      "            -0.005783984437584877,\n",
      "            0.06799137592315674,\n",
      "            0.06641168147325516,\n",
      "            -0.11113213002681732,\n",
      "            -0.1590615063905716,\n",
      "            0.11367050558328629,\n",
      "            -0.05586325377225876\n",
      "          ],\n",
      "          [\n",
      "            0.028263119980692863,\n",
      "            -0.16620613634586334,\n",
      "            0.06913375854492188,\n",
      "            0.14570589363574982,\n",
      "            0.02842128649353981,\n",
      "            -0.1488730013370514,\n",
      "            0.12626802921295166,\n",
      "            -0.07484159618616104,\n",
      "            0.030843904241919518,\n",
      "            0.08828318864107132,\n",
      "            -0.17620734870433807,\n",
      "            0.06576398760080338,\n",
      "            -0.06457390636205673,\n",
      "            -0.1344592422246933,\n",
      "            0.16709837317466736,\n",
      "            -0.13682687282562256,\n",
      "            0.11625178158283234,\n",
      "            0.09582892805337906,\n",
      "            0.10618085414171219,\n",
      "            -0.1532062143087387,\n",
      "            -0.010933905839920044,\n",
      "            0.15880760550498962,\n",
      "            0.16949917376041412,\n",
      "            0.01452345959842205,\n",
      "            -0.08021374046802521,\n",
      "            -0.17341957986354828,\n",
      "            0.16862638294696808,\n",
      "            -0.037056248635053635,\n",
      "            0.09148366749286652,\n",
      "            0.11987761408090591,\n",
      "            -0.047378089278936386,\n",
      "            0.14613357186317444,\n",
      "            0.09904344379901886,\n",
      "            0.09955142438411713,\n",
      "            0.020666494965553284,\n",
      "            0.022229300811886787,\n",
      "            -0.1047150269150734,\n",
      "            -0.0254305899143219,\n",
      "            -0.006870690267533064,\n",
      "            -0.06274686753749847,\n",
      "            -0.01181111577898264,\n",
      "            0.0653044730424881,\n",
      "            0.1052073985338211,\n",
      "            0.0631122812628746,\n",
      "            -0.1564946174621582,\n",
      "            -0.038169294595718384,\n",
      "            -0.07865376770496368,\n",
      "            0.12339135259389877,\n",
      "            0.03345077857375145,\n",
      "            0.09317902475595474,\n",
      "            -0.052358921617269516,\n",
      "            -0.05902748554944992,\n",
      "            -0.11778650432825089,\n",
      "            0.02103472501039505,\n",
      "            -0.16487455368041992,\n",
      "            -0.006921935360878706,\n",
      "            -0.04401872679591179,\n",
      "            -0.04923134669661522,\n",
      "            0.09773415327072144,\n",
      "            -0.1223883330821991,\n",
      "            -0.03904050961136818,\n",
      "            0.0677090510725975,\n",
      "            0.053682100027799606,\n",
      "            -0.03416840732097626,\n",
      "            -0.10590136796236038,\n",
      "            -0.06539908796548843,\n",
      "            -0.10282644629478455,\n",
      "            -0.16863447427749634,\n",
      "            -0.06217595934867859,\n",
      "            -0.1568973809480667,\n",
      "            0.09355337917804718,\n",
      "            -0.16774053871631622,\n",
      "            -0.09308536350727081,\n",
      "            -0.09838178008794785,\n",
      "            0.07846897840499878,\n",
      "            -0.09912054985761642,\n",
      "            -0.01456876378506422,\n",
      "            -0.010478546842932701,\n",
      "            -0.12663958966732025,\n",
      "            -0.16534200310707092,\n",
      "            0.16977861523628235,\n",
      "            0.05423295870423317,\n",
      "            0.04845278337597847,\n",
      "            -0.12043527513742447,\n",
      "            0.17351853847503662,\n",
      "            0.12099061906337738,\n",
      "            0.03498665243387222,\n",
      "            -0.04298749938607216,\n",
      "            0.0403762049973011,\n",
      "            -0.05903226509690285,\n",
      "            0.04899367317557335,\n",
      "            0.08271648734807968,\n",
      "            -0.09863787889480591,\n",
      "            0.05232936143875122,\n",
      "            0.024015123024582863,\n",
      "            -0.05495072156190872,\n",
      "            -0.07583560794591904,\n",
      "            -0.06916265189647675,\n",
      "            -0.11306405812501907,\n",
      "            0.06049371138215065,\n",
      "            0.09057928621768951,\n",
      "            -0.15719465911388397,\n",
      "            0.05509050190448761,\n",
      "            -0.07483387738466263,\n",
      "            -0.08967290073633194,\n",
      "            0.04398798942565918,\n",
      "            -0.10131200402975082,\n",
      "            -0.031034253537654877,\n",
      "            -0.1739548146724701,\n",
      "            0.056732404977083206,\n",
      "            0.0644007995724678,\n",
      "            -0.08590973168611526,\n",
      "            0.06069323047995567,\n",
      "            0.02531396970152855,\n",
      "            -0.004045675043016672,\n",
      "            -0.10030151158571243,\n",
      "            0.05735742673277855,\n",
      "            -0.03491145372390747,\n",
      "            -0.008690978400409222,\n",
      "            0.016658108681440353,\n",
      "            -0.13492868840694427,\n",
      "            0.08616241812705994,\n",
      "            0.15775641798973083,\n",
      "            0.04172680899500847,\n",
      "            -0.14764803647994995,\n",
      "            0.12095875293016434,\n",
      "            0.1327255815267563,\n",
      "            0.009758488275110722\n",
      "          ],\n",
      "          [\n",
      "            -0.02127808891236782,\n",
      "            -0.1477338820695877,\n",
      "            0.004888517316430807,\n",
      "            0.09910555183887482,\n",
      "            0.12020036578178406,\n",
      "            -0.0918392688035965,\n",
      "            0.02835722640156746,\n",
      "            0.05054762586951256,\n",
      "            -0.007569311652332544,\n",
      "            0.12872393429279327,\n",
      "            -0.06259460002183914,\n",
      "            0.0818377286195755,\n",
      "            -0.027265941724181175,\n",
      "            -0.17194095253944397,\n",
      "            -0.037376586347818375,\n",
      "            -0.007325038313865662,\n",
      "            0.18575212359428406,\n",
      "            0.1618185192346573,\n",
      "            0.013219733722507954,\n",
      "            0.11847690492868423,\n",
      "            -0.0646393820643425,\n",
      "            0.11629064381122589,\n",
      "            -0.18656308948993683,\n",
      "            -0.058410853147506714,\n",
      "            -0.21125929057598114,\n",
      "            0.10610733181238174,\n",
      "            0.02169683203101158,\n",
      "            0.001881329109892249,\n",
      "            0.15852028131484985,\n",
      "            -0.09620926529169083,\n",
      "            0.057067010551691055,\n",
      "            -0.10653942823410034,\n",
      "            -0.1325497180223465,\n",
      "            0.19123755395412445,\n",
      "            0.02017560601234436,\n",
      "            -0.15401612222194672,\n",
      "            -0.05189940705895424,\n",
      "            -0.03173109516501427,\n",
      "            -0.14740243554115295,\n",
      "            -0.082547627389431,\n",
      "            -0.09206100553274155,\n",
      "            0.05082617700099945,\n",
      "            0.06865299493074417,\n",
      "            -0.0018648429540917277,\n",
      "            0.07304462790489197,\n",
      "            0.14212480187416077,\n",
      "            -0.026901669800281525,\n",
      "            0.08820193260908127,\n",
      "            -0.04772203043103218,\n",
      "            -0.18839959800243378,\n",
      "            0.012890908867120743,\n",
      "            -0.10563069581985474,\n",
      "            0.08020640164613724,\n",
      "            -0.15955594182014465,\n",
      "            -0.02368004247546196,\n",
      "            0.0905797928571701,\n",
      "            0.1548045128583908,\n",
      "            -0.10703647881746292,\n",
      "            0.1544250249862671,\n",
      "            -0.1068912148475647,\n",
      "            -0.12258829176425934,\n",
      "            0.1405259668827057,\n",
      "            0.1688876748085022,\n",
      "            -0.05583098903298378,\n",
      "            0.05301472172141075,\n",
      "            0.0834154561161995,\n",
      "            -0.17296156287193298,\n",
      "            -0.12780660390853882,\n",
      "            -0.13646605610847473,\n",
      "            -0.14945925772190094,\n",
      "            -0.053438909351825714,\n",
      "            -0.035989731550216675,\n",
      "            0.08551101386547089,\n",
      "            0.03574351593852043,\n",
      "            0.19256103038787842,\n",
      "            -0.1457032412290573,\n",
      "            0.028824634850025177,\n",
      "            0.0889347642660141,\n",
      "            0.09479674696922302,\n",
      "            -0.11052269488573074,\n",
      "            -0.12732261419296265,\n",
      "            0.1558658629655838,\n",
      "            -0.04111121594905853,\n",
      "            0.12581081688404083,\n",
      "            0.09054065495729446,\n",
      "            -0.1708754301071167,\n",
      "            0.038712676614522934,\n",
      "            -0.08600263297557831,\n",
      "            -0.018321670591831207,\n",
      "            -0.13203972578048706,\n",
      "            -0.16386817395687103,\n",
      "            -0.17342202365398407,\n",
      "            0.16405199468135834,\n",
      "            -0.14685630798339844,\n",
      "            -0.03461115062236786,\n",
      "            0.10388638079166412,\n",
      "            0.03867505118250847,\n",
      "            0.000691742985509336,\n",
      "            -0.11376723647117615,\n",
      "            -0.14010180532932281,\n",
      "            0.20079703629016876,\n",
      "            0.04959935322403908,\n",
      "            -0.06005918234586716,\n",
      "            0.028321390971541405,\n",
      "            0.14085741341114044,\n",
      "            -0.05783196538686752,\n",
      "            -0.054339319467544556,\n",
      "            -0.018164733424782753,\n",
      "            0.0764961913228035,\n",
      "            -0.037140436470508575,\n",
      "            -0.051584646105766296,\n",
      "            -0.0598708800971508,\n",
      "            0.18228977918624878,\n",
      "            -0.08227530121803284,\n",
      "            -0.1057228147983551,\n",
      "            -0.08261289447546005,\n",
      "            -0.15924836695194244,\n",
      "            0.052568305283784866,\n",
      "            -0.10908494889736176,\n",
      "            -0.005763594526797533,\n",
      "            0.05209864675998688,\n",
      "            0.057409193366765976,\n",
      "            -0.1562163233757019,\n",
      "            0.07097547501325607,\n",
      "            0.06819804757833481,\n",
      "            0.08261685073375702,\n",
      "            0.1863630712032318,\n",
      "            -0.05035598576068878\n",
      "          ],\n",
      "          [\n",
      "            0.06732217967510223,\n",
      "            0.1407308578491211,\n",
      "            0.1713007390499115,\n",
      "            0.1373935341835022,\n",
      "            0.1361515074968338,\n",
      "            -0.039680831134319305,\n",
      "            0.17477938532829285,\n",
      "            -0.03621983155608177,\n",
      "            -0.06953117251396179,\n",
      "            0.15801408886909485,\n",
      "            -0.11626367270946503,\n",
      "            0.12114518135786057,\n",
      "            -0.06967296451330185,\n",
      "            0.013004439882934093,\n",
      "            0.07341275364160538,\n",
      "            -0.17334872484207153,\n",
      "            -0.11000126600265503,\n",
      "            0.007129970472306013,\n",
      "            -0.05618002265691757,\n",
      "            -0.05846858024597168,\n",
      "            0.06753652542829514,\n",
      "            0.067826047539711,\n",
      "            -0.1501828283071518,\n",
      "            0.09175212681293488,\n",
      "            0.18746019899845123,\n",
      "            0.1317378729581833,\n",
      "            0.057474877685308456,\n",
      "            0.17096728086471558,\n",
      "            0.05922502651810646,\n",
      "            -0.015210497193038464,\n",
      "            0.16341957449913025,\n",
      "            -0.0035467364359647036,\n",
      "            0.047371938824653625,\n",
      "            -0.14351965487003326,\n",
      "            -0.06100364029407501,\n",
      "            0.11083115637302399,\n",
      "            0.09752857685089111,\n",
      "            -0.0765613541007042,\n",
      "            0.017839837819337845,\n",
      "            0.057568248361349106,\n",
      "            0.022150851786136627,\n",
      "            0.16386239230632782,\n",
      "            -0.009143947623670101,\n",
      "            -0.09037943184375763,\n",
      "            0.16921229660511017,\n",
      "            -0.017862925305962563,\n",
      "            0.04203515499830246,\n",
      "            -0.07735475152730942,\n",
      "            -0.019948702305555344,\n",
      "            0.02300390973687172,\n",
      "            -0.0834643766283989,\n",
      "            -0.10571103543043137,\n",
      "            0.025004129856824875,\n",
      "            0.10339365899562836,\n",
      "            0.10096748173236847,\n",
      "            0.07860884815454483,\n",
      "            0.041732098907232285,\n",
      "            -0.06383208185434341,\n",
      "            -0.04990410804748535,\n",
      "            0.1178605705499649,\n",
      "            -0.07601392269134521,\n",
      "            0.007092614658176899,\n",
      "            0.04543016478419304,\n",
      "            -0.10308842360973358,\n",
      "            -0.11710216104984283,\n",
      "            -0.1589438021183014,\n",
      "            -0.06501436233520508,\n",
      "            0.2126302570104599,\n",
      "            -0.08304425328969955,\n",
      "            -0.12423615902662277,\n",
      "            0.05199238285422325,\n",
      "            0.14820870757102966,\n",
      "            -0.06598585098981857,\n",
      "            0.2178516983985901,\n",
      "            0.19121751189231873,\n",
      "            0.08548694849014282,\n",
      "            -0.1409541219472885,\n",
      "            0.0006058898288756609,\n",
      "            -0.04609004035592079,\n",
      "            0.03590657562017441,\n",
      "            -0.04834933578968048,\n",
      "            0.15102168917655945,\n",
      "            0.07534382492303848,\n",
      "            0.08555539697408676,\n",
      "            -0.06823458522558212,\n",
      "            -0.14526192843914032,\n",
      "            0.08713563531637192,\n",
      "            -0.040774233639240265,\n",
      "            -0.0676412582397461,\n",
      "            -0.0969221293926239,\n",
      "            -0.10152533650398254,\n",
      "            -0.1394539475440979,\n",
      "            -0.05878107622265816,\n",
      "            0.11120143532752991,\n",
      "            0.06176874414086342,\n",
      "            0.1531701683998108,\n",
      "            0.031100021675229073,\n",
      "            -0.10591228306293488,\n",
      "            0.1760641187429428,\n",
      "            0.15633632242679596,\n",
      "            -0.1307697892189026,\n",
      "            -0.0941542312502861,\n",
      "            0.11043253540992737,\n",
      "            0.054624974727630615,\n",
      "            -0.05838252604007721,\n",
      "            -0.024838820099830627,\n",
      "            0.12424284219741821,\n",
      "            0.06625132262706757,\n",
      "            0.09536343067884445,\n",
      "            -0.029146773740649223,\n",
      "            0.09830588102340698,\n",
      "            -0.08987373113632202,\n",
      "            -0.016292382031679153,\n",
      "            -0.1553909033536911,\n",
      "            -0.07755158096551895,\n",
      "            -0.08674801141023636,\n",
      "            0.046285465359687805,\n",
      "            0.05447186529636383,\n",
      "            -0.07196922600269318,\n",
      "            0.10605445504188538,\n",
      "            0.09644526988267899,\n",
      "            -0.10142865031957626,\n",
      "            0.17680767178535461,\n",
      "            0.12270566821098328,\n",
      "            0.15335679054260254,\n",
      "            0.07954934239387512,\n",
      "            0.12742553651332855,\n",
      "            -0.10300929844379425\n",
      "          ],\n",
      "          [\n",
      "            0.03987165167927742,\n",
      "            -0.0357172004878521,\n",
      "            -0.06130944937467575,\n",
      "            -0.14536996185779572,\n",
      "            0.04279576987028122,\n",
      "            0.05455607548356056,\n",
      "            0.06539221107959747,\n",
      "            0.16976824402809143,\n",
      "            0.06997954845428467,\n",
      "            0.1778927743434906,\n",
      "            -0.17307467758655548,\n",
      "            -0.07807810604572296,\n",
      "            -0.043274447321891785,\n",
      "            0.04244619235396385,\n",
      "            -0.15722990036010742,\n",
      "            -0.050208985805511475,\n",
      "            -0.015518052503466606,\n",
      "            0.17718608677387238,\n",
      "            -0.1600513458251953,\n",
      "            -0.10960525274276733,\n",
      "            -0.03608863428235054,\n",
      "            0.01915857568383217,\n",
      "            0.134935200214386,\n",
      "            -0.05618099495768547,\n",
      "            -0.11287771165370941,\n",
      "            -0.10200249403715134,\n",
      "            0.1462346315383911,\n",
      "            -0.06521055102348328,\n",
      "            0.0790303498506546,\n",
      "            -0.14230382442474365,\n",
      "            0.16641496121883392,\n",
      "            -0.17978207767009735,\n",
      "            -0.2064102590084076,\n",
      "            0.14951364696025848,\n",
      "            0.09852348268032074,\n",
      "            0.08868011832237244,\n",
      "            0.04870007187128067,\n",
      "            0.016662118956446648,\n",
      "            -0.15134723484516144,\n",
      "            -0.0038665609899908304,\n",
      "            0.04769112914800644,\n",
      "            0.07930580526590347,\n",
      "            0.16936524212360382,\n",
      "            0.05354171618819237,\n",
      "            0.1614549160003662,\n",
      "            -0.15150296688079834,\n",
      "            0.241781547665596,\n",
      "            -0.04129725322127342,\n",
      "            -0.17548319697380066,\n",
      "            0.10451667010784149,\n",
      "            0.06379960477352142,\n",
      "            0.12127983570098877,\n",
      "            -0.18210750818252563,\n",
      "            -0.07347184419631958,\n",
      "            0.013694384135305882,\n",
      "            0.02604769356548786,\n",
      "            0.0061953249387443066,\n",
      "            -0.0592677965760231,\n",
      "            0.12828905880451202,\n",
      "            -0.16792313754558563,\n",
      "            0.07093188166618347,\n",
      "            -0.07718959450721741,\n",
      "            0.0015007199253886938,\n",
      "            -0.1536954641342163,\n",
      "            0.14482855796813965,\n",
      "            -0.10557907074689865,\n",
      "            -0.10160420089960098,\n",
      "            0.13237766921520233,\n",
      "            0.17480941116809845,\n",
      "            0.17123477160930634,\n",
      "            0.061911605298519135,\n",
      "            0.11831975728273392,\n",
      "            -0.15424416959285736,\n",
      "            -0.13241733610630035,\n",
      "            0.14369913935661316,\n",
      "            -0.054179441183805466,\n",
      "            -0.06168193370103836,\n",
      "            0.13240790367126465,\n",
      "            -0.04602520912885666,\n",
      "            0.07518307119607925,\n",
      "            -0.14309538900852203,\n",
      "            -0.13823729753494263,\n",
      "            -0.0871569961309433,\n",
      "            0.15685033798217773,\n",
      "            -0.04300566017627716,\n",
      "            -0.047868937253952026,\n",
      "            -0.1390303075313568,\n",
      "            -0.17201334238052368,\n",
      "            0.11031879484653473,\n",
      "            -0.13229751586914062,\n",
      "            0.08617174625396729,\n",
      "            -0.17288221418857574,\n",
      "            0.08646440505981445,\n",
      "            -0.08558405190706253,\n",
      "            -0.016679920256137848,\n",
      "            -0.17845280468463898,\n",
      "            -0.06852448731660843,\n",
      "            -0.11310049891471863,\n",
      "            0.12941931188106537,\n",
      "            0.0288521870970726,\n",
      "            0.20412150025367737,\n",
      "            0.12538036704063416,\n",
      "            -0.17610110342502594,\n",
      "            -0.17212225496768951,\n",
      "            0.11296255886554718,\n",
      "            -0.10761477053165436,\n",
      "            0.03319675475358963,\n",
      "            -0.04112020507454872,\n",
      "            -0.13779254257678986,\n",
      "            -0.14911577105522156,\n",
      "            -0.11152489483356476,\n",
      "            -0.13552023470401764,\n",
      "            -0.13666819036006927,\n",
      "            -0.11539343744516373,\n",
      "            -0.07720216363668442,\n",
      "            -0.0726800337433815,\n",
      "            -0.1144118681550026,\n",
      "            0.18711450695991516,\n",
      "            -0.006097137928009033,\n",
      "            -0.12055414170026779,\n",
      "            -0.16615824401378632,\n",
      "            -0.043533530086278915,\n",
      "            0.05203868821263313,\n",
      "            0.024387232959270477,\n",
      "            0.21529459953308105,\n",
      "            0.049540504813194275,\n",
      "            -0.16395585238933563,\n",
      "            -0.05205800011754036\n",
      "          ],\n",
      "          [\n",
      "            0.1680651754140854,\n",
      "            0.0016659643733873963,\n",
      "            -0.17499133944511414,\n",
      "            0.0033748538699001074,\n",
      "            -0.1042807549238205,\n",
      "            -0.10587626695632935,\n",
      "            0.08027046918869019,\n",
      "            -0.08971186727285385,\n",
      "            0.0759175643324852,\n",
      "            -0.1270347684621811,\n",
      "            0.14555130898952484,\n",
      "            -0.08602967113256454,\n",
      "            -0.15064598619937897,\n",
      "            -0.015720592811703682,\n",
      "            0.027020473033189774,\n",
      "            -0.06018047034740448,\n",
      "            -0.07749012857675552,\n",
      "            -0.024100743234157562,\n",
      "            0.07153089344501495,\n",
      "            0.11820552498102188,\n",
      "            0.13002334535121918,\n",
      "            0.03716141730546951,\n",
      "            -0.133860781788826,\n",
      "            0.0009851903887465596,\n",
      "            -0.037955738604068756,\n",
      "            -0.11485808342695236,\n",
      "            0.012340187095105648,\n",
      "            0.12895146012306213,\n",
      "            -0.07911143451929092,\n",
      "            0.0019548546988517046,\n",
      "            -0.1097181886434555,\n",
      "            -0.0667986199259758,\n",
      "            0.05805997923016548,\n",
      "            -0.1345330774784088,\n",
      "            -0.07903402298688889,\n",
      "            0.029473595321178436,\n",
      "            0.0975038930773735,\n",
      "            -0.060013074427843094,\n",
      "            -0.09830750524997711,\n",
      "            0.023144105449318886,\n",
      "            -0.12495648860931396,\n",
      "            0.15238822996616364,\n",
      "            0.08359233289957047,\n",
      "            0.08979915827512741,\n",
      "            0.10415561497211456,\n",
      "            0.058821000158786774,\n",
      "            -0.08269255608320236,\n",
      "            0.10642704367637634,\n",
      "            -0.050497233867645264,\n",
      "            -0.13488878309726715,\n",
      "            -0.08060050755739212,\n",
      "            -0.16126352548599243,\n",
      "            0.04656025022268295,\n",
      "            -0.16904763877391815,\n",
      "            -0.030592355877161026,\n",
      "            -0.009630037471652031,\n",
      "            -0.04506717994809151,\n",
      "            -0.055219851434230804,\n",
      "            -0.16679950058460236,\n",
      "            -0.09598282724618912,\n",
      "            0.12967325747013092,\n",
      "            0.02746030129492283,\n",
      "            0.08150745183229446,\n",
      "            -0.006420554593205452,\n",
      "            0.046942759305238724,\n",
      "            0.13704602420330048,\n",
      "            0.07214716076850891,\n",
      "            -0.1640913188457489,\n",
      "            0.13618142902851105,\n",
      "            -0.05489366501569748,\n",
      "            0.032603681087493896,\n",
      "            0.17703071236610413,\n",
      "            0.13912348449230194,\n",
      "            -0.11616934090852737,\n",
      "            0.0225641131401062,\n",
      "            0.10430985689163208,\n",
      "            -0.17080779373645782,\n",
      "            -0.012201590463519096,\n",
      "            0.130407452583313,\n",
      "            0.09416211396455765,\n",
      "            -0.11760781705379486,\n",
      "            0.02431303635239601,\n",
      "            0.010476604104042053,\n",
      "            -0.1162797138094902,\n",
      "            0.06910191476345062,\n",
      "            0.03527002036571503,\n",
      "            0.14743898808956146,\n",
      "            -0.11959760636091232,\n",
      "            0.0908370167016983,\n",
      "            0.12413018941879272,\n",
      "            -0.030127612873911858,\n",
      "            0.03517452999949455,\n",
      "            0.17612509429454803,\n",
      "            0.14963583648204803,\n",
      "            0.09463519603013992,\n",
      "            0.10010042786598206,\n",
      "            0.09969142824411392,\n",
      "            -0.1323796659708023,\n",
      "            0.08513017743825912,\n",
      "            0.09632150083780289,\n",
      "            0.026800885796546936,\n",
      "            0.03426150605082512,\n",
      "            0.1288107931613922,\n",
      "            -0.06863348186016083,\n",
      "            -0.17401985824108124,\n",
      "            -0.023457378149032593,\n",
      "            0.07261769473552704,\n",
      "            -0.09432029724121094,\n",
      "            0.08687113970518112,\n",
      "            0.1063615083694458,\n",
      "            0.010140166617929935,\n",
      "            -0.14241816103458405,\n",
      "            0.04963025823235512,\n",
      "            0.13229678571224213,\n",
      "            0.006357773672789335,\n",
      "            -0.005977957975119352,\n",
      "            0.03321073576807976,\n",
      "            -0.04842185601592064,\n",
      "            0.15767991542816162,\n",
      "            0.06374592334032059,\n",
      "            0.001378282904624939,\n",
      "            -0.07522328943014145,\n",
      "            0.04274260625243187,\n",
      "            0.09184210747480392,\n",
      "            -0.11515220999717712,\n",
      "            0.1708262413740158,\n",
      "            0.1405084729194641,\n",
      "            0.06750624626874924\n",
      "          ],\n",
      "          [\n",
      "            0.08931075036525726,\n",
      "            -0.002001883927732706,\n",
      "            -0.01074160635471344,\n",
      "            0.0873584896326065,\n",
      "            -0.03981199115514755,\n",
      "            -0.1693163812160492,\n",
      "            0.16908302903175354,\n",
      "            0.24191144108772278,\n",
      "            0.01859530806541443,\n",
      "            0.024666594341397285,\n",
      "            -0.07224830240011215,\n",
      "            -0.15605418384075165,\n",
      "            0.08678747713565826,\n",
      "            -0.16419436037540436,\n",
      "            0.1626041829586029,\n",
      "            -0.05346293747425079,\n",
      "            0.04802154004573822,\n",
      "            -0.08026482164859772,\n",
      "            -0.021597541868686676,\n",
      "            -0.03811773657798767,\n",
      "            0.12243808060884476,\n",
      "            -0.013886814936995506,\n",
      "            -0.17551088333129883,\n",
      "            0.10962368547916412,\n",
      "            0.0829489678144455,\n",
      "            -0.06657171994447708,\n",
      "            0.051575370132923126,\n",
      "            0.13635626435279846,\n",
      "            -0.09797456860542297,\n",
      "            -0.019355393946170807,\n",
      "            0.1068665012717247,\n",
      "            0.010064547881484032,\n",
      "            -0.0529891662299633,\n",
      "            0.004262798931449652,\n",
      "            -0.06846321374177933,\n",
      "            -0.0426141619682312,\n",
      "            0.11250866204500198,\n",
      "            -0.0004201846313662827,\n",
      "            0.01025743130594492,\n",
      "            -0.047825492918491364,\n",
      "            -0.12254028767347336,\n",
      "            -0.015214185230433941,\n",
      "            -0.04211658984422684,\n",
      "            -0.08039101213216782,\n",
      "            -0.039678458124399185,\n",
      "            -0.08875393867492676,\n",
      "            0.197662353515625,\n",
      "            -0.07372884452342987,\n",
      "            -0.16010114550590515,\n",
      "            -0.05751785635948181,\n",
      "            -0.19378963112831116,\n",
      "            -0.06331642717123032,\n",
      "            -0.01254187896847725,\n",
      "            0.05126965418457985,\n",
      "            -0.03844304755330086,\n",
      "            0.05915435776114464,\n",
      "            0.18614095449447632,\n",
      "            0.06253524869680405,\n",
      "            -0.08762910217046738,\n",
      "            -0.16723963618278503,\n",
      "            0.18040522933006287,\n",
      "            -0.1219080463051796,\n",
      "            0.20548149943351746,\n",
      "            0.014559854753315449,\n",
      "            0.10167352110147476,\n",
      "            -0.002246076939627528,\n",
      "            0.08056926727294922,\n",
      "            0.16312947869300842,\n",
      "            -0.08891049772500992,\n",
      "            0.05214919522404671,\n",
      "            -0.029452279210090637,\n",
      "            -0.10593642294406891,\n",
      "            -0.08224616944789886,\n",
      "            -0.14854703843593597,\n",
      "            -0.14218749105930328,\n",
      "            -0.10696791857481003,\n",
      "            0.10686297714710236,\n",
      "            0.05524789169430733,\n",
      "            -0.1694864183664322,\n",
      "            -0.12034351378679276,\n",
      "            0.015967758372426033,\n",
      "            0.018650690093636513,\n",
      "            0.032733190804719925,\n",
      "            -0.13160479068756104,\n",
      "            -0.09141535311937332,\n",
      "            0.04094519093632698,\n",
      "            0.1152990534901619,\n",
      "            0.029060835018754005,\n",
      "            0.12330228835344315,\n",
      "            0.16850878298282623,\n",
      "            -0.1622580885887146,\n",
      "            -0.1315333992242813,\n",
      "            0.09200259298086166,\n",
      "            -0.10129386931657791,\n",
      "            -0.022056974470615387,\n",
      "            -0.03451643884181976,\n",
      "            -0.023323150351643562,\n",
      "            -0.16614240407943726,\n",
      "            -0.13873498141765594,\n",
      "            -0.10150562226772308,\n",
      "            -0.10849243402481079,\n",
      "            0.07355722784996033,\n",
      "            0.017690083011984825,\n",
      "            -0.1848537176847458,\n",
      "            0.04376617446541786,\n",
      "            0.06274515390396118,\n",
      "            0.1678730696439743,\n",
      "            -0.13811083137989044,\n",
      "            0.1254330426454544,\n",
      "            -0.14872901141643524,\n",
      "            -0.01826419308781624,\n",
      "            0.12739184498786926,\n",
      "            -0.08627159148454666,\n",
      "            0.057164836674928665,\n",
      "            -0.10217055678367615,\n",
      "            0.07883723825216293,\n",
      "            0.00782652385532856,\n",
      "            0.057300079613924026,\n",
      "            -0.015602563507854939,\n",
      "            0.014399641193449497,\n",
      "            0.16999542713165283,\n",
      "            -0.13498902320861816,\n",
      "            0.14979413151741028,\n",
      "            -0.13771651685237885,\n",
      "            0.15685376524925232,\n",
      "            0.048985809087753296,\n",
      "            0.18962515890598297,\n",
      "            0.014704405330121517\n",
      "          ],\n",
      "          [\n",
      "            -0.06929926574230194,\n",
      "            0.005454780999571085,\n",
      "            0.21908651292324066,\n",
      "            0.007378190290182829,\n",
      "            -0.08335915952920914,\n",
      "            0.11271996796131134,\n",
      "            -0.03518449142575264,\n",
      "            -0.07774923741817474,\n",
      "            -0.006088943220674992,\n",
      "            0.02114252559840679,\n",
      "            -0.034772973507642746,\n",
      "            0.15905967354774475,\n",
      "            0.20494739711284637,\n",
      "            0.06934019178152084,\n",
      "            -0.010183180682361126,\n",
      "            0.08360664546489716,\n",
      "            -0.11745087802410126,\n",
      "            -0.0011375263566151261,\n",
      "            0.08172006905078888,\n",
      "            0.1602061241865158,\n",
      "            0.05872304365038872,\n",
      "            -0.08148511499166489,\n",
      "            0.0023541864939033985,\n",
      "            0.11506792902946472,\n",
      "            0.1922088861465454,\n",
      "            0.2411181628704071,\n",
      "            -0.015940329059958458,\n",
      "            0.0992652103304863,\n",
      "            -0.09154298901557922,\n",
      "            -0.062285374850034714,\n",
      "            -0.05703185871243477,\n",
      "            0.024970462545752525,\n",
      "            0.2277790606021881,\n",
      "            -0.15854978561401367,\n",
      "            -0.07455579191446304,\n",
      "            0.09250758588314056,\n",
      "            0.09751030802726746,\n",
      "            -0.08762548863887787,\n",
      "            0.18634086847305298,\n",
      "            0.18816325068473816,\n",
      "            0.22387324273586273,\n",
      "            -0.07963065057992935,\n",
      "            -0.06731126457452774,\n",
      "            -0.021048184484243393,\n",
      "            -0.15830619633197784,\n",
      "            0.09177101403474808,\n",
      "            -0.12046520411968231,\n",
      "            -0.12104447931051254,\n",
      "            -0.15848243236541748,\n",
      "            0.058796968311071396,\n",
      "            0.052012812346220016,\n",
      "            -0.10926098376512527,\n",
      "            0.2067367434501648,\n",
      "            -0.06244804337620735,\n",
      "            0.1905292123556137,\n",
      "            -0.0806170254945755,\n",
      "            0.18962328135967255,\n",
      "            -0.036742452532052994,\n",
      "            -0.08549707382917404,\n",
      "            -0.04224565252661705,\n",
      "            -0.12055595219135284,\n",
      "            -0.026170285418629646,\n",
      "            -0.13760283589363098,\n",
      "            0.03302270546555519,\n",
      "            -0.198204904794693,\n",
      "            -0.05342886224389076,\n",
      "            0.13413260877132416,\n",
      "            0.22454212605953217,\n",
      "            0.006622176617383957,\n",
      "            0.07628928124904633,\n",
      "            0.17116668820381165,\n",
      "            0.019939035177230835,\n",
      "            -0.1309366375207901,\n",
      "            0.19507727026939392,\n",
      "            -0.05311599373817444,\n",
      "            0.13057304918766022,\n",
      "            0.057069189846515656,\n",
      "            0.0930866003036499,\n",
      "            0.0775168240070343,\n",
      "            0.19493138790130615,\n",
      "            0.1463545709848404,\n",
      "            -0.029421523213386536,\n",
      "            0.1275189369916916,\n",
      "            -0.09025277942419052,\n",
      "            -0.08236206322908401,\n",
      "            0.017071397975087166,\n",
      "            0.04418492689728737,\n",
      "            -0.14962249994277954,\n",
      "            0.24054783582687378,\n",
      "            -0.04749103635549545,\n",
      "            0.14885161817073822,\n",
      "            0.06047597900032997,\n",
      "            0.11028037220239639,\n",
      "            0.03302809223532677,\n",
      "            0.14130975306034088,\n",
      "            0.000882097810972482,\n",
      "            -0.007300861179828644,\n",
      "            0.2727912962436676,\n",
      "            0.21236425638198853,\n",
      "            -0.01526205986738205,\n",
      "            -0.055572621524333954,\n",
      "            0.057828985154628754,\n",
      "            -0.10494574159383774,\n",
      "            0.08943551778793335,\n",
      "            0.03175218403339386,\n",
      "            0.07634289562702179,\n",
      "            -0.04246940463781357,\n",
      "            -0.07464569061994553,\n",
      "            0.03186042606830597,\n",
      "            -0.004704799968749285,\n",
      "            -0.08192119747400284,\n",
      "            0.14695337414741516,\n",
      "            -0.08344001322984695,\n",
      "            -0.05974848195910454,\n",
      "            -0.08954426646232605,\n",
      "            0.1348341852426529,\n",
      "            0.036235034465789795,\n",
      "            -0.18299542367458344,\n",
      "            0.010348795913159847,\n",
      "            0.04721696302294731,\n",
      "            0.1643567681312561,\n",
      "            0.11955846101045609,\n",
      "            -0.017036499455571175,\n",
      "            0.046824440360069275,\n",
      "            -0.1832248419523239,\n",
      "            -0.11348164826631546,\n",
      "            0.09559345990419388,\n",
      "            -0.10105372220277786\n",
      "          ],\n",
      "          [\n",
      "            0.0729418471455574,\n",
      "            -0.08595524728298187,\n",
      "            0.02931530959904194,\n",
      "            0.03351430222392082,\n",
      "            -0.0616803802549839,\n",
      "            0.04643309488892555,\n",
      "            0.1462521106004715,\n",
      "            0.07613075524568558,\n",
      "            0.11734939366579056,\n",
      "            -0.0266102347522974,\n",
      "            0.13511338829994202,\n",
      "            -0.11708752810955048,\n",
      "            0.14989349246025085,\n",
      "            -0.01822388730943203,\n",
      "            -0.022537007927894592,\n",
      "            -0.00011405348777770996,\n",
      "            0.061478182673454285,\n",
      "            0.13123725354671478,\n",
      "            -0.0037980626802891493,\n",
      "            0.1542714387178421,\n",
      "            -0.06451724469661713,\n",
      "            -0.015833290293812752,\n",
      "            -0.1675967574119568,\n",
      "            0.10581512749195099,\n",
      "            0.13546708226203918,\n",
      "            -0.05218745768070221,\n",
      "            -0.0264540184289217,\n",
      "            0.11206414550542831,\n",
      "            -0.031653981655836105,\n",
      "            0.00512889726087451,\n",
      "            -0.08667151629924774,\n",
      "            -0.11695591360330582,\n",
      "            -0.04509254917502403,\n",
      "            0.031199101358652115,\n",
      "            0.025462806224822998,\n",
      "            -0.011233947239816189,\n",
      "            -0.03602367639541626,\n",
      "            -0.11931291222572327,\n",
      "            -0.11888933181762695,\n",
      "            -0.11355685442686081,\n",
      "            0.05279942974448204,\n",
      "            -0.03691641613841057,\n",
      "            0.16499333083629608,\n",
      "            -0.13423219323158264,\n",
      "            -0.13422851264476776,\n",
      "            -0.09324837476015091,\n",
      "            -0.14574570953845978,\n",
      "            -0.15926747024059296,\n",
      "            0.14458820223808289,\n",
      "            -0.10698667913675308,\n",
      "            0.15289431810379028,\n",
      "            -0.04683303460478783,\n",
      "            -0.028750289231538773,\n",
      "            -0.08980578184127808,\n",
      "            0.006528415717184544,\n",
      "            -0.05592823773622513,\n",
      "            0.03760046884417534,\n",
      "            0.11641257256269455,\n",
      "            0.1530345231294632,\n",
      "            0.09290638566017151,\n",
      "            -0.040451813489198685,\n",
      "            0.01612098328769207,\n",
      "            0.13733337819576263,\n",
      "            0.08679531514644623,\n",
      "            0.12037551403045654,\n",
      "            0.13933850824832916,\n",
      "            -0.09873761981725693,\n",
      "            -0.019875261932611465,\n",
      "            0.1321413367986679,\n",
      "            -0.0554787814617157,\n",
      "            -0.16430668532848358,\n",
      "            0.015858760103583336,\n",
      "            -0.16705474257469177,\n",
      "            -0.04560353606939316,\n",
      "            -0.13371309638023376,\n",
      "            -0.07744324207305908,\n",
      "            -0.06642542779445648,\n",
      "            -0.07165315747261047,\n",
      "            -0.1766480803489685,\n",
      "            0.044624097645282745,\n",
      "            0.010883751325309277,\n",
      "            0.16101519763469696,\n",
      "            0.04522408917546272,\n",
      "            -0.08375819772481918,\n",
      "            0.03816557303071022,\n",
      "            0.15623313188552856,\n",
      "            0.14710648357868195,\n",
      "            0.128332257270813,\n",
      "            -0.09288996458053589,\n",
      "            -0.1628231406211853,\n",
      "            -0.11253802478313446,\n",
      "            0.06972622871398926,\n",
      "            0.11063180863857269,\n",
      "            0.038757409900426865,\n",
      "            -0.08572439849376678,\n",
      "            -0.0647667720913887,\n",
      "            0.1260017305612564,\n",
      "            -0.1369042992591858,\n",
      "            -0.05088493600487709,\n",
      "            0.014384284615516663,\n",
      "            -0.056325849145650864,\n",
      "            -0.048794761300086975,\n",
      "            -0.006094309035688639,\n",
      "            0.1660321056842804,\n",
      "            -0.06421990692615509,\n",
      "            -0.1447301208972931,\n",
      "            -0.1766839623451233,\n",
      "            -0.03153263032436371,\n",
      "            0.0975659191608429,\n",
      "            -0.0047226171009242535,\n",
      "            0.022657424211502075,\n",
      "            -0.12046806514263153,\n",
      "            -0.09473621845245361,\n",
      "            0.03080587089061737,\n",
      "            0.06929583102464676,\n",
      "            0.07684624940156937,\n",
      "            0.1315077692270279,\n",
      "            -0.14728738367557526,\n",
      "            -0.1468108743429184,\n",
      "            -0.031007403507828712,\n",
      "            0.05913497507572174,\n",
      "            0.06606737524271011,\n",
      "            0.07919517159461975,\n",
      "            0.10075510293245316,\n",
      "            -0.00937718152999878,\n",
      "            -0.10868079215288162,\n",
      "            0.0653843879699707,\n",
      "            0.05268861725926399\n",
      "          ],\n",
      "          [\n",
      "            0.1603160947561264,\n",
      "            -0.017245396971702576,\n",
      "            -0.07523782551288605,\n",
      "            0.1634892374277115,\n",
      "            -0.16550223529338837,\n",
      "            -0.013710473664104939,\n",
      "            -0.17506738007068634,\n",
      "            0.12634311616420746,\n",
      "            -0.04323968663811684,\n",
      "            -0.09621580690145493,\n",
      "            0.002426117891445756,\n",
      "            -0.15587638318538666,\n",
      "            -0.10661371797323227,\n",
      "            -0.16831664741039276,\n",
      "            0.015000401064753532,\n",
      "            -0.051236823201179504,\n",
      "            0.12438120692968369,\n",
      "            -0.1393916755914688,\n",
      "            0.03904221951961517,\n",
      "            -0.006083284970372915,\n",
      "            -0.1486937552690506,\n",
      "            0.15372467041015625,\n",
      "            -0.08827126771211624,\n",
      "            0.1267920583486557,\n",
      "            -0.13110944628715515,\n",
      "            0.015362597070634365,\n",
      "            -0.021235167980194092,\n",
      "            0.10224033147096634,\n",
      "            0.13034500181674957,\n",
      "            0.12517492473125458,\n",
      "            -0.02306174300611019,\n",
      "            -0.10518557578325272,\n",
      "            0.12408050894737244,\n",
      "            0.11093497276306152,\n",
      "            -0.16354170441627502,\n",
      "            0.034890372306108475,\n",
      "            -0.03590759262442589,\n",
      "            -0.04060470312833786,\n",
      "            0.006125505547970533,\n",
      "            -0.052521713078022,\n",
      "            0.13568364083766937,\n",
      "            0.09792324900627136,\n",
      "            -0.16063818335533142,\n",
      "            0.026179857552051544,\n",
      "            -0.08199213445186615,\n",
      "            0.07123458385467529,\n",
      "            0.026363274082541466,\n",
      "            -0.016929928213357925,\n",
      "            -0.11780093610286713,\n",
      "            -0.03624048829078674,\n",
      "            -0.020598823204636574,\n",
      "            -0.06215595826506615,\n",
      "            -0.12283056229352951,\n",
      "            -0.11097418516874313,\n",
      "            -0.16601403057575226,\n",
      "            -0.037991397082805634,\n",
      "            -0.10176972299814224,\n",
      "            -0.03964743763208389,\n",
      "            0.0338994562625885,\n",
      "            0.15732911229133606,\n",
      "            -0.043868470937013626,\n",
      "            0.07064967602491379,\n",
      "            0.11111516505479813,\n",
      "            0.1362954080104828,\n",
      "            0.13147874176502228,\n",
      "            0.001958526438102126,\n",
      "            -0.15756915509700775,\n",
      "            -0.06909802556037903,\n",
      "            -0.13663017749786377,\n",
      "            0.09243747591972351,\n",
      "            0.11219672858715057,\n",
      "            0.01860515959560871,\n",
      "            -0.15910743176937103,\n",
      "            -0.16176104545593262,\n",
      "            -0.07750490307807922,\n",
      "            -0.11554628610610962,\n",
      "            0.15667371451854706,\n",
      "            -0.10902755707502365,\n",
      "            -0.10621212422847748,\n",
      "            -0.04919596016407013,\n",
      "            0.07243100553750992,\n",
      "            -0.04509884491562843,\n",
      "            0.005906078964471817,\n",
      "            0.1791333556175232,\n",
      "            -0.05247965827584267,\n",
      "            -0.1549805849790573,\n",
      "            0.1399664431810379,\n",
      "            -0.04944465681910515,\n",
      "            -0.13121631741523743,\n",
      "            0.05041341856122017,\n",
      "            0.05548165738582611,\n",
      "            0.14235235750675201,\n",
      "            -0.17699363827705383,\n",
      "            0.07262494415044785,\n",
      "            0.14417360723018646,\n",
      "            -0.1700681895017624,\n",
      "            -0.05879871919751167,\n",
      "            -0.14547407627105713,\n",
      "            -0.06504014879465103,\n",
      "            -0.09542650729417801,\n",
      "            -0.15400002896785736,\n",
      "            0.11952906101942062,\n",
      "            -0.16415613889694214,\n",
      "            -0.016303136944770813,\n",
      "            0.12026210874319077,\n",
      "            0.05982942879199982,\n",
      "            -0.048278987407684326,\n",
      "            -0.14228662848472595,\n",
      "            0.05721450597047806,\n",
      "            0.011035267263650894,\n",
      "            -0.010542156174778938,\n",
      "            -0.11642946302890778,\n",
      "            0.07789450883865356,\n",
      "            0.07693049311637878,\n",
      "            0.0077257598750293255,\n",
      "            -0.07210130989551544,\n",
      "            0.01280232984572649,\n",
      "            0.07035715132951736,\n",
      "            -0.06317906081676483,\n",
      "            0.1754184067249298,\n",
      "            0.07419565320014954,\n",
      "            0.04377640038728714,\n",
      "            0.13760577142238617,\n",
      "            -0.16220249235630035,\n",
      "            -0.04669611155986786,\n",
      "            -0.11386982351541519,\n",
      "            0.06107771396636963,\n",
      "            0.033633194863796234\n",
      "          ],\n",
      "          [\n",
      "            -0.1180252879858017,\n",
      "            -0.04969831556081772,\n",
      "            0.12028730660676956,\n",
      "            -0.16804924607276917,\n",
      "            0.1617363542318344,\n",
      "            0.15021292865276337,\n",
      "            -0.06874901801347733,\n",
      "            -0.03275308012962341,\n",
      "            -0.13760581612586975,\n",
      "            0.013595311902463436,\n",
      "            0.1623278558254242,\n",
      "            0.022243602201342583,\n",
      "            0.002190323080867529,\n",
      "            0.1572410762310028,\n",
      "            0.041864074766635895,\n",
      "            0.05007261037826538,\n",
      "            -0.19052830338478088,\n",
      "            -0.03903437778353691,\n",
      "            0.048532869666814804,\n",
      "            -0.04190405085682869,\n",
      "            -0.01269960030913353,\n",
      "            -0.038993604481220245,\n",
      "            -0.001420705928467214,\n",
      "            0.14913222193717957,\n",
      "            0.13799235224723816,\n",
      "            0.14577773213386536,\n",
      "            0.19745832681655884,\n",
      "            -0.14281070232391357,\n",
      "            -0.100658118724823,\n",
      "            0.005680992268025875,\n",
      "            0.006031152792274952,\n",
      "            -0.05434367060661316,\n",
      "            0.20136870443820953,\n",
      "            -0.15827804803848267,\n",
      "            0.17490755021572113,\n",
      "            0.22176440060138702,\n",
      "            0.05707118287682533,\n",
      "            -0.04118650406599045,\n",
      "            -0.11551505327224731,\n",
      "            0.09019096940755844,\n",
      "            -0.08139423280954361,\n",
      "            0.04080650955438614,\n",
      "            -0.0868208184838295,\n",
      "            -0.0059481519274413586,\n",
      "            0.023884190246462822,\n",
      "            0.08254814147949219,\n",
      "            -0.18386420607566833,\n",
      "            -0.03153442591428757,\n",
      "            -0.12941224873065948,\n",
      "            -0.01707508973777294,\n",
      "            0.023074457421898842,\n",
      "            0.034307826310396194,\n",
      "            0.06890546530485153,\n",
      "            -0.09673072397708893,\n",
      "            0.12080822885036469,\n",
      "            0.13048231601715088,\n",
      "            0.12534436583518982,\n",
      "            0.1312723010778427,\n",
      "            -0.16007021069526672,\n",
      "            0.0792020857334137,\n",
      "            0.1560833603143692,\n",
      "            0.19309638440608978,\n",
      "            0.014990855008363724,\n",
      "            0.06698048859834671,\n",
      "            0.02342023141682148,\n",
      "            -0.03075954131782055,\n",
      "            0.044386353343725204,\n",
      "            0.19753995537757874,\n",
      "            0.13364370167255402,\n",
      "            -0.014333228580653667,\n",
      "            0.12571729719638824,\n",
      "            -0.06315457075834274,\n",
      "            -0.16688786447048187,\n",
      "            0.00850444845855236,\n",
      "            0.0990186259150505,\n",
      "            0.002339964499697089,\n",
      "            -0.04504895210266113,\n",
      "            -0.10714442282915115,\n",
      "            -0.11356886476278305,\n",
      "            -0.08754681050777435,\n",
      "            0.2435045838356018,\n",
      "            0.14965294301509857,\n",
      "            0.16966213285923004,\n",
      "            0.14420489966869354,\n",
      "            -0.05180153250694275,\n",
      "            0.06936309486627579,\n",
      "            0.16540618240833282,\n",
      "            0.024770306423306465,\n",
      "            0.22938555479049683,\n",
      "            -0.13220609724521637,\n",
      "            -0.18326030671596527,\n",
      "            0.013746201060712337,\n",
      "            0.15472015738487244,\n",
      "            0.07027466595172882,\n",
      "            0.16928724944591522,\n",
      "            0.20003634691238403,\n",
      "            -0.04286358132958412,\n",
      "            0.04744113236665726,\n",
      "            0.0027484295424073935,\n",
      "            -0.0027794688940048218,\n",
      "            -0.10512302815914154,\n",
      "            -0.07832930982112885,\n",
      "            -0.03109920397400856,\n",
      "            -0.0819321945309639,\n",
      "            -0.0995868518948555,\n",
      "            -0.06602876633405685,\n",
      "            -0.21096865832805634,\n",
      "            -0.12020694464445114,\n",
      "            -0.0630003958940506,\n",
      "            0.038963429629802704,\n",
      "            -0.026212073862552643,\n",
      "            -0.018298916518688202,\n",
      "            -0.01126543153077364,\n",
      "            -0.006580629851669073,\n",
      "            0.1894637495279312,\n",
      "            0.21034643054008484,\n",
      "            -0.036961641162633896,\n",
      "            -0.10979222506284714,\n",
      "            0.1883658617734909,\n",
      "            -0.1429755538702011,\n",
      "            0.1270851343870163,\n",
      "            0.0024395016953349113,\n",
      "            0.11828114092350006,\n",
      "            -0.14700695872306824,\n",
      "            -0.0722685232758522,\n",
      "            -0.15202471613883972,\n",
      "            0.029283521696925163,\n",
      "            0.08913099020719528\n",
      "          ],\n",
      "          [\n",
      "            -0.05123435705900192,\n",
      "            0.020571663975715637,\n",
      "            -0.10197169333696365,\n",
      "            0.16191387176513672,\n",
      "            0.12031693756580353,\n",
      "            -0.0853959247469902,\n",
      "            0.15886278450489044,\n",
      "            0.021846916526556015,\n",
      "            -0.04033908247947693,\n",
      "            0.0525028221309185,\n",
      "            0.016490086913108826,\n",
      "            -0.11105360090732574,\n",
      "            0.053514961153268814,\n",
      "            0.05858520418405533,\n",
      "            -0.050022780895233154,\n",
      "            0.13698016107082367,\n",
      "            0.13699722290039062,\n",
      "            -0.017248084768652916,\n",
      "            -0.13331417739391327,\n",
      "            0.17390982806682587,\n",
      "            0.14904649555683136,\n",
      "            0.05951856076717377,\n",
      "            -0.05271386355161667,\n",
      "            -0.13205303251743317,\n",
      "            0.054197970777750015,\n",
      "            -0.046749506145715714,\n",
      "            -0.050610799342393875,\n",
      "            -0.009460791014134884,\n",
      "            -0.11392616480588913,\n",
      "            0.00014088425086811185,\n",
      "            -0.07754325866699219,\n",
      "            -0.1705692559480667,\n",
      "            0.017521223053336143,\n",
      "            -0.15241780877113342,\n",
      "            -0.11221838742494583,\n",
      "            -0.11607825756072998,\n",
      "            -0.1635204255580902,\n",
      "            -0.03820333629846573,\n",
      "            -0.06076635792851448,\n",
      "            0.120533786714077,\n",
      "            0.12572936713695526,\n",
      "            0.14581908285617828,\n",
      "            0.13303889334201813,\n",
      "            -0.13501714169979095,\n",
      "            -0.08687147498130798,\n",
      "            -0.14359979331493378,\n",
      "            -0.0032686120830476284,\n",
      "            0.10679039359092712,\n",
      "            -0.10720792412757874,\n",
      "            -0.07217195630073547,\n",
      "            0.09340020269155502,\n",
      "            0.018994132056832314,\n",
      "            0.00202217698097229,\n",
      "            -0.05310316011309624,\n",
      "            -0.01766013354063034,\n",
      "            -0.05469236895442009,\n",
      "            -0.1341707557439804,\n",
      "            -0.052202433347702026,\n",
      "            0.065283864736557,\n",
      "            -0.030926531180739403,\n",
      "            -0.1764286458492279,\n",
      "            -0.11682549864053726,\n",
      "            0.008576829917728901,\n",
      "            -0.09509588778018951,\n",
      "            0.12676607072353363,\n",
      "            0.176168754696846,\n",
      "            0.1707346886396408,\n",
      "            0.05764796584844589,\n",
      "            -0.025352925062179565,\n",
      "            0.06659451127052307,\n",
      "            0.025833813473582268,\n",
      "            -0.1356794536113739,\n",
      "            -0.06839222460985184,\n",
      "            -0.13017699122428894,\n",
      "            -0.1646282970905304,\n",
      "            0.07835207879543304,\n",
      "            0.10647425800561905,\n",
      "            0.12805087864398956,\n",
      "            -0.08475831896066666,\n",
      "            0.05426159128546715,\n",
      "            0.03863245248794556,\n",
      "            -0.050066739320755005,\n",
      "            -0.132696270942688,\n",
      "            -0.06585542857646942,\n",
      "            -0.12877795100212097,\n",
      "            -0.00025200709933415055,\n",
      "            -0.1726323515176773,\n",
      "            -0.1582963913679123,\n",
      "            -0.05741167441010475,\n",
      "            0.08481310307979584,\n",
      "            0.10681714117527008,\n",
      "            0.048124078661203384,\n",
      "            -0.12957054376602173,\n",
      "            -0.017359573394060135,\n",
      "            0.11053551733493805,\n",
      "            -0.039643626660108566,\n",
      "            -0.0464804545044899,\n",
      "            -0.16825903952121735,\n",
      "            -0.018969658762216568,\n",
      "            0.15923616290092468,\n",
      "            0.009704777970910072,\n",
      "            0.08580370247364044,\n",
      "            -0.08025559037923813,\n",
      "            0.13962946832180023,\n",
      "            -0.12798985838890076,\n",
      "            -0.044625505805015564,\n",
      "            -0.15040387213230133,\n",
      "            -0.004503101110458374,\n",
      "            -0.12721523642539978,\n",
      "            -0.16626498103141785,\n",
      "            -0.07205357402563095,\n",
      "            0.08668987452983856,\n",
      "            -0.13523662090301514,\n",
      "            -0.15781089663505554,\n",
      "            0.031020835041999817,\n",
      "            0.046089306473731995,\n",
      "            0.01945282705128193,\n",
      "            -0.015782561153173447,\n",
      "            -0.06651291996240616,\n",
      "            -0.13365885615348816,\n",
      "            0.07844869792461395,\n",
      "            0.1616460233926773,\n",
      "            -0.05279507115483284,\n",
      "            0.12329402565956116,\n",
      "            0.16022305190563202,\n",
      "            0.08717004954814911,\n",
      "            0.05387662723660469,\n",
      "            0.04530742019414902\n",
      "          ],\n",
      "          [\n",
      "            -0.15290217101573944,\n",
      "            -0.12797358632087708,\n",
      "            0.21990007162094116,\n",
      "            -0.13390609622001648,\n",
      "            -0.06644144654273987,\n",
      "            0.18174506723880768,\n",
      "            0.12594828009605408,\n",
      "            -0.05233313515782356,\n",
      "            0.11857134103775024,\n",
      "            0.15370365977287292,\n",
      "            0.07752321660518646,\n",
      "            0.03995662182569504,\n",
      "            -0.029569881036877632,\n",
      "            -0.14792321622371674,\n",
      "            0.16305385529994965,\n",
      "            0.008591845631599426,\n",
      "            -0.1901404708623886,\n",
      "            -0.10973076522350311,\n",
      "            0.13214412331581116,\n",
      "            0.00507100997492671,\n",
      "            -0.12616966664791107,\n",
      "            0.11742377281188965,\n",
      "            0.06343244016170502,\n",
      "            0.09749383479356766,\n",
      "            0.22198709845542908,\n",
      "            0.09853360056877136,\n",
      "            0.05363298952579498,\n",
      "            -0.10166802257299423,\n",
      "            -0.0569300502538681,\n",
      "            0.1504650115966797,\n",
      "            0.03424316644668579,\n",
      "            0.10017477720975876,\n",
      "            0.17191265523433685,\n",
      "            0.02050909958779812,\n",
      "            -0.042786553502082825,\n",
      "            0.07844972610473633,\n",
      "            0.08378354460000992,\n",
      "            -0.046256497502326965,\n",
      "            -0.08976055681705475,\n",
      "            -0.005439226049929857,\n",
      "            0.10354041308164597,\n",
      "            0.1930704563856125,\n",
      "            0.12249789386987686,\n",
      "            -0.07323359698057175,\n",
      "            0.13508813083171844,\n",
      "            -0.059904031455516815,\n",
      "            -0.20958197116851807,\n",
      "            0.06649887561798096,\n",
      "            -0.1276574730873108,\n",
      "            0.1319890171289444,\n",
      "            0.01141311228275299,\n",
      "            0.1381412297487259,\n",
      "            0.022036472335457802,\n",
      "            -0.06319894641637802,\n",
      "            0.10675467550754547,\n",
      "            0.06096867471933365,\n",
      "            0.07785941660404205,\n",
      "            0.16025538742542267,\n",
      "            0.10401999205350876,\n",
      "            0.030122077092528343,\n",
      "            0.1795615553855896,\n",
      "            0.08813825994729996,\n",
      "            0.05123789981007576,\n",
      "            -0.17487867176532745,\n",
      "            0.003955720458179712,\n",
      "            -0.09655307233333588,\n",
      "            0.10554032027721405,\n",
      "            -0.0027337234932929277,\n",
      "            0.11083681881427765,\n",
      "            -0.14326834678649902,\n",
      "            0.006322308909147978,\n",
      "            -0.03662242740392685,\n",
      "            0.04688824713230133,\n",
      "            0.06682279706001282,\n",
      "            0.14091970026493073,\n",
      "            0.010890309698879719,\n",
      "            -0.03358009085059166,\n",
      "            0.08338533341884613,\n",
      "            -0.10652617365121841,\n",
      "            0.09518023580312729,\n",
      "            -0.08813194185495377,\n",
      "            0.10915076732635498,\n",
      "            -0.15826411545276642,\n",
      "            -0.0030255962628871202,\n",
      "            -0.11697308719158173,\n",
      "            0.13051293790340424,\n",
      "            -0.013767547905445099,\n",
      "            0.08045651018619537,\n",
      "            0.2288728505373001,\n",
      "            -0.06860590726137161,\n",
      "            0.017122343182563782,\n",
      "            0.11063661426305771,\n",
      "            -0.08261999487876892,\n",
      "            -0.06399688869714737,\n",
      "            0.1817929595708847,\n",
      "            0.10711583495140076,\n",
      "            -0.07232701778411865,\n",
      "            0.23246489465236664,\n",
      "            0.19224515557289124,\n",
      "            0.055211056023836136,\n",
      "            0.03555645793676376,\n",
      "            -0.16130004823207855,\n",
      "            -0.12385644018650055,\n",
      "            -0.12473499029874802,\n",
      "            -0.017915915697813034,\n",
      "            0.08315517008304596,\n",
      "            -0.197941392660141,\n",
      "            0.13383476436138153,\n",
      "            -0.05698598921298981,\n",
      "            0.0507858581840992,\n",
      "            -0.07918846607208252,\n",
      "            0.14654359221458435,\n",
      "            0.03804831951856613,\n",
      "            -0.009565314278006554,\n",
      "            0.19654546678066254,\n",
      "            0.15718261897563934,\n",
      "            -0.10937230288982391,\n",
      "            0.10277911275625229,\n",
      "            0.2579902708530426,\n",
      "            -0.19732452929019928,\n",
      "            0.0024445950984954834,\n",
      "            -0.04798004776239395,\n",
      "            0.14870236814022064,\n",
      "            -0.140834242105484,\n",
      "            -0.019533686339855194,\n",
      "            0.06626015156507492,\n",
      "            -0.1467849612236023,\n",
      "            -0.19691592454910278\n",
      "          ],\n",
      "          [\n",
      "            -0.001569046638906002,\n",
      "            -0.06833523511886597,\n",
      "            0.05925581976771355,\n",
      "            -0.02752823941409588,\n",
      "            -0.04461868107318878,\n",
      "            0.06626907736063004,\n",
      "            -0.024243948981165886,\n",
      "            0.05353371053934097,\n",
      "            -0.06623010337352753,\n",
      "            0.02630525454878807,\n",
      "            0.08982260525226593,\n",
      "            -0.13591144979000092,\n",
      "            0.044835902750492096,\n",
      "            0.056620605289936066,\n",
      "            -0.16484412550926208,\n",
      "            0.1321568340063095,\n",
      "            -0.1533316969871521,\n",
      "            -0.10278048366308212,\n",
      "            -0.07037406414747238,\n",
      "            -0.041148215532302856,\n",
      "            -0.07318305224180222,\n",
      "            0.011562468484044075,\n",
      "            0.12719379365444183,\n",
      "            -0.04614633321762085,\n",
      "            -0.15356452763080597,\n",
      "            -0.10745711624622345,\n",
      "            0.008801193907856941,\n",
      "            0.06629256159067154,\n",
      "            -0.1001785546541214,\n",
      "            -0.1756320744752884,\n",
      "            0.09324277192354202,\n",
      "            -0.162889763712883,\n",
      "            0.16948489844799042,\n",
      "            -0.14878155291080475,\n",
      "            0.1068173199892044,\n",
      "            -0.0973953828215599,\n",
      "            0.0752052292227745,\n",
      "            -0.08731798827648163,\n",
      "            0.03239081799983978,\n",
      "            -0.1545863002538681,\n",
      "            0.039611540734767914,\n",
      "            -0.16072426736354828,\n",
      "            0.025906894356012344,\n",
      "            -0.05401923507452011,\n",
      "            0.03153171390295029,\n",
      "            0.06700539588928223,\n",
      "            0.038263238966464996,\n",
      "            0.0844356119632721,\n",
      "            -0.12759047746658325,\n",
      "            -0.16221141815185547,\n",
      "            -0.06109747290611267,\n",
      "            0.014721574261784554,\n",
      "            -0.1417158991098404,\n",
      "            0.127594456076622,\n",
      "            0.11310099065303802,\n",
      "            0.16662132740020752,\n",
      "            -0.0348639078438282,\n",
      "            0.141316756606102,\n",
      "            -0.17167186737060547,\n",
      "            0.023197662085294724,\n",
      "            0.03284233435988426,\n",
      "            0.10949565470218658,\n",
      "            -0.0644366517663002,\n",
      "            -0.07910344004631042,\n",
      "            0.14549580216407776,\n",
      "            -0.05385460704565048,\n",
      "            0.006734564900398254,\n",
      "            -0.061522435396909714,\n",
      "            -0.17005443572998047,\n",
      "            0.038357555866241455,\n",
      "            0.08319540321826935,\n",
      "            -0.09984711557626724,\n",
      "            -0.15552322566509247,\n",
      "            -0.07541760057210922,\n",
      "            0.05593718960881233,\n",
      "            -0.08531389385461807,\n",
      "            -0.03630019724369049,\n",
      "            0.10250123590230942,\n",
      "            0.15181244909763336,\n",
      "            0.005838818848133087,\n",
      "            0.0001898562186397612,\n",
      "            0.14067386090755463,\n",
      "            0.03040376305580139,\n",
      "            0.045574214309453964,\n",
      "            0.12425337731838226,\n",
      "            0.09894683212041855,\n",
      "            0.0694420263171196,\n",
      "            0.1362740844488144,\n",
      "            -0.06996871531009674,\n",
      "            -0.02599879913032055,\n",
      "            -0.12832951545715332,\n",
      "            -0.07935240119695663,\n",
      "            -0.02923452854156494,\n",
      "            -0.0426819883286953,\n",
      "            0.09483576565980911,\n",
      "            0.031002869829535484,\n",
      "            -0.12473256886005402,\n",
      "            -0.011093505658209324,\n",
      "            0.013586051762104034,\n",
      "            -0.03408075496554375,\n",
      "            0.13227377831935883,\n",
      "            -0.11664555966854095,\n",
      "            -0.13365796208381653,\n",
      "            0.07045961916446686,\n",
      "            0.09586064517498016,\n",
      "            0.12760628759860992,\n",
      "            -0.08218042552471161,\n",
      "            0.1560920625925064,\n",
      "            -0.01823917031288147,\n",
      "            -0.007731019519269466,\n",
      "            0.017870495095849037,\n",
      "            0.09870458394289017,\n",
      "            0.1302821785211563,\n",
      "            -0.11044105887413025,\n",
      "            0.024986663833260536,\n",
      "            0.08212000131607056,\n",
      "            -0.15503305196762085,\n",
      "            -0.026773601770401,\n",
      "            0.12306420505046844,\n",
      "            -0.049503766000270844,\n",
      "            -0.0005985647439956665,\n",
      "            -0.06022385507822037,\n",
      "            0.0015079928562045097,\n",
      "            0.04463958740234375,\n",
      "            -0.09808483719825745,\n",
      "            0.15404798090457916,\n",
      "            -0.08396334201097488,\n",
      "            -0.12020035833120346\n",
      "          ],\n",
      "          [\n",
      "            -0.16458484530448914,\n",
      "            0.04114750400185585,\n",
      "            -0.09684257209300995,\n",
      "            0.04978075250983238,\n",
      "            -0.0019973788876086473,\n",
      "            0.09276310354471207,\n",
      "            0.08089397847652435,\n",
      "            0.13354359567165375,\n",
      "            0.0971689373254776,\n",
      "            0.05583900585770607,\n",
      "            -0.0008859157096594572,\n",
      "            -0.018544215708971024,\n",
      "            0.044176653027534485,\n",
      "            -0.06962797045707703,\n",
      "            -0.16232292354106903,\n",
      "            0.14810936152935028,\n",
      "            0.061943043023347855,\n",
      "            0.1561187356710434,\n",
      "            0.08233669400215149,\n",
      "            0.16257676482200623,\n",
      "            0.19958391785621643,\n",
      "            0.1325957477092743,\n",
      "            -0.08383354544639587,\n",
      "            -0.15142656862735748,\n",
      "            0.22902147471904755,\n",
      "            -0.034588634967803955,\n",
      "            -0.010677451267838478,\n",
      "            -0.04319434612989426,\n",
      "            -0.043172094970941544,\n",
      "            0.16055867075920105,\n",
      "            -0.13246680796146393,\n",
      "            -0.0007618560339324176,\n",
      "            -0.007660606876015663,\n",
      "            -0.07457838207483292,\n",
      "            -0.05848423391580582,\n",
      "            0.09361081570386887,\n",
      "            0.1870414912700653,\n",
      "            0.16623973846435547,\n",
      "            -0.0563172847032547,\n",
      "            -0.07966797798871994,\n",
      "            0.06129122152924538,\n",
      "            0.16239389777183533,\n",
      "            0.09337028861045837,\n",
      "            0.06803680956363678,\n",
      "            -0.021855857223272324,\n",
      "            -0.04331079497933388,\n",
      "            -0.11334002017974854,\n",
      "            -0.03648309409618378,\n",
      "            0.06354223936796188,\n",
      "            -0.009038271382451057,\n",
      "            -0.08708811551332474,\n",
      "            -0.0701611191034317,\n",
      "            0.14022207260131836,\n",
      "            0.04758763313293457,\n",
      "            0.19875960052013397,\n",
      "            0.1457974761724472,\n",
      "            0.09190204739570618,\n",
      "            -0.0599738210439682,\n",
      "            0.12307164818048477,\n",
      "            0.06738121807575226,\n",
      "            0.01032956037670374,\n",
      "            0.023450588807463646,\n",
      "            -0.08245772123336792,\n",
      "            -0.09091394394636154,\n",
      "            0.004168974235653877,\n",
      "            -0.1473729908466339,\n",
      "            0.01784149557352066,\n",
      "            0.18149688839912415,\n",
      "            -0.13778293132781982,\n",
      "            0.004541363567113876,\n",
      "            0.04730081185698509,\n",
      "            -0.022177109494805336,\n",
      "            -0.1518610566854477,\n",
      "            0.014059099368751049,\n",
      "            0.17314212024211884,\n",
      "            0.05299045890569687,\n",
      "            -0.013349377550184727,\n",
      "            0.04317139834165573,\n",
      "            -0.01496872864663601,\n",
      "            0.10894963145256042,\n",
      "            -0.004624507389962673,\n",
      "            -0.027261069044470787,\n",
      "            0.0467245988547802,\n",
      "            -0.13528601825237274,\n",
      "            -0.08825840801000595,\n",
      "            -0.1025867909193039,\n",
      "            0.10387247800827026,\n",
      "            0.0028839129954576492,\n",
      "            0.08467472344636917,\n",
      "            -0.14011304080486298,\n",
      "            -0.04613780975341797,\n",
      "            0.06896303594112396,\n",
      "            0.05808267369866371,\n",
      "            0.11365102976560593,\n",
      "            0.21393534541130066,\n",
      "            -0.07380278408527374,\n",
      "            -0.21532107889652252,\n",
      "            0.25424912571907043,\n",
      "            0.25600409507751465,\n",
      "            0.02388800121843815,\n",
      "            -0.09453950822353363,\n",
      "            0.03566378355026245,\n",
      "            0.06943100690841675,\n",
      "            -0.11056046187877655,\n",
      "            0.11329986900091171,\n",
      "            -0.08305605500936508,\n",
      "            -0.11270876228809357,\n",
      "            -0.11677934229373932,\n",
      "            0.0675000324845314,\n",
      "            0.053516969084739685,\n",
      "            0.10742053389549255,\n",
      "            0.01333769503980875,\n",
      "            -0.08494804054498672,\n",
      "            0.13727930188179016,\n",
      "            -0.026945792138576508,\n",
      "            0.15311799943447113,\n",
      "            0.04637971147894859,\n",
      "            0.1361680030822754,\n",
      "            -0.03520384803414345,\n",
      "            -0.06047143414616585,\n",
      "            0.10811945050954819,\n",
      "            -0.09076865017414093,\n",
      "            0.051970645785331726,\n",
      "            0.005309732630848885,\n",
      "            0.07699770480394363,\n",
      "            -0.005687682889401913,\n",
      "            0.15429110825061798,\n",
      "            0.11971518397331238\n",
      "          ],\n",
      "          [\n",
      "            0.10122691839933395,\n",
      "            -0.03931819647550583,\n",
      "            -0.02265881933271885,\n",
      "            0.12317799776792526,\n",
      "            0.16767235100269318,\n",
      "            -0.16071373224258423,\n",
      "            -0.07937072217464447,\n",
      "            0.03504468873143196,\n",
      "            0.10321428626775742,\n",
      "            0.13951170444488525,\n",
      "            -0.09542113542556763,\n",
      "            0.14244095981121063,\n",
      "            -0.01641109772026539,\n",
      "            0.0827854797244072,\n",
      "            0.0966176986694336,\n",
      "            0.13737986981868744,\n",
      "            -0.14901961386203766,\n",
      "            -0.00994125846773386,\n",
      "            0.09429675340652466,\n",
      "            0.12608879804611206,\n",
      "            0.04015028104186058,\n",
      "            0.11416489630937576,\n",
      "            -0.023607777431607246,\n",
      "            0.0065205032005906105,\n",
      "            -0.08780032396316528,\n",
      "            -0.033984728157520294,\n",
      "            -0.0017689591040834785,\n",
      "            -0.08748602867126465,\n",
      "            -0.1620131880044937,\n",
      "            0.03863891214132309,\n",
      "            0.005957613233476877,\n",
      "            -0.04428795725107193,\n",
      "            -0.1363004893064499,\n",
      "            0.09191040694713593,\n",
      "            -0.11196765303611755,\n",
      "            0.1281585693359375,\n",
      "            -0.086000956594944,\n",
      "            0.07654435187578201,\n",
      "            0.09069894254207611,\n",
      "            0.16351011395454407,\n",
      "            0.12055458128452301,\n",
      "            -0.012162755243480206,\n",
      "            -0.10677283257246017,\n",
      "            -0.06125573813915253,\n",
      "            0.1430298537015915,\n",
      "            -0.03048870898783207,\n",
      "            0.14699110388755798,\n",
      "            -0.07578036934137344,\n",
      "            -0.07528059929609299,\n",
      "            0.1067514643073082,\n",
      "            -0.04871534928679466,\n",
      "            0.0810890793800354,\n",
      "            0.17307262122631073,\n",
      "            0.023085881024599075,\n",
      "            -0.02651010826230049,\n",
      "            0.019033392891287804,\n",
      "            -0.016739245504140854,\n",
      "            -0.07337073981761932,\n",
      "            -0.13795556128025055,\n",
      "            0.08354712277650833,\n",
      "            0.0282029677182436,\n",
      "            0.13491348922252655,\n",
      "            0.14369726181030273,\n",
      "            -0.11322718858718872,\n",
      "            -0.14091117680072784,\n",
      "            0.15822894871234894,\n",
      "            0.15944872796535492,\n",
      "            -0.1025354340672493,\n",
      "            0.06581756472587585,\n",
      "            -0.14911331236362457,\n",
      "            0.027524197474122047,\n",
      "            -0.09229373186826706,\n",
      "            0.01651856116950512,\n",
      "            -0.0809255987405777,\n",
      "            0.16281579434871674,\n",
      "            0.1606801599264145,\n",
      "            0.03984779864549637,\n",
      "            0.16573835909366608,\n",
      "            0.11790917068719864,\n",
      "            -0.15990547835826874,\n",
      "            0.0524883009493351,\n",
      "            -0.06931547075510025,\n",
      "            -0.10658629238605499,\n",
      "            0.16838373243808746,\n",
      "            -0.09699870645999908,\n",
      "            -0.022699883207678795,\n",
      "            0.013217604719102383,\n",
      "            0.11845501512289047,\n",
      "            0.09228655695915222,\n",
      "            0.01907043345272541,\n",
      "            -0.06741952151060104,\n",
      "            -0.1713370531797409,\n",
      "            0.025126561522483826,\n",
      "            0.12827111780643463,\n",
      "            -0.04270997643470764,\n",
      "            -0.10954182595014572,\n",
      "            -0.11850202828645706,\n",
      "            0.048440638929605484,\n",
      "            0.07326303422451019,\n",
      "            -0.12903620302677155,\n",
      "            -0.04398095980286598,\n",
      "            -0.06279625743627548,\n",
      "            -0.16645123064517975,\n",
      "            -0.16732922196388245,\n",
      "            0.014475145377218723,\n",
      "            -0.05855933576822281,\n",
      "            0.09479021281003952,\n",
      "            0.039925504475831985,\n",
      "            0.09972012788057327,\n",
      "            -0.12458053976297379,\n",
      "            0.10632048547267914,\n",
      "            0.0451141856610775,\n",
      "            -0.10499154031276703,\n",
      "            -0.0029675490222871304,\n",
      "            0.13333159685134888,\n",
      "            0.11431922763586044,\n",
      "            -0.0744103342294693,\n",
      "            0.006849153898656368,\n",
      "            -0.10500101745128632,\n",
      "            0.08102316409349442,\n",
      "            0.021775497123599052,\n",
      "            -0.027081985026597977,\n",
      "            -0.1710026115179062,\n",
      "            -0.16011957824230194,\n",
      "            -0.06737460941076279,\n",
      "            7.06017017364502e-05,\n",
      "            -0.16481800377368927,\n",
      "            -0.09092473983764648\n",
      "          ],\n",
      "          [\n",
      "            -0.09688647836446762,\n",
      "            0.11074633151292801,\n",
      "            0.14935217797756195,\n",
      "            0.07763794809579849,\n",
      "            0.052981965243816376,\n",
      "            -0.0017781704664230347,\n",
      "            -0.13341498374938965,\n",
      "            -0.136779323220253,\n",
      "            -0.11068113148212433,\n",
      "            -0.1271066814661026,\n",
      "            0.014019417576491833,\n",
      "            -0.1712837964296341,\n",
      "            -0.10655660182237625,\n",
      "            0.03443778306245804,\n",
      "            -0.11854619532823563,\n",
      "            0.1577402800321579,\n",
      "            0.049893490970134735,\n",
      "            -0.1277257651090622,\n",
      "            0.016483083367347717,\n",
      "            0.1574961543083191,\n",
      "            0.010166073217988014,\n",
      "            -0.03050285577774048,\n",
      "            0.09875033795833588,\n",
      "            -0.031001076102256775,\n",
      "            0.11711035668849945,\n",
      "            -0.03166079521179199,\n",
      "            0.13349761068820953,\n",
      "            -0.16755376756191254,\n",
      "            0.14101041853427887,\n",
      "            0.08228220045566559,\n",
      "            -0.07188165932893753,\n",
      "            -0.13664774596691132,\n",
      "            0.09824465960264206,\n",
      "            0.07397470623254776,\n",
      "            -0.15795032680034637,\n",
      "            -0.001437751343473792,\n",
      "            0.11498042196035385,\n",
      "            -0.07489064335823059,\n",
      "            -0.13450483977794647,\n",
      "            -0.0051132263615727425,\n",
      "            -0.1294056624174118,\n",
      "            0.08152325451374054,\n",
      "            -0.08695179224014282,\n",
      "            -0.17314372956752777,\n",
      "            0.028617799282073975,\n",
      "            -0.13634085655212402,\n",
      "            -0.15045306086540222,\n",
      "            0.1063966304063797,\n",
      "            0.0006347643211483955,\n",
      "            0.07250563055276871,\n",
      "            0.1753026396036148,\n",
      "            -0.06868205219507217,\n",
      "            -0.08666791021823883,\n",
      "            0.0065026916563510895,\n",
      "            -0.05415501445531845,\n",
      "            -0.0326557531952858,\n",
      "            0.10847803205251694,\n",
      "            -0.07226202636957169,\n",
      "            0.011183613911271095,\n",
      "            -0.02715257741510868,\n",
      "            0.12875023484230042,\n",
      "            -0.08949250727891922,\n",
      "            0.03995387256145477,\n",
      "            -0.10982313007116318,\n",
      "            0.023091653361916542,\n",
      "            0.02241748385131359,\n",
      "            -0.07533054798841476,\n",
      "            0.009752136655151844,\n",
      "            -0.13274002075195312,\n",
      "            -0.030664488673210144,\n",
      "            0.060903921723365784,\n",
      "            -0.07257416099309921,\n",
      "            0.16719578206539154,\n",
      "            0.05882725492119789,\n",
      "            -0.1520576924085617,\n",
      "            -0.03516654297709465,\n",
      "            -0.03262251242995262,\n",
      "            0.11963324248790741,\n",
      "            -0.09782706201076508,\n",
      "            -0.0553697869181633,\n",
      "            -0.13457834720611572,\n",
      "            0.02255932241678238,\n",
      "            0.05450805649161339,\n",
      "            0.1253778636455536,\n",
      "            -0.08720086514949799,\n",
      "            0.03426884859800339,\n",
      "            -0.1371975541114807,\n",
      "            0.027711383998394012,\n",
      "            0.01711971126496792,\n",
      "            0.09646596759557724,\n",
      "            0.11857514083385468,\n",
      "            -0.03226311877369881,\n",
      "            0.01807570643723011,\n",
      "            -0.16907978057861328,\n",
      "            -0.024872535839676857,\n",
      "            -0.09252423048019409,\n",
      "            -0.15481550991535187,\n",
      "            -0.09927135705947876,\n",
      "            -0.0037963930517435074,\n",
      "            -0.10905495285987854,\n",
      "            0.032257456332445145,\n",
      "            -0.10260473936796188,\n",
      "            0.15667371451854706,\n",
      "            0.08243037015199661,\n",
      "            0.15369103848934174,\n",
      "            0.16189886629581451,\n",
      "            -0.16691280901432037,\n",
      "            0.15825925767421722,\n",
      "            0.17447929084300995,\n",
      "            -0.021458962932229042,\n",
      "            -0.0707908570766449,\n",
      "            -0.08509975671768188,\n",
      "            0.02372409589588642,\n",
      "            -0.07596998661756516,\n",
      "            0.15771739184856415,\n",
      "            -0.11231998354196548,\n",
      "            0.13291685283184052,\n",
      "            0.004520270507782698,\n",
      "            -0.12175394594669342,\n",
      "            0.13353751599788666,\n",
      "            -0.010652674362063408,\n",
      "            -0.1705833226442337,\n",
      "            -0.009028553031384945,\n",
      "            -0.036914192140102386,\n",
      "            0.1265781670808792,\n",
      "            -0.14640574157238007,\n",
      "            0.006624684203416109,\n",
      "            -0.09973379969596863\n",
      "          ],\n",
      "          [\n",
      "            0.07511655986309052,\n",
      "            0.04678305238485336,\n",
      "            0.00381399760954082,\n",
      "            -0.14056788384914398,\n",
      "            -0.17556139826774597,\n",
      "            0.09004055708646774,\n",
      "            -0.06998532265424728,\n",
      "            -0.09811194986104965,\n",
      "            -0.16058290004730225,\n",
      "            0.14213038980960846,\n",
      "            -0.1430014669895172,\n",
      "            -0.05148124694824219,\n",
      "            0.20903345942497253,\n",
      "            0.15577609837055206,\n",
      "            0.11759164929389954,\n",
      "            0.008088871836662292,\n",
      "            0.11275850981473923,\n",
      "            -0.0036287005059421062,\n",
      "            -0.07607103884220123,\n",
      "            0.02230999432504177,\n",
      "            0.19932246208190918,\n",
      "            0.07479863613843918,\n",
      "            0.050982777029275894,\n",
      "            -0.1065763384103775,\n",
      "            -0.004261359106749296,\n",
      "            0.021513083949685097,\n",
      "            -0.04110463336110115,\n",
      "            -0.15549619495868683,\n",
      "            0.172748401761055,\n",
      "            0.06564751267433167,\n",
      "            -0.09786220639944077,\n",
      "            -0.005106878466904163,\n",
      "            -0.000716217968147248,\n",
      "            0.01908252201974392,\n",
      "            -0.12404613941907883,\n",
      "            -0.09548059850931168,\n",
      "            0.1183989867568016,\n",
      "            -0.17122966051101685,\n",
      "            -0.02451203018426895,\n",
      "            0.1336139440536499,\n",
      "            -0.07802393287420273,\n",
      "            0.19464804232120514,\n",
      "            -0.07930373400449753,\n",
      "            0.009667241014540195,\n",
      "            0.12631762027740479,\n",
      "            -0.10937155783176422,\n",
      "            0.08152936398983002,\n",
      "            -0.0605262890458107,\n",
      "            -0.08121670037508011,\n",
      "            0.02711494080722332,\n",
      "            0.14573509991168976,\n",
      "            0.18560540676116943,\n",
      "            0.13408996164798737,\n",
      "            -0.1317138969898224,\n",
      "            0.21040116250514984,\n",
      "            -0.06062585487961769,\n",
      "            0.03411073237657547,\n",
      "            0.2142837941646576,\n",
      "            -0.0334753543138504,\n",
      "            0.014764458872377872,\n",
      "            0.20612303912639618,\n",
      "            0.14993490278720856,\n",
      "            -0.1652013063430786,\n",
      "            -0.012271665968000889,\n",
      "            0.18277044594287872,\n",
      "            0.1557895392179489,\n",
      "            -0.12762397527694702,\n",
      "            0.1856669932603836,\n",
      "            0.10566703975200653,\n",
      "            0.09081294387578964,\n",
      "            0.08323084563016891,\n",
      "            -0.04969516396522522,\n",
      "            -0.08663957566022873,\n",
      "            0.2654067873954773,\n",
      "            -0.04436881095170975,\n",
      "            0.10242445021867752,\n",
      "            0.04642690345644951,\n",
      "            0.16579492390155792,\n",
      "            -0.07426824420690536,\n",
      "            0.018391450867056847,\n",
      "            0.22794215381145477,\n",
      "            -0.10569554567337036,\n",
      "            0.010450874455273151,\n",
      "            -0.1509195864200592,\n",
      "            0.08718334138393402,\n",
      "            -0.08289626985788345,\n",
      "            -0.09229865670204163,\n",
      "            -0.1347070187330246,\n",
      "            -0.09257465600967407,\n",
      "            -0.14103896915912628,\n",
      "            -0.1205005943775177,\n",
      "            0.018708810210227966,\n",
      "            0.046090446412563324,\n",
      "            0.13800550997257233,\n",
      "            0.24624210596084595,\n",
      "            0.10060185939073563,\n",
      "            -0.08221255242824554,\n",
      "            0.06979057937860489,\n",
      "            -0.11958248168230057,\n",
      "            0.13912002742290497,\n",
      "            0.0864868089556694,\n",
      "            -0.10143700987100601,\n",
      "            0.06871996074914932,\n",
      "            -0.04586710035800934,\n",
      "            -0.14090102910995483,\n",
      "            0.0006856918334960938,\n",
      "            0.10081866383552551,\n",
      "            0.022842131555080414,\n",
      "            -0.1561940312385559,\n",
      "            0.17383429408073425,\n",
      "            0.09235765784978867,\n",
      "            0.16191183030605316,\n",
      "            -0.0897129699587822,\n",
      "            0.14343012869358063,\n",
      "            0.1341649442911148,\n",
      "            -0.028948919847607613,\n",
      "            0.18505869805812836,\n",
      "            -0.053762827068567276,\n",
      "            -0.07506701350212097,\n",
      "            0.07408449053764343,\n",
      "            -0.04122721403837204,\n",
      "            0.14138299226760864,\n",
      "            0.0037969218101352453,\n",
      "            -0.14579962193965912,\n",
      "            0.06624041497707367,\n",
      "            0.06372548639774323,\n",
      "            0.0886981263756752,\n",
      "            0.16865301132202148\n",
      "          ],\n",
      "          [\n",
      "            0.0876256600022316,\n",
      "            0.11265284568071365,\n",
      "            -0.02793831191956997,\n",
      "            -0.0734495222568512,\n",
      "            -0.10920926928520203,\n",
      "            0.04298488423228264,\n",
      "            -0.06263388693332672,\n",
      "            -0.07042846828699112,\n",
      "            0.03734181821346283,\n",
      "            0.05975458770990372,\n",
      "            0.03376321494579315,\n",
      "            -0.11776827275753021,\n",
      "            -0.09561829268932343,\n",
      "            0.0557265542447567,\n",
      "            -0.037831421941518784,\n",
      "            -0.019681736826896667,\n",
      "            -0.05332257226109505,\n",
      "            -0.15829117596149445,\n",
      "            -0.07829006761312485,\n",
      "            -0.08684267103672028,\n",
      "            0.051199138164520264,\n",
      "            -0.00509451562538743,\n",
      "            0.0030192898120731115,\n",
      "            -0.046190060675144196,\n",
      "            0.07898703217506409,\n",
      "            -0.00030296773184090853,\n",
      "            0.09925570338964462,\n",
      "            0.1322663277387619,\n",
      "            0.02993677370250225,\n",
      "            -0.00308739161118865,\n",
      "            0.07259007543325424,\n",
      "            0.1730809062719345,\n",
      "            0.16982591152191162,\n",
      "            0.08373912423849106,\n",
      "            0.03530421853065491,\n",
      "            0.029282579198479652,\n",
      "            0.07946977764368057,\n",
      "            -0.07647468894720078,\n",
      "            0.17723698914051056,\n",
      "            0.004569415934383869,\n",
      "            0.24075379967689514,\n",
      "            -0.051599856466054916,\n",
      "            0.03968622162938118,\n",
      "            -0.12204593420028687,\n",
      "            0.17574454843997955,\n",
      "            0.09063361585140228,\n",
      "            -0.024386784061789513,\n",
      "            -0.0644407644867897,\n",
      "            0.019496150314807892,\n",
      "            0.10904894024133682,\n",
      "            0.187037855386734,\n",
      "            0.14484664797782898,\n",
      "            0.16504546999931335,\n",
      "            -0.2009667456150055,\n",
      "            0.07969213277101517,\n",
      "            0.08273173868656158,\n",
      "            0.14322757720947266,\n",
      "            -0.020934851840138435,\n",
      "            0.10791348665952682,\n",
      "            0.13873858749866486,\n",
      "            -0.031282976269721985,\n",
      "            -0.060139741748571396,\n",
      "            -0.07981828600168228,\n",
      "            -0.023940470069646835,\n",
      "            -0.13420698046684265,\n",
      "            -0.07963645458221436,\n",
      "            0.15681786835193634,\n",
      "            0.18248052895069122,\n",
      "            -0.12066061049699783,\n",
      "            0.08247438073158264,\n",
      "            0.16036386787891388,\n",
      "            -0.03139292448759079,\n",
      "            0.1470605432987213,\n",
      "            0.17040766775608063,\n",
      "            -0.0325930193066597,\n",
      "            -0.09985789656639099,\n",
      "            -0.15158270299434662,\n",
      "            0.09743255376815796,\n",
      "            0.11372562497854233,\n",
      "            -0.08103535324335098,\n",
      "            0.19382868707180023,\n",
      "            0.13250420987606049,\n",
      "            -0.13091380894184113,\n",
      "            0.06061483919620514,\n",
      "            0.038511376827955246,\n",
      "            0.14935557544231415,\n",
      "            -0.020403211936354637,\n",
      "            -0.10715803503990173,\n",
      "            0.24152836203575134,\n",
      "            0.06872929632663727,\n",
      "            -0.1692657172679901,\n",
      "            0.21135392785072327,\n",
      "            -0.0028595002368092537,\n",
      "            0.03166733309626579,\n",
      "            0.07314746826887131,\n",
      "            0.10277067124843597,\n",
      "            0.004947488196194172,\n",
      "            0.017763547599315643,\n",
      "            0.04743301868438721,\n",
      "            0.11651664227247238,\n",
      "            0.06311272829771042,\n",
      "            -0.055304985493421555,\n",
      "            0.08445747941732407,\n",
      "            0.01331068854779005,\n",
      "            -0.12612088024616241,\n",
      "            -0.16445620357990265,\n",
      "            -0.1541709303855896,\n",
      "            0.15630251169204712,\n",
      "            -0.05662653595209122,\n",
      "            -0.09505848586559296,\n",
      "            -0.13340674340724945,\n",
      "            -0.02416161634027958,\n",
      "            0.03577558696269989,\n",
      "            0.0012175040319561958,\n",
      "            -0.013098719529807568,\n",
      "            -0.07927129417657852,\n",
      "            -0.03077886812388897,\n",
      "            -0.10914357751607895,\n",
      "            0.17889365553855896,\n",
      "            0.10586008429527283,\n",
      "            0.16849106550216675,\n",
      "            -0.11567196249961853,\n",
      "            -0.11723575741052628,\n",
      "            -0.12588801980018616,\n",
      "            -0.11519737541675568,\n",
      "            -0.03549612686038017,\n",
      "            0.14934895932674408,\n",
      "            -0.17134800553321838\n",
      "          ],\n",
      "          [\n",
      "            -0.10598777234554291,\n",
      "            -0.11158497631549835,\n",
      "            -0.17379437386989594,\n",
      "            0.1305505782365799,\n",
      "            -0.15296515822410583,\n",
      "            -0.18814781308174133,\n",
      "            0.14624232053756714,\n",
      "            0.11915776133537292,\n",
      "            0.17860889434814453,\n",
      "            0.05514492839574814,\n",
      "            0.1715194582939148,\n",
      "            0.10370781272649765,\n",
      "            -0.19868144392967224,\n",
      "            -0.14617876708507538,\n",
      "            -0.1713494509458542,\n",
      "            -0.002116367220878601,\n",
      "            0.1496971845626831,\n",
      "            -0.009268166497349739,\n",
      "            -0.13641174137592316,\n",
      "            -0.2064553201198578,\n",
      "            -0.06889982521533966,\n",
      "            0.17600801587104797,\n",
      "            -0.03196582570672035,\n",
      "            0.14841850101947784,\n",
      "            -0.1206946074962616,\n",
      "            0.0628272145986557,\n",
      "            -0.151589497923851,\n",
      "            -0.09809926897287369,\n",
      "            0.14926350116729736,\n",
      "            -0.02746441774070263,\n",
      "            -0.054446909576654434,\n",
      "            -0.20292086899280548,\n",
      "            -0.17591769993305206,\n",
      "            0.13095712661743164,\n",
      "            0.10691358149051666,\n",
      "            0.10151305794715881,\n",
      "            0.14622245728969574,\n",
      "            -0.10199496895074844,\n",
      "            -0.18129459023475647,\n",
      "            0.018675396218895912,\n",
      "            -0.16814613342285156,\n",
      "            -0.008612362667918205,\n",
      "            0.12295187264680862,\n",
      "            -0.11079361289739609,\n",
      "            -0.0892651304602623,\n",
      "            0.1427372246980667,\n",
      "            0.28654396533966064,\n",
      "            0.17201517522335052,\n",
      "            0.15751266479492188,\n",
      "            -0.0927896723151207,\n",
      "            -0.15783005952835083,\n",
      "            -0.0614459291100502,\n",
      "            -0.05967428535223007,\n",
      "            0.18313564360141754,\n",
      "            -0.054435182362794876,\n",
      "            -0.12071023136377335,\n",
      "            -0.08245386928319931,\n",
      "            0.10427980124950409,\n",
      "            0.02774781547486782,\n",
      "            0.12604957818984985,\n",
      "            -0.1342759132385254,\n",
      "            0.1078215166926384,\n",
      "            0.13957437872886658,\n",
      "            0.14130893349647522,\n",
      "            0.20764224231243134,\n",
      "            -0.06151953712105751,\n",
      "            -0.14906413853168488,\n",
      "            -0.22812078893184662,\n",
      "            0.16912667453289032,\n",
      "            0.06236374378204346,\n",
      "            -0.1929149627685547,\n",
      "            0.15035641193389893,\n",
      "            -0.06946144253015518,\n",
      "            -0.14144685864448547,\n",
      "            0.016414545476436615,\n",
      "            -0.18437233567237854,\n",
      "            -0.127624049782753,\n",
      "            -0.10156844556331635,\n",
      "            -0.11988259106874466,\n",
      "            -0.07429513335227966,\n",
      "            -0.17018632590770721,\n",
      "            -0.11248140037059784,\n",
      "            -0.06063023582100868,\n",
      "            0.03290589153766632,\n",
      "            -0.16078704595565796,\n",
      "            -0.20349347591400146,\n",
      "            -0.11964159458875656,\n",
      "            -0.06930384784936905,\n",
      "            -0.2319878190755844,\n",
      "            0.1478234976530075,\n",
      "            -0.05485249310731888,\n",
      "            -0.0725402981042862,\n",
      "            0.02417656220495701,\n",
      "            -0.149924173951149,\n",
      "            -0.15623344480991364,\n",
      "            -0.05651906132698059,\n",
      "            0.15606144070625305,\n",
      "            -0.12627722322940826,\n",
      "            -0.017472125589847565,\n",
      "            -0.06042461842298508,\n",
      "            0.149958997964859,\n",
      "            -0.05422459542751312,\n",
      "            0.07551232725381851,\n",
      "            -0.04500039294362068,\n",
      "            0.10506945103406906,\n",
      "            0.10518644750118256,\n",
      "            -0.011692718602716923,\n",
      "            0.027264345437288284,\n",
      "            0.01397593691945076,\n",
      "            -0.11794697493314743,\n",
      "            0.15382720530033112,\n",
      "            0.14098256826400757,\n",
      "            0.12508919835090637,\n",
      "            -0.03209901973605156,\n",
      "            0.06646495312452316,\n",
      "            0.07423534989356995,\n",
      "            0.10278236120939255,\n",
      "            0.0803794413805008,\n",
      "            0.05519041419029236,\n",
      "            0.030412815511226654,\n",
      "            -0.04551924020051956,\n",
      "            0.17123666405677795,\n",
      "            -0.13094618916511536,\n",
      "            -0.12641964852809906,\n",
      "            0.060586679726839066,\n",
      "            0.15016557276248932,\n",
      "            -0.10760526359081268,\n",
      "            0.08453476428985596\n",
      "          ],\n",
      "          [\n",
      "            -0.06579120457172394,\n",
      "            0.0017446614801883698,\n",
      "            -0.056792762130498886,\n",
      "            0.05067376419901848,\n",
      "            0.05226854607462883,\n",
      "            -0.1289634257555008,\n",
      "            0.13441547751426697,\n",
      "            -0.06362331658601761,\n",
      "            0.13677726686000824,\n",
      "            0.0007236255914904177,\n",
      "            0.11622768640518188,\n",
      "            0.10373511910438538,\n",
      "            0.07350732386112213,\n",
      "            0.06686536222696304,\n",
      "            0.0208949763327837,\n",
      "            -0.05482419580221176,\n",
      "            -0.07027572393417358,\n",
      "            -0.13839377462863922,\n",
      "            -0.008309798315167427,\n",
      "            0.05219752714037895,\n",
      "            0.11434470117092133,\n",
      "            0.02890363521873951,\n",
      "            0.1163199245929718,\n",
      "            0.10981713980436325,\n",
      "            0.0659598857164383,\n",
      "            -0.06026889383792877,\n",
      "            -0.16978079080581665,\n",
      "            0.10739333182573318,\n",
      "            0.12843461334705353,\n",
      "            -0.09396880120038986,\n",
      "            0.01685444451868534,\n",
      "            -0.021966498345136642,\n",
      "            -0.07899435609579086,\n",
      "            -0.014638475142419338,\n",
      "            -0.06928646564483643,\n",
      "            -0.11781267821788788,\n",
      "            -0.020484574139118195,\n",
      "            -0.0008731987327337265,\n",
      "            -0.13698667287826538,\n",
      "            0.1269191950559616,\n",
      "            0.1640147566795349,\n",
      "            -0.10726524144411087,\n",
      "            -0.058041248470544815,\n",
      "            0.15504777431488037,\n",
      "            -0.02680187113583088,\n",
      "            0.06271325796842575,\n",
      "            0.10750614106655121,\n",
      "            -0.11533188819885254,\n",
      "            -0.007670512888580561,\n",
      "            -0.09760506451129913,\n",
      "            0.000697860901709646,\n",
      "            -0.00087224185699597,\n",
      "            0.16731564700603485,\n",
      "            -0.13441458344459534,\n",
      "            -0.05465627461671829,\n",
      "            -0.17100907862186432,\n",
      "            0.1518464833498001,\n",
      "            -0.11933086812496185,\n",
      "            -0.01932237111032009,\n",
      "            0.08163167536258698,\n",
      "            -0.05574532970786095,\n",
      "            -0.04011450707912445,\n",
      "            0.0220999326556921,\n",
      "            0.0767776221036911,\n",
      "            0.013523786328732967,\n",
      "            -0.05641971156001091,\n",
      "            -0.16666051745414734,\n",
      "            -0.1408558487892151,\n",
      "            -0.011893375776708126,\n",
      "            0.042666759341955185,\n",
      "            0.03776620700955391,\n",
      "            0.04349485784769058,\n",
      "            -0.03592150658369064,\n",
      "            0.11959629505872726,\n",
      "            -0.0427449531853199,\n",
      "            -0.035053569823503494,\n",
      "            -0.032172128558158875,\n",
      "            0.03998427838087082,\n",
      "            -0.015624584630131721,\n",
      "            0.048104241490364075,\n",
      "            -0.037081245332956314,\n",
      "            0.056590184569358826,\n",
      "            -0.012850266881287098,\n",
      "            0.046156760305166245,\n",
      "            -0.06057161092758179,\n",
      "            0.12566687166690826,\n",
      "            -0.06278076767921448,\n",
      "            -0.019858617335557938,\n",
      "            0.07920791953802109,\n",
      "            -0.0753287822008133,\n",
      "            -0.0872015729546547,\n",
      "            0.13240648806095123,\n",
      "            0.0542234443128109,\n",
      "            -0.008103377185761929,\n",
      "            0.031554725021123886,\n",
      "            0.13926593959331512,\n",
      "            -0.019782986491918564,\n",
      "            0.05818292498588562,\n",
      "            -0.03641314432024956,\n",
      "            -0.01757078990340233,\n",
      "            -0.002228345023468137,\n",
      "            -0.04904140904545784,\n",
      "            -0.009357745759189129,\n",
      "            0.025461366400122643,\n",
      "            -0.15113264322280884,\n",
      "            0.1719387322664261,\n",
      "            0.09801515191793442,\n",
      "            -0.12437066435813904,\n",
      "            0.03946765512228012,\n",
      "            -0.08949357271194458,\n",
      "            0.1658327579498291,\n",
      "            -0.09367319196462631,\n",
      "            0.030033053830266,\n",
      "            -0.12491293251514435,\n",
      "            0.048510678112506866,\n",
      "            0.000705969228874892,\n",
      "            0.12175841629505157,\n",
      "            0.12669849395751953,\n",
      "            0.0866202861070633,\n",
      "            0.14535202085971832,\n",
      "            0.025383001193404198,\n",
      "            0.11437032371759415,\n",
      "            -0.1265539675951004,\n",
      "            -0.06314753741025925,\n",
      "            -0.1727123260498047,\n",
      "            0.08067740499973297,\n",
      "            0.0929267555475235,\n",
      "            -0.15325336158275604\n",
      "          ],\n",
      "          [\n",
      "            -0.13152484595775604,\n",
      "            -0.13298378884792328,\n",
      "            0.05916011333465576,\n",
      "            -0.03725028783082962,\n",
      "            -0.16731704771518707,\n",
      "            -0.14265678822994232,\n",
      "            0.15299582481384277,\n",
      "            0.00369276013225317,\n",
      "            0.1534235179424286,\n",
      "            -0.01162367220968008,\n",
      "            -0.01734970696270466,\n",
      "            -0.02005936950445175,\n",
      "            0.03631189838051796,\n",
      "            0.14299629628658295,\n",
      "            0.0360528863966465,\n",
      "            0.07746393978595734,\n",
      "            -0.09653910249471664,\n",
      "            -0.03650816157460213,\n",
      "            -0.1310955286026001,\n",
      "            -0.010093302465975285,\n",
      "            -0.03630592301487923,\n",
      "            -0.17183421552181244,\n",
      "            0.019398629665374756,\n",
      "            -0.043824538588523865,\n",
      "            -0.17510858178138733,\n",
      "            -0.047627635300159454,\n",
      "            -0.06781412661075592,\n",
      "            0.013765046373009682,\n",
      "            0.060466982424259186,\n",
      "            -0.16665488481521606,\n",
      "            -0.07632323354482651,\n",
      "            -0.026702821254730225,\n",
      "            0.12127779424190521,\n",
      "            -0.0010515338508412242,\n",
      "            -0.014539897441864014,\n",
      "            -0.05229080468416214,\n",
      "            -0.12867392599582672,\n",
      "            0.08762012422084808,\n",
      "            0.1406695544719696,\n",
      "            -0.13371659815311432,\n",
      "            0.03357979655265808,\n",
      "            -0.14905163645744324,\n",
      "            -0.14134801924228668,\n",
      "            -0.025851940736174583,\n",
      "            -0.08156806230545044,\n",
      "            0.08058997243642807,\n",
      "            0.0189041905105114,\n",
      "            0.07376793026924133,\n",
      "            0.020401477813720703,\n",
      "            0.12739993631839752,\n",
      "            -0.04328037053346634,\n",
      "            0.1676955223083496,\n",
      "            0.0966934934258461,\n",
      "            0.10588379204273224,\n",
      "            0.09209985285997391,\n",
      "            -0.16831238567829132,\n",
      "            -0.11939232796430588,\n",
      "            0.021773168817162514,\n",
      "            -0.020401444286108017,\n",
      "            -0.15371981263160706,\n",
      "            -0.03324270248413086,\n",
      "            -0.10705209523439407,\n",
      "            -0.09254171699285507,\n",
      "            -0.06999338418245316,\n",
      "            0.1412971168756485,\n",
      "            0.09799554198980331,\n",
      "            -0.012469752691686153,\n",
      "            0.17115837335586548,\n",
      "            0.13727326691150665,\n",
      "            0.17074181139469147,\n",
      "            -0.011715656146407127,\n",
      "            0.018928227946162224,\n",
      "            0.06376233696937561,\n",
      "            0.01793057657778263,\n",
      "            -0.10666996240615845,\n",
      "            -0.13057482242584229,\n",
      "            0.08680602163076401,\n",
      "            -0.10792327672243118,\n",
      "            -0.14224179089069366,\n",
      "            0.08756699413061142,\n",
      "            -0.11258730292320251,\n",
      "            -0.09935719519853592,\n",
      "            -0.12164487689733505,\n",
      "            -0.1749383509159088,\n",
      "            -0.12681394815444946,\n",
      "            -0.09721731394529343,\n",
      "            -0.10035093128681183,\n",
      "            -0.07094264030456543,\n",
      "            0.10249466449022293,\n",
      "            -0.129725381731987,\n",
      "            -0.12722305953502655,\n",
      "            -0.006523522082716227,\n",
      "            -0.16158276796340942,\n",
      "            0.1633441150188446,\n",
      "            -0.10247267037630081,\n",
      "            -0.12806318700313568,\n",
      "            0.021591277793049812,\n",
      "            -0.1620461642742157,\n",
      "            0.004356814548373222,\n",
      "            0.02502424642443657,\n",
      "            -0.1761278510093689,\n",
      "            -0.15952381491661072,\n",
      "            0.06806962937116623,\n",
      "            0.14192113280296326,\n",
      "            -0.0972149670124054,\n",
      "            0.06910417973995209,\n",
      "            0.1652030646800995,\n",
      "            -0.13917101919651031,\n",
      "            -0.11366089433431625,\n",
      "            0.04619712382555008,\n",
      "            0.0914827212691307,\n",
      "            0.06141238287091255,\n",
      "            -0.08580399304628372,\n",
      "            0.12238527834415436,\n",
      "            0.14966678619384766,\n",
      "            0.1353851556777954,\n",
      "            0.05815085768699646,\n",
      "            0.017120525240898132,\n",
      "            -0.0965404286980629,\n",
      "            0.00983097217977047,\n",
      "            -0.062003783881664276,\n",
      "            -0.11996640264987946,\n",
      "            -0.09668023139238358,\n",
      "            -0.16895200312137604,\n",
      "            0.08498081564903259,\n",
      "            -0.08486973494291306,\n",
      "            0.14299212396144867,\n",
      "            0.0919218584895134\n",
      "          ],\n",
      "          [\n",
      "            0.08349189162254333,\n",
      "            0.13818280398845673,\n",
      "            0.10494081676006317,\n",
      "            0.007920942269265652,\n",
      "            0.04360969364643097,\n",
      "            -0.00940899457782507,\n",
      "            -0.025791581720113754,\n",
      "            -0.15154854953289032,\n",
      "            -0.1846119612455368,\n",
      "            0.052067700773477554,\n",
      "            0.02409704215824604,\n",
      "            -0.166809543967247,\n",
      "            0.06720221042633057,\n",
      "            0.11971509456634521,\n",
      "            -0.14564363658428192,\n",
      "            0.0075066834688186646,\n",
      "            0.05562220886349678,\n",
      "            0.006355433724820614,\n",
      "            -0.10157938301563263,\n",
      "            0.004473888780921698,\n",
      "            0.06252488493919373,\n",
      "            -0.017998667433857918,\n",
      "            -0.05323026329278946,\n",
      "            -0.11951042711734772,\n",
      "            0.05050495266914368,\n",
      "            0.26476895809173584,\n",
      "            0.012256584130227566,\n",
      "            0.031060639768838882,\n",
      "            0.0013569241855293512,\n",
      "            -0.051215194165706635,\n",
      "            0.08282334357500076,\n",
      "            0.08434893935918808,\n",
      "            -0.011223171837627888,\n",
      "            0.07008600980043411,\n",
      "            -0.07178181409835815,\n",
      "            0.2316918522119522,\n",
      "            0.10179976373910904,\n",
      "            -0.061906326562166214,\n",
      "            0.007589050102978945,\n",
      "            0.22131510078907013,\n",
      "            0.09989326447248459,\n",
      "            -0.08136850595474243,\n",
      "            -0.09411510080099106,\n",
      "            -0.0379195436835289,\n",
      "            -0.05493062734603882,\n",
      "            0.06344538927078247,\n",
      "            0.011096942238509655,\n",
      "            -0.18998880684375763,\n",
      "            0.00618232786655426,\n",
      "            0.014968582428991795,\n",
      "            0.07250063121318817,\n",
      "            0.03556285426020622,\n",
      "            0.1787807047367096,\n",
      "            -0.1961156576871872,\n",
      "            0.2151985764503479,\n",
      "            -0.005942273885011673,\n",
      "            0.007451977580785751,\n",
      "            0.08749137818813324,\n",
      "            -0.17382977902889252,\n",
      "            0.1532714068889618,\n",
      "            0.06795200705528259,\n",
      "            0.04658119007945061,\n",
      "            -0.2556396722793579,\n",
      "            0.039433907717466354,\n",
      "            0.059111058712005615,\n",
      "            -0.15767021477222443,\n",
      "            -0.14324887096881866,\n",
      "            0.17074839770793915,\n",
      "            -0.09590259194374084,\n",
      "            0.15722784399986267,\n",
      "            0.14498606324195862,\n",
      "            0.06544418632984161,\n",
      "            0.08866311609745026,\n",
      "            0.09526225179433823,\n",
      "            -0.05442749708890915,\n",
      "            0.0807839035987854,\n",
      "            0.13605035841464996,\n",
      "            -0.09657257795333862,\n",
      "            0.1737198382616043,\n",
      "            0.045942965894937515,\n",
      "            0.05209116265177727,\n",
      "            -0.11868806928396225,\n",
      "            0.05688982084393501,\n",
      "            0.08812166750431061,\n",
      "            -0.09839211404323578,\n",
      "            0.022657284513115883,\n",
      "            -0.07019932568073273,\n",
      "            -0.008717571385204792,\n",
      "            0.07875172793865204,\n",
      "            0.03720436245203018,\n",
      "            -0.08317596465349197,\n",
      "            0.1577576994895935,\n",
      "            0.012532983906567097,\n",
      "            0.14907416701316833,\n",
      "            0.16166509687900543,\n",
      "            0.1845148205757141,\n",
      "            -0.1644139438867569,\n",
      "            0.17284823954105377,\n",
      "            -0.0665259137749672,\n",
      "            -0.11877715587615967,\n",
      "            0.11389648914337158,\n",
      "            0.003068390768021345,\n",
      "            -0.08844669163227081,\n",
      "            -0.13785800337791443,\n",
      "            -0.06848912686109543,\n",
      "            0.14262409508228302,\n",
      "            -0.22194264829158783,\n",
      "            -0.01642533950507641,\n",
      "            -0.16672196984291077,\n",
      "            0.10326886922121048,\n",
      "            -0.14154917001724243,\n",
      "            0.09992334246635437,\n",
      "            0.04711161181330681,\n",
      "            0.0842142328619957,\n",
      "            -0.1306692659854889,\n",
      "            0.20004206895828247,\n",
      "            0.18851464986801147,\n",
      "            -0.10586478561162949,\n",
      "            0.04983165115118027,\n",
      "            -0.16421270370483398,\n",
      "            -0.12925215065479279,\n",
      "            0.13785560429096222,\n",
      "            0.06205432116985321,\n",
      "            0.09468884766101837,\n",
      "            -0.12871642410755157,\n",
      "            0.051573194563388824,\n",
      "            -0.04038628563284874,\n",
      "            0.055812206119298935\n",
      "          ],\n",
      "          [\n",
      "            0.1359153687953949,\n",
      "            -0.03194735199213028,\n",
      "            -0.11707238852977753,\n",
      "            0.17260129749774933,\n",
      "            0.14245398342609406,\n",
      "            -0.1532888114452362,\n",
      "            0.14819537103176117,\n",
      "            0.1539151668548584,\n",
      "            0.006182434502989054,\n",
      "            -0.027564553543925285,\n",
      "            0.06005343794822693,\n",
      "            -0.025479121133685112,\n",
      "            -0.045703258365392685,\n",
      "            0.09345657378435135,\n",
      "            -0.13767042756080627,\n",
      "            0.06382313370704651,\n",
      "            -0.031372684985399246,\n",
      "            -0.14718925952911377,\n",
      "            0.10566399246454239,\n",
      "            0.006583210546523333,\n",
      "            0.027292249724268913,\n",
      "            -0.11372272670269012,\n",
      "            0.11504615098237991,\n",
      "            0.04956359788775444,\n",
      "            -0.18893665075302124,\n",
      "            -0.06625067442655563,\n",
      "            -0.1411500871181488,\n",
      "            0.07037068158388138,\n",
      "            -0.020816965028643608,\n",
      "            -0.09630272537469864,\n",
      "            -0.1257457137107849,\n",
      "            0.12945644557476044,\n",
      "            0.13234415650367737,\n",
      "            0.11240033060312271,\n",
      "            0.15782351791858673,\n",
      "            0.006807534955441952,\n",
      "            0.08096195757389069,\n",
      "            -0.09181127697229385,\n",
      "            -0.06304632872343063,\n",
      "            0.0230970848351717,\n",
      "            -0.04714851826429367,\n",
      "            0.027005072683095932,\n",
      "            -0.13847652077674866,\n",
      "            0.07514876872301102,\n",
      "            -0.14189520478248596,\n",
      "            -0.075469471514225,\n",
      "            0.0010364878689870238,\n",
      "            -0.06439771503210068,\n",
      "            0.03382733464241028,\n",
      "            -0.010934199206531048,\n",
      "            -0.03265531733632088,\n",
      "            -0.17457419633865356,\n",
      "            0.005001028999686241,\n",
      "            -0.0993892103433609,\n",
      "            0.10236954689025879,\n",
      "            -0.009672340005636215,\n",
      "            0.0863088071346283,\n",
      "            0.13906645774841309,\n",
      "            0.012710126116871834,\n",
      "            0.03015298955142498,\n",
      "            0.1424652338027954,\n",
      "            -0.04823847860097885,\n",
      "            0.04846838489174843,\n",
      "            -0.025806723162531853,\n",
      "            -0.08774533867835999,\n",
      "            0.12403218448162079,\n",
      "            -0.10048569738864899,\n",
      "            -0.0026402275543659925,\n",
      "            0.08707770705223083,\n",
      "            -0.055310264229774475,\n",
      "            -0.04298066720366478,\n",
      "            -0.06849753111600876,\n",
      "            -0.11503292620182037,\n",
      "            0.05666256695985794,\n",
      "            0.030034761875867844,\n",
      "            0.14340591430664062,\n",
      "            -0.02850511111319065,\n",
      "            0.19731004536151886,\n",
      "            -0.01697472669184208,\n",
      "            -0.11906963586807251,\n",
      "            -0.0639670342206955,\n",
      "            -0.1290089637041092,\n",
      "            -0.166350319981575,\n",
      "            0.037438374012708664,\n",
      "            0.11413281410932541,\n",
      "            -0.11826357245445251,\n",
      "            -0.02976827509701252,\n",
      "            -0.10629133880138397,\n",
      "            -0.18999220430850983,\n",
      "            -0.1350346952676773,\n",
      "            -0.14956612884998322,\n",
      "            0.002954429015517235,\n",
      "            -0.08802711963653564,\n",
      "            -0.04091418907046318,\n",
      "            -0.04786444455385208,\n",
      "            -0.002212030813097954,\n",
      "            0.17697834968566895,\n",
      "            -0.0848717913031578,\n",
      "            -0.1803157776594162,\n",
      "            -0.18075166642665863,\n",
      "            0.1223725974559784,\n",
      "            0.09990335255861282,\n",
      "            0.005560123361647129,\n",
      "            -0.05555262789130211,\n",
      "            -0.06829331070184708,\n",
      "            -0.16866102814674377,\n",
      "            0.046449944376945496,\n",
      "            0.0883750468492508,\n",
      "            -0.13830500841140747,\n",
      "            -0.12810121476650238,\n",
      "            -0.15065303444862366,\n",
      "            0.1619948297739029,\n",
      "            0.20258022844791412,\n",
      "            0.11207105219364166,\n",
      "            0.05395764857530594,\n",
      "            -0.10250347852706909,\n",
      "            -0.16451726853847504,\n",
      "            -0.07546819746494293,\n",
      "            -0.18232648074626923,\n",
      "            0.12348760664463043,\n",
      "            0.08556920289993286,\n",
      "            0.027291344478726387,\n",
      "            -0.055114734917879105,\n",
      "            0.028641145676374435,\n",
      "            -0.09279163926839828,\n",
      "            0.02644394338130951,\n",
      "            0.10186341404914856,\n",
      "            0.061863917857408524\n",
      "          ],\n",
      "          [\n",
      "            0.17447681725025177,\n",
      "            -0.06188493221998215,\n",
      "            0.1736060231924057,\n",
      "            -0.03927530348300934,\n",
      "            0.13219408690929413,\n",
      "            -0.16566236317157745,\n",
      "            0.04393361508846283,\n",
      "            -0.13885009288787842,\n",
      "            -0.01876942813396454,\n",
      "            -0.06380084156990051,\n",
      "            0.04279603064060211,\n",
      "            0.1366284042596817,\n",
      "            -0.14719407260417938,\n",
      "            -0.1232512891292572,\n",
      "            -0.0024691373109817505,\n",
      "            -0.15821130573749542,\n",
      "            0.03483365476131439,\n",
      "            0.08152730762958527,\n",
      "            -0.05221530795097351,\n",
      "            -0.15292979776859283,\n",
      "            -0.17136988043785095,\n",
      "            0.02498355507850647,\n",
      "            0.054746776819229126,\n",
      "            0.08096902072429657,\n",
      "            0.09033291041851044,\n",
      "            -0.11773388832807541,\n",
      "            -0.011406540870666504,\n",
      "            -0.0014221221208572388,\n",
      "            -0.13699769973754883,\n",
      "            -0.13850799202919006,\n",
      "            -0.055677883327007294,\n",
      "            -0.04736003279685974,\n",
      "            -0.13714048266410828,\n",
      "            0.06761601567268372,\n",
      "            0.06310082972049713,\n",
      "            0.15012811124324799,\n",
      "            -0.021071314811706543,\n",
      "            0.1594730168581009,\n",
      "            0.10254167020320892,\n",
      "            -0.07811129838228226,\n",
      "            0.1394307166337967,\n",
      "            0.12090279161930084,\n",
      "            0.04988117516040802,\n",
      "            0.02425563335418701,\n",
      "            0.1254671961069107,\n",
      "            -0.11066139489412308,\n",
      "            -0.11548244953155518,\n",
      "            -0.13393816351890564,\n",
      "            0.024161770939826965,\n",
      "            0.007381007075309753,\n",
      "            0.039944469928741455,\n",
      "            -0.09150435030460358,\n",
      "            -0.08877934515476227,\n",
      "            0.09166483581066132,\n",
      "            -0.0012675672769546509,\n",
      "            0.07722713053226471,\n",
      "            0.10814674198627472,\n",
      "            0.027792304754257202,\n",
      "            -0.11386392265558243,\n",
      "            0.04988834261894226,\n",
      "            -0.056036002933979034,\n",
      "            0.14279769361019135,\n",
      "            -0.14347536861896515,\n",
      "            -0.07692710310220718,\n",
      "            0.030003786087036133,\n",
      "            -0.15474346280097961,\n",
      "            0.029202237725257874,\n",
      "            -0.006126090884208679,\n",
      "            0.14949224889278412,\n",
      "            -0.13210853934288025,\n",
      "            -0.028627946972846985,\n",
      "            -0.14553213119506836,\n",
      "            -0.14277568459510803,\n",
      "            -0.17552749812602997,\n",
      "            0.04400211572647095,\n",
      "            -0.10593213886022568,\n",
      "            -0.1526547372341156,\n",
      "            -0.02714581787586212,\n",
      "            -0.059806711971759796,\n",
      "            -0.05310743302106857,\n",
      "            0.1727515310049057,\n",
      "            0.16179756820201874,\n",
      "            0.06998977065086365,\n",
      "            -0.1708364337682724,\n",
      "            -0.16712422668933868,\n",
      "            0.17246641218662262,\n",
      "            0.030747011303901672,\n",
      "            0.08459602296352386,\n",
      "            -0.05242764204740524,\n",
      "            0.16051505506038666,\n",
      "            -0.16485272347927094,\n",
      "            0.029833808541297913,\n",
      "            -0.10576136410236359,\n",
      "            -0.031526386737823486,\n",
      "            0.12052051723003387,\n",
      "            -0.01929369568824768,\n",
      "            -0.041433677077293396,\n",
      "            0.0404876172542572,\n",
      "            -0.024882987141609192,\n",
      "            -0.11916730552911758,\n",
      "            0.004447713494300842,\n",
      "            0.13840089738368988,\n",
      "            0.018629878759384155,\n",
      "            0.16843558847904205,\n",
      "            -0.02425454556941986,\n",
      "            -0.08888698369264603,\n",
      "            -0.10691656172275543,\n",
      "            -0.07225276529788971,\n",
      "            -0.06824430078268051,\n",
      "            0.12380312383174896,\n",
      "            0.05767025053501129,\n",
      "            0.0468815416097641,\n",
      "            -0.1352759599685669,\n",
      "            -0.09696047008037567,\n",
      "            -0.1046135351061821,\n",
      "            -0.08210661262273788,\n",
      "            0.08876843750476837,\n",
      "            -0.16667591035366058,\n",
      "            -0.1487656831741333,\n",
      "            -0.09614302963018417,\n",
      "            -0.014729231595993042,\n",
      "            0.16128389537334442,\n",
      "            0.13723759353160858,\n",
      "            0.17663027346134186,\n",
      "            -0.08801610767841339,\n",
      "            0.10053785145282745,\n",
      "            0.0745132714509964,\n",
      "            -0.10356926172971725\n",
      "          ],\n",
      "          [\n",
      "            0.1764463484287262,\n",
      "            -0.1029697060585022,\n",
      "            -0.059216536581516266,\n",
      "            -0.05096805840730667,\n",
      "            0.01012234203517437,\n",
      "            0.15268614888191223,\n",
      "            -0.07940378785133362,\n",
      "            0.12228143960237503,\n",
      "            -0.11817625164985657,\n",
      "            -0.1358523666858673,\n",
      "            -0.14668670296669006,\n",
      "            0.11646869033575058,\n",
      "            0.0809239074587822,\n",
      "            -0.12708722054958344,\n",
      "            0.0041906023398041725,\n",
      "            -0.12213470041751862,\n",
      "            -0.056385643780231476,\n",
      "            0.05737977847456932,\n",
      "            -0.058921072632074356,\n",
      "            0.1442100554704666,\n",
      "            0.029561065137386322,\n",
      "            0.12181920558214188,\n",
      "            0.13504882156848907,\n",
      "            0.02835523895919323,\n",
      "            -0.13363605737686157,\n",
      "            0.06889094412326813,\n",
      "            -0.11601804196834564,\n",
      "            0.11289539933204651,\n",
      "            0.019131317734718323,\n",
      "            -0.02937743067741394,\n",
      "            0.10065317898988724,\n",
      "            0.07269510626792908,\n",
      "            -0.07253945618867874,\n",
      "            -0.055835824459791183,\n",
      "            -0.16972008347511292,\n",
      "            0.07115133851766586,\n",
      "            -0.11419929563999176,\n",
      "            0.15962538123130798,\n",
      "            0.09908215701580048,\n",
      "            0.07427304983139038,\n",
      "            0.03240317851305008,\n",
      "            -0.10536152869462967,\n",
      "            -0.056602053344249725,\n",
      "            0.18442361056804657,\n",
      "            -0.10076849907636642,\n",
      "            -0.009154404513537884,\n",
      "            -0.1571669578552246,\n",
      "            0.0958128347992897,\n",
      "            -0.05363602936267853,\n",
      "            0.04792042449116707,\n",
      "            0.040684234350919724,\n",
      "            -0.10132628679275513,\n",
      "            0.06152379512786865,\n",
      "            0.1286262422800064,\n",
      "            -0.1641177237033844,\n",
      "            -0.042223501950502396,\n",
      "            -0.15645776689052582,\n",
      "            0.06817779690027237,\n",
      "            -0.17299461364746094,\n",
      "            0.08090442419052124,\n",
      "            0.0694105252623558,\n",
      "            0.153048574924469,\n",
      "            -0.10126063227653503,\n",
      "            -0.07514115422964096,\n",
      "            0.09393742680549622,\n",
      "            0.08757133781909943,\n",
      "            -0.04334108904004097,\n",
      "            0.07808573544025421,\n",
      "            0.07860876619815826,\n",
      "            -0.16679343581199646,\n",
      "            0.06544441729784012,\n",
      "            -0.1258106678724289,\n",
      "            0.1399630904197693,\n",
      "            -0.17379963397979736,\n",
      "            0.11718960851430893,\n",
      "            0.08370513468980789,\n",
      "            0.0054193888790905476,\n",
      "            0.09541162103414536,\n",
      "            0.01451785210520029,\n",
      "            -0.0615408793091774,\n",
      "            -0.14904233813285828,\n",
      "            -0.1068301647901535,\n",
      "            0.1787872016429901,\n",
      "            -0.096804179251194,\n",
      "            -0.1401483714580536,\n",
      "            -0.024451661854982376,\n",
      "            0.014301970601081848,\n",
      "            0.18167774379253387,\n",
      "            0.1745489537715912,\n",
      "            0.021851832047104836,\n",
      "            0.11398683488368988,\n",
      "            -0.11730772256851196,\n",
      "            -0.01289342064410448,\n",
      "            -0.10680732131004333,\n",
      "            0.006692384369671345,\n",
      "            0.1162862703204155,\n",
      "            -0.03427685797214508,\n",
      "            -0.021812986582517624,\n",
      "            -0.032162830233573914,\n",
      "            -0.17642173171043396,\n",
      "            0.13510626554489136,\n",
      "            -0.04766317084431648,\n",
      "            0.14305849373340607,\n",
      "            0.06014365330338478,\n",
      "            -0.09011442959308624,\n",
      "            -0.15093502402305603,\n",
      "            -0.07938127219676971,\n",
      "            0.15442009270191193,\n",
      "            -0.013464930467307568,\n",
      "            0.047677766531705856,\n",
      "            -0.1668589562177658,\n",
      "            0.11988595128059387,\n",
      "            0.013076988980174065,\n",
      "            0.13339145481586456,\n",
      "            0.13586702942848206,\n",
      "            0.15363174676895142,\n",
      "            0.06827131658792496,\n",
      "            -0.06556161493062973,\n",
      "            -0.03616558760404587,\n",
      "            0.11979258805513382,\n",
      "            0.1449851095676422,\n",
      "            0.1446668803691864,\n",
      "            -0.17301376163959503,\n",
      "            -0.14730669558048248,\n",
      "            0.0376582145690918,\n",
      "            0.13812337815761566,\n",
      "            0.008717811666429043,\n",
      "            -0.08628617972135544\n",
      "          ],\n",
      "          [\n",
      "            -0.15721699595451355,\n",
      "            0.05887240543961525,\n",
      "            -0.15645883977413177,\n",
      "            -0.033727362751960754,\n",
      "            0.129774808883667,\n",
      "            0.14404742419719696,\n",
      "            -0.1325824111700058,\n",
      "            -0.1321454495191574,\n",
      "            -0.0008582003647461534,\n",
      "            -0.1628471463918686,\n",
      "            -0.16316145658493042,\n",
      "            0.054155293852090836,\n",
      "            0.05910705402493477,\n",
      "            -0.17032547295093536,\n",
      "            -0.03093922883272171,\n",
      "            0.14114569127559662,\n",
      "            -0.10350893437862396,\n",
      "            0.08559612929821014,\n",
      "            0.17321687936782837,\n",
      "            0.09658588469028473,\n",
      "            -0.14620935916900635,\n",
      "            0.16912996768951416,\n",
      "            -0.09432074427604675,\n",
      "            0.14903612434864044,\n",
      "            0.09034606069326401,\n",
      "            0.06957349181175232,\n",
      "            0.07127295434474945,\n",
      "            0.15069743990898132,\n",
      "            -0.049568820744752884,\n",
      "            -0.12972670793533325,\n",
      "            0.07408717274665833,\n",
      "            -0.13708005845546722,\n",
      "            0.05987676978111267,\n",
      "            0.16004113852977753,\n",
      "            -0.06831254065036774,\n",
      "            0.09155471622943878,\n",
      "            0.08623682707548141,\n",
      "            0.1178661435842514,\n",
      "            0.14867186546325684,\n",
      "            -0.024684704840183258,\n",
      "            0.003950741607695818,\n",
      "            0.07322729378938675,\n",
      "            0.0009784373687580228,\n",
      "            -0.02707883156836033,\n",
      "            0.04340808838605881,\n",
      "            0.11021719127893448,\n",
      "            0.1543286144733429,\n",
      "            0.10749738663434982,\n",
      "            -0.05542973428964615,\n",
      "            -0.059641070663928986,\n",
      "            0.16166147589683533,\n",
      "            0.14904004335403442,\n",
      "            -0.062461670488119125,\n",
      "            -0.1551014930009842,\n",
      "            -0.009398525580763817,\n",
      "            0.1645304262638092,\n",
      "            0.10427265614271164,\n",
      "            -0.16274933516979218,\n",
      "            -0.04044581204652786,\n",
      "            0.004492340609431267,\n",
      "            0.11998718976974487,\n",
      "            -0.015995409339666367,\n",
      "            -0.13161474466323853,\n",
      "            -0.055371981114149094,\n",
      "            0.02764204889535904,\n",
      "            0.13325388729572296,\n",
      "            0.08700741082429886,\n",
      "            0.1270686239004135,\n",
      "            0.07754547894001007,\n",
      "            -0.06526774168014526,\n",
      "            0.13842257857322693,\n",
      "            0.13804370164871216,\n",
      "            0.1329028457403183,\n",
      "            0.06535068899393082,\n",
      "            0.05408918112516403,\n",
      "            -0.027703609317541122,\n",
      "            -0.04015037417411804,\n",
      "            0.037739600986242294,\n",
      "            0.17228861153125763,\n",
      "            0.11197678744792938,\n",
      "            -0.006876936182379723,\n",
      "            -0.13002343475818634,\n",
      "            -0.10721736401319504,\n",
      "            0.1573236584663391,\n",
      "            -0.1587667614221573,\n",
      "            -0.05084649845957756,\n",
      "            0.14762763679027557,\n",
      "            -0.06647487729787827,\n",
      "            -0.16345521807670593,\n",
      "            0.07682234048843384,\n",
      "            -0.10871583223342896,\n",
      "            -0.14987115561962128,\n",
      "            0.16958826780319214,\n",
      "            -0.007838829420506954,\n",
      "            -0.0783628299832344,\n",
      "            -0.10844773799180984,\n",
      "            -0.11212693154811859,\n",
      "            -0.05269566550850868,\n",
      "            0.17163793742656708,\n",
      "            0.13524137437343597,\n",
      "            0.06701959669589996,\n",
      "            0.14928536117076874,\n",
      "            0.0412292405962944,\n",
      "            -0.1210986003279686,\n",
      "            -0.018827442079782486,\n",
      "            0.06301303207874298,\n",
      "            0.12008733302354813,\n",
      "            0.11829359829425812,\n",
      "            -0.061375416815280914,\n",
      "            0.11445101350545883,\n",
      "            0.07738662511110306,\n",
      "            -0.1358584612607956,\n",
      "            0.10531913489103317,\n",
      "            0.04992174729704857,\n",
      "            0.16281816363334656,\n",
      "            -0.1670018881559372,\n",
      "            0.007030233275145292,\n",
      "            0.09602300822734833,\n",
      "            -0.07213294506072998,\n",
      "            -0.007788544055074453,\n",
      "            0.034776076674461365,\n",
      "            -0.14518845081329346,\n",
      "            0.040043678134679794,\n",
      "            0.16055554151535034,\n",
      "            -0.003085887525230646,\n",
      "            0.08967261016368866,\n",
      "            -0.08802296221256256,\n",
      "            0.14808225631713867\n",
      "          ],\n",
      "          [\n",
      "            0.09612945467233658,\n",
      "            -0.02059232071042061,\n",
      "            -0.06096678972244263,\n",
      "            -0.11309783160686493,\n",
      "            0.001746294554322958,\n",
      "            -0.17558293044567108,\n",
      "            -0.04074431210756302,\n",
      "            0.12151997536420822,\n",
      "            -0.0432594008743763,\n",
      "            0.05203711986541748,\n",
      "            0.1755664348602295,\n",
      "            0.10779696702957153,\n",
      "            -0.11300662159919739,\n",
      "            0.1167115867137909,\n",
      "            -0.11508800089359283,\n",
      "            -0.006260529160499573,\n",
      "            -0.017162321135401726,\n",
      "            0.15038344264030457,\n",
      "            -0.028650958091020584,\n",
      "            -0.11189677566289902,\n",
      "            0.0027732192538678646,\n",
      "            -0.026716675609350204,\n",
      "            0.0004966394044458866,\n",
      "            0.021457117050886154,\n",
      "            0.012736113741993904,\n",
      "            0.10279315710067749,\n",
      "            0.041471268981695175,\n",
      "            -0.03411288931965828,\n",
      "            -0.03735790029168129,\n",
      "            0.04577704519033432,\n",
      "            0.08636489510536194,\n",
      "            -0.14446625113487244,\n",
      "            0.006664277520030737,\n",
      "            0.1820693165063858,\n",
      "            0.0930808037519455,\n",
      "            0.01927553117275238,\n",
      "            -0.14891375601291656,\n",
      "            -0.1578669250011444,\n",
      "            -0.028002986684441566,\n",
      "            0.15296685695648193,\n",
      "            -0.10836563259363174,\n",
      "            -0.1386663019657135,\n",
      "            -0.15570232272148132,\n",
      "            -0.04709634929895401,\n",
      "            -0.17063015699386597,\n",
      "            -0.15016399323940277,\n",
      "            0.013187261298298836,\n",
      "            0.17154282331466675,\n",
      "            -0.10119681805372238,\n",
      "            0.09788306802511215,\n",
      "            0.015382884070277214,\n",
      "            -0.07165198028087616,\n",
      "            -0.07335258275270462,\n",
      "            0.029945133253932,\n",
      "            -0.055435337126255035,\n",
      "            -0.10128100961446762,\n",
      "            0.08032151311635971,\n",
      "            0.06997218728065491,\n",
      "            -0.06424718350172043,\n",
      "            -0.09177912771701813,\n",
      "            0.15969230234622955,\n",
      "            0.16565267741680145,\n",
      "            0.08866443485021591,\n",
      "            -0.04195426404476166,\n",
      "            0.07208417356014252,\n",
      "            -0.07817403972148895,\n",
      "            0.14386986196041107,\n",
      "            -0.0032790410332381725,\n",
      "            0.12967287003993988,\n",
      "            0.0591815784573555,\n",
      "            -0.006233057472854853,\n",
      "            0.048025600612163544,\n",
      "            -0.05745178088545799,\n",
      "            -0.20110036432743073,\n",
      "            -0.14881901443004608,\n",
      "            0.1279710829257965,\n",
      "            0.0825851708650589,\n",
      "            0.14521335065364838,\n",
      "            0.02486211434006691,\n",
      "            -0.2103162556886673,\n",
      "            -0.050349459052085876,\n",
      "            -0.1620267778635025,\n",
      "            -0.16288994252681732,\n",
      "            0.16796499490737915,\n",
      "            -0.035576317459344864,\n",
      "            -0.1307171881198883,\n",
      "            0.05074620246887207,\n",
      "            0.04336549714207649,\n",
      "            -0.042307619005441666,\n",
      "            -0.09448539465665817,\n",
      "            0.09257583320140839,\n",
      "            -0.09458006918430328,\n",
      "            -0.09417048841714859,\n",
      "            0.042510733008384705,\n",
      "            -0.09126542508602142,\n",
      "            0.1071392074227333,\n",
      "            0.05542462691664696,\n",
      "            0.06928221881389618,\n",
      "            -0.20119066536426544,\n",
      "            0.044973600655794144,\n",
      "            0.05876036360859871,\n",
      "            0.12625105679035187,\n",
      "            0.13117633759975433,\n",
      "            -0.01549525000154972,\n",
      "            0.1333073377609253,\n",
      "            -0.12272436916828156,\n",
      "            -0.16070809960365295,\n",
      "            0.11393649876117706,\n",
      "            -0.162451833486557,\n",
      "            -0.04302879795432091,\n",
      "            0.04913445562124252,\n",
      "            0.0518995001912117,\n",
      "            0.0940828025341034,\n",
      "            -0.020131215453147888,\n",
      "            0.1496163159608841,\n",
      "            0.11051120609045029,\n",
      "            0.06721339374780655,\n",
      "            0.03899853676557541,\n",
      "            -0.03815519064664841,\n",
      "            0.09207991510629654,\n",
      "            0.12623317539691925,\n",
      "            0.07014266401529312,\n",
      "            -0.18517239391803741,\n",
      "            0.06319453567266464,\n",
      "            0.03863460198044777,\n",
      "            -0.15845437347888947,\n",
      "            -0.16187693178653717,\n",
      "            -0.05740427225828171\n",
      "          ],\n",
      "          [\n",
      "            -0.07046268880367279,\n",
      "            0.1327604055404663,\n",
      "            -0.1684277355670929,\n",
      "            0.0068101221695542336,\n",
      "            0.002050303854048252,\n",
      "            -0.1306500881910324,\n",
      "            -0.16457858681678772,\n",
      "            -0.005572634283453226,\n",
      "            0.004126859828829765,\n",
      "            -0.07487238198518753,\n",
      "            0.1462233066558838,\n",
      "            0.009048683568835258,\n",
      "            0.022891076281666756,\n",
      "            -0.14551471173763275,\n",
      "            -0.1683349758386612,\n",
      "            -0.16664855182170868,\n",
      "            -0.1164555624127388,\n",
      "            -0.11096208542585373,\n",
      "            -0.10927049070596695,\n",
      "            -0.12703317403793335,\n",
      "            0.046368151903152466,\n",
      "            -0.11656861007213593,\n",
      "            0.1621050238609314,\n",
      "            0.08776292949914932,\n",
      "            0.0034341581631451845,\n",
      "            0.11164690554141998,\n",
      "            -0.14531870186328888,\n",
      "            -0.1735454648733139,\n",
      "            0.06163695827126503,\n",
      "            0.15025587379932404,\n",
      "            -0.11501487344503403,\n",
      "            -0.09737642109394073,\n",
      "            -0.0536290667951107,\n",
      "            -0.0857928916811943,\n",
      "            -0.13047248125076294,\n",
      "            0.12606172263622284,\n",
      "            -0.16367819905281067,\n",
      "            -0.1610407680273056,\n",
      "            -0.1594163179397583,\n",
      "            -0.16793473064899445,\n",
      "            -0.09764236956834793,\n",
      "            0.16080625355243683,\n",
      "            0.16357795894145966,\n",
      "            -0.016682572662830353,\n",
      "            0.018442906439304352,\n",
      "            0.10955879837274551,\n",
      "            -0.16246527433395386,\n",
      "            0.17437195777893066,\n",
      "            0.08245444297790527,\n",
      "            -0.0023454667534679174,\n",
      "            -0.10228525847196579,\n",
      "            -0.05329998582601547,\n",
      "            -0.14383651316165924,\n",
      "            0.1652936190366745,\n",
      "            -0.031644683331251144,\n",
      "            0.021282415837049484,\n",
      "            -0.016076255589723587,\n",
      "            -0.1679970920085907,\n",
      "            0.05171674117445946,\n",
      "            -0.08815094083547592,\n",
      "            0.010414390824735165,\n",
      "            0.04386381432414055,\n",
      "            0.05597807466983795,\n",
      "            -0.08304024487733841,\n",
      "            -0.07154754549264908,\n",
      "            0.03628784045577049,\n",
      "            -0.10909710079431534,\n",
      "            0.050494492053985596,\n",
      "            -0.09638427942991257,\n",
      "            -0.012887166813015938,\n",
      "            0.15120308101177216,\n",
      "            -0.05492686107754707,\n",
      "            0.04710640385746956,\n",
      "            0.10188312828540802,\n",
      "            0.1410510092973709,\n",
      "            0.1345023661851883,\n",
      "            -0.07060608267784119,\n",
      "            0.1111404076218605,\n",
      "            -0.033742379397153854,\n",
      "            0.14406746625900269,\n",
      "            0.11192964762449265,\n",
      "            0.15208221971988678,\n",
      "            -0.10370104014873505,\n",
      "            0.06800375133752823,\n",
      "            0.0753091350197792,\n",
      "            -0.11022134870290756,\n",
      "            0.03774666041135788,\n",
      "            0.034352291375398636,\n",
      "            0.1614275872707367,\n",
      "            0.11587189137935638,\n",
      "            0.09092163294553757,\n",
      "            0.16235469281673431,\n",
      "            0.08738894760608673,\n",
      "            0.056889090687036514,\n",
      "            0.003925619181245565,\n",
      "            0.023388106375932693,\n",
      "            0.005396853666752577,\n",
      "            -0.038180168718099594,\n",
      "            -0.13804975152015686,\n",
      "            -0.03886999934911728,\n",
      "            -0.15149305760860443,\n",
      "            0.13966670632362366,\n",
      "            0.05079608038067818,\n",
      "            0.1565520316362381,\n",
      "            -0.07733797281980515,\n",
      "            -0.1733214557170868,\n",
      "            -0.11930812150239944,\n",
      "            -0.08019781857728958,\n",
      "            0.1583561897277832,\n",
      "            0.04061370715498924,\n",
      "            -0.05492037534713745,\n",
      "            -0.17119421064853668,\n",
      "            0.14780454337596893,\n",
      "            -0.14768260717391968,\n",
      "            -0.05805931240320206,\n",
      "            -0.004665108397603035,\n",
      "            0.1079273447394371,\n",
      "            0.07889984548091888,\n",
      "            0.0201181098818779,\n",
      "            -0.15134012699127197,\n",
      "            -0.1354018896818161,\n",
      "            0.17268329858779907,\n",
      "            0.16230763494968414,\n",
      "            -0.007876637391746044,\n",
      "            -0.12065611034631729,\n",
      "            -0.020101958885788918,\n",
      "            -0.08478953689336777,\n",
      "            0.15356995165348053\n",
      "          ],\n",
      "          [\n",
      "            0.0006014787359163165,\n",
      "            0.1754104495048523,\n",
      "            0.1154051348567009,\n",
      "            -0.1598217934370041,\n",
      "            0.08853279799222946,\n",
      "            -0.12095482647418976,\n",
      "            -0.01113726943731308,\n",
      "            -0.08155651390552521,\n",
      "            0.10833945870399475,\n",
      "            0.06981527805328369,\n",
      "            -0.07103964686393738,\n",
      "            0.0042271665297448635,\n",
      "            -0.039620108902454376,\n",
      "            0.05432678759098053,\n",
      "            0.11722266674041748,\n",
      "            0.11739851534366608,\n",
      "            -0.030365901067852974,\n",
      "            -0.11936518549919128,\n",
      "            0.07214166224002838,\n",
      "            -0.11403951793909073,\n",
      "            0.11597362160682678,\n",
      "            -0.11920223385095596,\n",
      "            -0.15145137906074524,\n",
      "            -0.06898713856935501,\n",
      "            -0.08899246156215668,\n",
      "            0.15912161767482758,\n",
      "            -0.1669340580701828,\n",
      "            0.037142787128686905,\n",
      "            -0.0057434020563960075,\n",
      "            -0.11889197677373886,\n",
      "            0.105442114174366,\n",
      "            -0.07273460179567337,\n",
      "            -0.15110504627227783,\n",
      "            0.019906096160411835,\n",
      "            0.02606913447380066,\n",
      "            -0.12481076270341873,\n",
      "            -0.07701832056045532,\n",
      "            -0.038373950868844986,\n",
      "            0.1689712554216385,\n",
      "            0.16954092681407928,\n",
      "            -0.03750026226043701,\n",
      "            -0.15568163990974426,\n",
      "            -0.0015674802707508206,\n",
      "            -0.049062103033065796,\n",
      "            0.009269610047340393,\n",
      "            -0.14265882968902588,\n",
      "            -0.11949451267719269,\n",
      "            0.07380452752113342,\n",
      "            0.17388129234313965,\n",
      "            -0.11381705850362778,\n",
      "            -0.06412267684936523,\n",
      "            -0.02979869954288006,\n",
      "            0.03609185665845871,\n",
      "            0.10741616040468216,\n",
      "            -0.07382169365882874,\n",
      "            -0.05313992127776146,\n",
      "            0.07169035822153091,\n",
      "            -0.13994354009628296,\n",
      "            0.02586771547794342,\n",
      "            0.012587100267410278,\n",
      "            -0.08994109183549881,\n",
      "            0.03691422939300537,\n",
      "            0.09573254734277725,\n",
      "            0.07576548308134079,\n",
      "            0.03306068852543831,\n",
      "            -0.14654751121997833,\n",
      "            0.15665486454963684,\n",
      "            -0.13335026800632477,\n",
      "            -0.07775452733039856,\n",
      "            -0.05183824524283409,\n",
      "            -0.10678126662969589,\n",
      "            -0.13381971418857574,\n",
      "            -0.05990258604288101,\n",
      "            -0.11858527362346649,\n",
      "            -0.06629578024148941,\n",
      "            -0.09236233681440353,\n",
      "            -0.021844064816832542,\n",
      "            0.09464005380868912,\n",
      "            0.022371681407094002,\n",
      "            -0.060114432126283646,\n",
      "            0.17256389558315277,\n",
      "            -0.17192935943603516,\n",
      "            0.17466187477111816,\n",
      "            -0.1345728635787964,\n",
      "            -0.04413263872265816,\n",
      "            -0.15314985811710358,\n",
      "            0.006657405290752649,\n",
      "            -0.04228845611214638,\n",
      "            0.16955947875976562,\n",
      "            -0.17401838302612305,\n",
      "            0.04861588031053543,\n",
      "            0.028847908601164818,\n",
      "            -0.04631626978516579,\n",
      "            -0.13565587997436523,\n",
      "            0.02486630156636238,\n",
      "            -0.09355192631483078,\n",
      "            0.029095392674207687,\n",
      "            0.08107618987560272,\n",
      "            -0.014797081239521503,\n",
      "            0.043033868074417114,\n",
      "            -0.016754671931266785,\n",
      "            0.08111246675252914,\n",
      "            -0.05118376761674881,\n",
      "            -0.10087673366069794,\n",
      "            -0.1455356776714325,\n",
      "            -0.08630111068487167,\n",
      "            0.07057739794254303,\n",
      "            0.06527570635080338,\n",
      "            -0.02621009759604931,\n",
      "            0.10759413242340088,\n",
      "            0.1027774065732956,\n",
      "            -0.03882046043872833,\n",
      "            0.1338874101638794,\n",
      "            -0.11016536504030228,\n",
      "            0.17392730712890625,\n",
      "            -0.07563227415084839,\n",
      "            0.02882710099220276,\n",
      "            0.08853131532669067,\n",
      "            0.015725132077932358,\n",
      "            0.003238649806007743,\n",
      "            0.057091325521469116,\n",
      "            -0.06811585277318954,\n",
      "            -1.8873795852414332e-05,\n",
      "            -0.13890016078948975,\n",
      "            -0.15528124570846558,\n",
      "            0.08921106904745102,\n",
      "            -0.10388824343681335,\n",
      "            -0.07052288204431534\n",
      "          ],\n",
      "          [\n",
      "            0.1705341637134552,\n",
      "            0.16109082102775574,\n",
      "            -0.1367495357990265,\n",
      "            0.07673114538192749,\n",
      "            0.21045520901679993,\n",
      "            -0.0703192874789238,\n",
      "            0.06989345699548721,\n",
      "            -0.08895762264728546,\n",
      "            0.01639562100172043,\n",
      "            -0.10378418862819672,\n",
      "            -0.019686007872223854,\n",
      "            -0.15459740161895752,\n",
      "            0.14139145612716675,\n",
      "            -0.013080865144729614,\n",
      "            -0.1588047593832016,\n",
      "            -0.0896264910697937,\n",
      "            0.09601680934429169,\n",
      "            -0.08591655641794205,\n",
      "            -0.10029865801334381,\n",
      "            0.03607631474733353,\n",
      "            -0.0986931249499321,\n",
      "            -0.12521150708198547,\n",
      "            0.19229665398597717,\n",
      "            -0.05348607897758484,\n",
      "            0.06930175423622131,\n",
      "            0.00479126675054431,\n",
      "            -0.049101684242486954,\n",
      "            0.07768326997756958,\n",
      "            0.12703019380569458,\n",
      "            0.14923571050167084,\n",
      "            0.06589269638061523,\n",
      "            -0.14769437909126282,\n",
      "            -0.13123315572738647,\n",
      "            -0.1039794534444809,\n",
      "            0.12277625501155853,\n",
      "            -0.1404459923505783,\n",
      "            0.048802975565195084,\n",
      "            -0.13246245682239532,\n",
      "            0.12343122810125351,\n",
      "            -0.04212108254432678,\n",
      "            0.07153059542179108,\n",
      "            -0.06200259178876877,\n",
      "            -0.029073312878608704,\n",
      "            0.19372376799583435,\n",
      "            -0.17110924422740936,\n",
      "            -0.07720238715410233,\n",
      "            0.0642281249165535,\n",
      "            0.11141645163297653,\n",
      "            -0.05273841321468353,\n",
      "            0.05470922216773033,\n",
      "            0.14726661145687103,\n",
      "            -0.018820837140083313,\n",
      "            0.03482066094875336,\n",
      "            0.14732025563716888,\n",
      "            -0.046696264296770096,\n",
      "            0.20681068301200867,\n",
      "            -0.17519885301589966,\n",
      "            -0.13561928272247314,\n",
      "            0.10349529981613159,\n",
      "            -0.13524393737316132,\n",
      "            -0.11756253242492676,\n",
      "            0.07319024950265884,\n",
      "            0.14837823808193207,\n",
      "            -0.01064546499401331,\n",
      "            0.07427728176116943,\n",
      "            0.06219897046685219,\n",
      "            -0.12290459126234055,\n",
      "            0.002599845640361309,\n",
      "            0.10861919075250626,\n",
      "            -0.06845002621412277,\n",
      "            -0.11828231066465378,\n",
      "            -0.12306265532970428,\n",
      "            0.1599719077348709,\n",
      "            -0.16662146151065826,\n",
      "            0.0032530385069549084,\n",
      "            -0.14977456629276276,\n",
      "            0.1540067344903946,\n",
      "            0.10757013410329819,\n",
      "            -0.04376301169395447,\n",
      "            -0.1059546172618866,\n",
      "            0.09105636924505234,\n",
      "            0.05366949364542961,\n",
      "            -0.07149171829223633,\n",
      "            -0.06372694671154022,\n",
      "            -0.13090752065181732,\n",
      "            0.04125049710273743,\n",
      "            -0.02481716312468052,\n",
      "            0.08461786061525345,\n",
      "            -0.017914533615112305,\n",
      "            0.07772145420312881,\n",
      "            -0.14733412861824036,\n",
      "            -0.07011084258556366,\n",
      "            0.017187517136335373,\n",
      "            0.18331684172153473,\n",
      "            0.07218261063098907,\n",
      "            -0.16139252483844757,\n",
      "            0.08196527510881424,\n",
      "            -0.12697511911392212,\n",
      "            0.12619715929031372,\n",
      "            0.08659981936216354,\n",
      "            0.05523025989532471,\n",
      "            0.07089421153068542,\n",
      "            0.11719696968793869,\n",
      "            0.027246663346886635,\n",
      "            -0.09744405746459961,\n",
      "            -0.018709823489189148,\n",
      "            0.04928603768348694,\n",
      "            0.06073371320962906,\n",
      "            0.055592235177755356,\n",
      "            0.06984801590442657,\n",
      "            -0.05178794264793396,\n",
      "            -0.18874014914035797,\n",
      "            -0.12886300683021545,\n",
      "            0.01501274760812521,\n",
      "            0.21739816665649414,\n",
      "            0.143015056848526,\n",
      "            0.10145591199398041,\n",
      "            -0.15443220734596252,\n",
      "            0.054752785712480545,\n",
      "            0.05032103881239891,\n",
      "            -0.04151412844657898,\n",
      "            0.061936553567647934,\n",
      "            0.0029348982498049736,\n",
      "            0.007161653134971857,\n",
      "            -0.09409237653017044,\n",
      "            0.10016659647226334,\n",
      "            -0.10794103145599365,\n",
      "            -0.08993709832429886\n",
      "          ],\n",
      "          [\n",
      "            0.01544133946299553,\n",
      "            0.015947557985782623,\n",
      "            -0.12987931072711945,\n",
      "            0.09301166236400604,\n",
      "            -0.042163580656051636,\n",
      "            0.05530768632888794,\n",
      "            0.12775707244873047,\n",
      "            -0.16947510838508606,\n",
      "            -0.012250527739524841,\n",
      "            -0.1719149351119995,\n",
      "            0.011678770184516907,\n",
      "            -0.002218097448348999,\n",
      "            -0.16496938467025757,\n",
      "            0.008969509974122047,\n",
      "            0.0111958933994174,\n",
      "            0.08838488161563873,\n",
      "            -0.04523094370961189,\n",
      "            0.08323152363300323,\n",
      "            0.1445038765668869,\n",
      "            -0.12903717160224915,\n",
      "            0.02269703522324562,\n",
      "            0.13895422220230103,\n",
      "            -0.1269734501838684,\n",
      "            -0.05324197933077812,\n",
      "            -0.040370069444179535,\n",
      "            -0.159086212515831,\n",
      "            -0.04857251048088074,\n",
      "            -0.08402875810861588,\n",
      "            -0.029727334156632423,\n",
      "            -0.09608816355466843,\n",
      "            0.16519670188426971,\n",
      "            -0.07759206742048264,\n",
      "            0.06373132765293121,\n",
      "            0.08572275936603546,\n",
      "            -0.08899589627981186,\n",
      "            -0.09453248232603073,\n",
      "            0.09024398028850555,\n",
      "            0.05126485228538513,\n",
      "            0.12225700914859772,\n",
      "            0.07995785772800446,\n",
      "            0.020887959748506546,\n",
      "            -0.044854193925857544,\n",
      "            0.09820827841758728,\n",
      "            -0.09052953124046326,\n",
      "            0.14924675226211548,\n",
      "            -0.13203579187393188,\n",
      "            -0.013343200087547302,\n",
      "            -0.16705481708049774,\n",
      "            -0.07395393401384354,\n",
      "            -0.14407125115394592,\n",
      "            -0.1163288950920105,\n",
      "            0.0674043595790863,\n",
      "            0.10602448880672455,\n",
      "            0.013478304259479046,\n",
      "            0.16104057431221008,\n",
      "            0.09724915027618408,\n",
      "            0.1629260629415512,\n",
      "            0.12741316854953766,\n",
      "            -0.11495211720466614,\n",
      "            0.009036140516400337,\n",
      "            0.08584548532962799,\n",
      "            -0.09004587680101395,\n",
      "            -0.09720870852470398,\n",
      "            0.0835065096616745,\n",
      "            0.13260434567928314,\n",
      "            -0.04405854642391205,\n",
      "            -0.023316949605941772,\n",
      "            -0.1671830266714096,\n",
      "            -0.030810147523880005,\n",
      "            0.1078483909368515,\n",
      "            0.06496354937553406,\n",
      "            -0.14135710895061493,\n",
      "            0.13326992094516754,\n",
      "            -0.014108642004430294,\n",
      "            0.0905289500951767,\n",
      "            -0.059768274426460266,\n",
      "            -0.017172440886497498,\n",
      "            -0.12862204015254974,\n",
      "            0.06140492856502533,\n",
      "            0.10620872676372528,\n",
      "            -0.09942418336868286,\n",
      "            0.14901727437973022,\n",
      "            0.07974626123905182,\n",
      "            0.031435221433639526,\n",
      "            -0.020229894667863846,\n",
      "            -0.0672789216041565,\n",
      "            0.1127028614282608,\n",
      "            0.08174772560596466,\n",
      "            -0.06036245450377464,\n",
      "            -0.11795617640018463,\n",
      "            -0.04833868518471718,\n",
      "            0.10457148402929306,\n",
      "            -0.1494184136390686,\n",
      "            0.10160015523433685,\n",
      "            -0.1137552410364151,\n",
      "            0.1373376101255417,\n",
      "            0.10294749587774277,\n",
      "            -0.13697350025177002,\n",
      "            -0.15689678490161896,\n",
      "            -0.023641720414161682,\n",
      "            -0.11940616369247437,\n",
      "            -0.10034066438674927,\n",
      "            0.03265475854277611,\n",
      "            -0.002003997564315796,\n",
      "            -0.16678763926029205,\n",
      "            0.14790575206279755,\n",
      "            -0.01634041778743267,\n",
      "            -0.014853477478027344,\n",
      "            -0.06544950604438782,\n",
      "            -0.13093174993991852,\n",
      "            -0.08621091395616531,\n",
      "            -0.017379695549607277,\n",
      "            -0.0313638374209404,\n",
      "            0.08600401878356934,\n",
      "            -0.041275203227996826,\n",
      "            -0.10148978233337402,\n",
      "            0.1257334202528,\n",
      "            -0.10640298575162888,\n",
      "            -0.10384292900562286,\n",
      "            0.1105441153049469,\n",
      "            0.06801806390285492,\n",
      "            -0.05909382551908493,\n",
      "            -0.1308111697435379,\n",
      "            -0.12036271393299103,\n",
      "            -0.04919016733765602,\n",
      "            -0.08442717790603638,\n",
      "            -0.1035228967666626,\n",
      "            -0.15222035348415375\n",
      "          ],\n",
      "          [\n",
      "            0.013767971657216549,\n",
      "            0.15882909297943115,\n",
      "            0.17129471898078918,\n",
      "            -0.15370704233646393,\n",
      "            -0.12693938612937927,\n",
      "            -0.029244590550661087,\n",
      "            0.11689690500497818,\n",
      "            0.11077618598937988,\n",
      "            -0.010258011519908905,\n",
      "            0.012652904726564884,\n",
      "            -0.07526293396949768,\n",
      "            0.13030283153057098,\n",
      "            0.0072599356062710285,\n",
      "            -0.002660229802131653,\n",
      "            0.0364549420773983,\n",
      "            0.017214328050613403,\n",
      "            0.132947638630867,\n",
      "            -0.13258038461208344,\n",
      "            0.11028438806533813,\n",
      "            0.15806880593299866,\n",
      "            -0.06412261724472046,\n",
      "            0.019361212849617004,\n",
      "            0.0065340339206159115,\n",
      "            -0.12142367660999298,\n",
      "            -0.0995706170797348,\n",
      "            -0.1018586978316307,\n",
      "            -0.11877283453941345,\n",
      "            -0.1178911030292511,\n",
      "            -0.11881641298532486,\n",
      "            0.007758816704154015,\n",
      "            -0.12604469060897827,\n",
      "            0.15549913048744202,\n",
      "            0.1322297900915146,\n",
      "            0.062092263251543045,\n",
      "            0.055441856384277344,\n",
      "            -0.019727036356925964,\n",
      "            -0.16075293719768524,\n",
      "            -0.07210849225521088,\n",
      "            -0.176661878824234,\n",
      "            -0.053147245198488235,\n",
      "            -0.08692221343517303,\n",
      "            -0.10392239689826965,\n",
      "            -0.06926598399877548,\n",
      "            0.1235540434718132,\n",
      "            -0.09620591253042221,\n",
      "            -0.1616683155298233,\n",
      "            0.06442223489284515,\n",
      "            -0.018589859828352928,\n",
      "            -0.10904812812805176,\n",
      "            0.12143050879240036,\n",
      "            -0.06482543796300888,\n",
      "            -0.15487061440944672,\n",
      "            0.12079974263906479,\n",
      "            -0.03919615224003792,\n",
      "            0.17625385522842407,\n",
      "            -0.06700422614812851,\n",
      "            0.0227601770311594,\n",
      "            -0.11619599908590317,\n",
      "            0.1458110362291336,\n",
      "            -0.15866981446743011,\n",
      "            -0.1480654925107956,\n",
      "            -0.12021245062351227,\n",
      "            -0.15643355250358582,\n",
      "            -0.04777884855866432,\n",
      "            0.09265710413455963,\n",
      "            0.006873802747577429,\n",
      "            -0.15326133370399475,\n",
      "            -0.15708833932876587,\n",
      "            0.08122511208057404,\n",
      "            -0.1033860296010971,\n",
      "            -0.17295467853546143,\n",
      "            0.03652128949761391,\n",
      "            0.16970908641815186,\n",
      "            -0.0727323368191719,\n",
      "            -0.1161174327135086,\n",
      "            -0.10745853930711746,\n",
      "            -0.023597901687026024,\n",
      "            0.11860176920890808,\n",
      "            -0.12972679734230042,\n",
      "            0.030010277405381203,\n",
      "            -0.1387031227350235,\n",
      "            -0.11477310210466385,\n",
      "            -0.07665365934371948,\n",
      "            0.12133236229419708,\n",
      "            -0.02219397947192192,\n",
      "            -0.1080363541841507,\n",
      "            -0.07825608551502228,\n",
      "            -0.05878208577632904,\n",
      "            0.0845741257071495,\n",
      "            -0.03210873156785965,\n",
      "            0.05743062123656273,\n",
      "            0.0006442893063649535,\n",
      "            0.1123846024274826,\n",
      "            -0.01562408171594143,\n",
      "            -0.14105908572673798,\n",
      "            -0.05553770437836647,\n",
      "            -0.0023400564678013325,\n",
      "            0.04719381034374237,\n",
      "            0.0707642212510109,\n",
      "            -0.07002092897891998,\n",
      "            0.08419838547706604,\n",
      "            0.15900324285030365,\n",
      "            -0.11126606166362762,\n",
      "            0.10615033656358719,\n",
      "            -0.06038495525717735,\n",
      "            0.06047157943248749,\n",
      "            -0.12852482497692108,\n",
      "            -0.0766785591840744,\n",
      "            0.027183260768651962,\n",
      "            0.027025070041418076,\n",
      "            -0.1362476348876953,\n",
      "            -0.10137956589460373,\n",
      "            0.023819413036108017,\n",
      "            -0.07750941067934036,\n",
      "            0.13225813210010529,\n",
      "            0.12899957597255707,\n",
      "            0.001997781917452812,\n",
      "            0.02309696562588215,\n",
      "            0.002588333562016487,\n",
      "            0.011577880941331387,\n",
      "            0.04331883415579796,\n",
      "            -0.0685059130191803,\n",
      "            -0.05773407965898514,\n",
      "            0.0830005332827568,\n",
      "            -0.07179363816976547,\n",
      "            0.07518728077411652,\n",
      "            0.06596425175666809,\n",
      "            0.046453021466732025\n",
      "          ],\n",
      "          [\n",
      "            -0.158469095826149,\n",
      "            0.008087479509413242,\n",
      "            0.23456819355487823,\n",
      "            -0.015877651050686836,\n",
      "            0.0979686751961708,\n",
      "            0.15844114124774933,\n",
      "            0.11468717455863953,\n",
      "            0.13639385998249054,\n",
      "            0.031195687130093575,\n",
      "            -0.12607181072235107,\n",
      "            0.017456119880080223,\n",
      "            0.047679830342531204,\n",
      "            0.23404644429683685,\n",
      "            -0.04387986660003662,\n",
      "            0.1512889415025711,\n",
      "            0.11722733080387115,\n",
      "            -0.08377407491207123,\n",
      "            -0.025793572887778282,\n",
      "            -0.08082887530326843,\n",
      "            -0.09188073128461838,\n",
      "            0.17212112247943878,\n",
      "            -0.05018529295921326,\n",
      "            0.029347123578190804,\n",
      "            -0.09998088330030441,\n",
      "            0.2610638439655304,\n",
      "            0.03305128589272499,\n",
      "            0.16069582104682922,\n",
      "            0.08032292127609253,\n",
      "            -0.0881488248705864,\n",
      "            0.03969382867217064,\n",
      "            -0.16775810718536377,\n",
      "            0.045678943395614624,\n",
      "            -0.030257906764745712,\n",
      "            -0.0012447427725419402,\n",
      "            -0.06347530335187912,\n",
      "            -0.057246364653110504,\n",
      "            -0.010841431096196175,\n",
      "            0.06546937674283981,\n",
      "            0.213057279586792,\n",
      "            0.049087777733802795,\n",
      "            0.1551976352930069,\n",
      "            0.13550569117069244,\n",
      "            0.015543277375400066,\n",
      "            0.008648025803267956,\n",
      "            -0.008664817549288273,\n",
      "            -0.04945042356848717,\n",
      "            -0.1213848739862442,\n",
      "            -0.0024812063202261925,\n",
      "            0.16475147008895874,\n",
      "            0.11232295632362366,\n",
      "            0.24335846304893494,\n",
      "            -0.1008065864443779,\n",
      "            0.19693727791309357,\n",
      "            -0.08027785271406174,\n",
      "            0.16724692285060883,\n",
      "            0.17392532527446747,\n",
      "            -0.10804259777069092,\n",
      "            0.18095627427101135,\n",
      "            -0.053330302238464355,\n",
      "            0.16052791476249695,\n",
      "            0.01572718657553196,\n",
      "            -0.013719663955271244,\n",
      "            -0.18231980502605438,\n",
      "            0.15271000564098358,\n",
      "            -0.06993196159601212,\n",
      "            0.19001039862632751,\n",
      "            -0.04793967306613922,\n",
      "            0.03545793890953064,\n",
      "            -0.02290528267621994,\n",
      "            0.1674337387084961,\n",
      "            -0.1287601888179779,\n",
      "            0.1664542704820633,\n",
      "            -0.11257919669151306,\n",
      "            0.26852947473526,\n",
      "            -0.13996082544326782,\n",
      "            0.19061388075351715,\n",
      "            -0.05950641259551048,\n",
      "            0.07297693192958832,\n",
      "            -0.041793499141931534,\n",
      "            0.22576630115509033,\n",
      "            -0.06534682214260101,\n",
      "            0.12278240919113159,\n",
      "            0.18955785036087036,\n",
      "            0.17071911692619324,\n",
      "            -0.015424834564328194,\n",
      "            -0.020361872389912605,\n",
      "            0.019816620275378227,\n",
      "            0.11384521424770355,\n",
      "            0.10042127966880798,\n",
      "            0.07927751541137695,\n",
      "            0.06022657826542854,\n",
      "            0.011582644656300545,\n",
      "            -0.13244900107383728,\n",
      "            0.12974873185157776,\n",
      "            0.2160165011882782,\n",
      "            0.07601000368595123,\n",
      "            0.08202696591615677,\n",
      "            0.05159616097807884,\n",
      "            0.055240921676158905,\n",
      "            0.19064804911613464,\n",
      "            0.0008832318126223981,\n",
      "            -0.0895199254155159,\n",
      "            0.0005607621278613806,\n",
      "            0.016912881284952164,\n",
      "            -0.0668925940990448,\n",
      "            -0.011585026979446411,\n",
      "            -0.16496559977531433,\n",
      "            -0.061337947845458984,\n",
      "            0.19391082227230072,\n",
      "            0.12126421183347702,\n",
      "            0.05581169202923775,\n",
      "            -0.1826256811618805,\n",
      "            -0.13743653893470764,\n",
      "            -0.17061980068683624,\n",
      "            0.14238040149211884,\n",
      "            0.1990232616662979,\n",
      "            0.020646467804908752,\n",
      "            0.12211952358484268,\n",
      "            0.143405020236969,\n",
      "            -0.13973170518875122,\n",
      "            0.08196539431810379,\n",
      "            0.12542712688446045,\n",
      "            0.04490339010953903,\n",
      "            0.09931148588657379,\n",
      "            -0.19872704148292542,\n",
      "            -0.02137009985744953,\n",
      "            0.1996164619922638,\n",
      "            0.01243236567825079\n",
      "          ],\n",
      "          [\n",
      "            0.10761971771717072,\n",
      "            0.11433304846286774,\n",
      "            0.06252379715442657,\n",
      "            0.10690338909626007,\n",
      "            0.03310862183570862,\n",
      "            0.1490907520055771,\n",
      "            0.10332884639501572,\n",
      "            -0.044112563133239746,\n",
      "            -0.052770886570215225,\n",
      "            -0.08165566623210907,\n",
      "            0.1570805311203003,\n",
      "            0.09144613146781921,\n",
      "            -0.048506516963243484,\n",
      "            0.0582958422601223,\n",
      "            0.06100546196103096,\n",
      "            0.12950028479099274,\n",
      "            0.0009020236902870238,\n",
      "            -0.15456523001194,\n",
      "            0.007343473378568888,\n",
      "            -0.09374286979436874,\n",
      "            0.08207553625106812,\n",
      "            0.11286130547523499,\n",
      "            -0.17020463943481445,\n",
      "            0.15507616102695465,\n",
      "            -0.03362646326422691,\n",
      "            -0.015927160158753395,\n",
      "            -0.10677140206098557,\n",
      "            -0.0289999321103096,\n",
      "            0.14399482309818268,\n",
      "            -0.054512619972229004,\n",
      "            -0.04633234068751335,\n",
      "            -0.018579203635454178,\n",
      "            -0.025357365608215332,\n",
      "            -0.023816484957933426,\n",
      "            -0.1356346309185028,\n",
      "            -0.11182497441768646,\n",
      "            0.030333830043673515,\n",
      "            0.11218980699777603,\n",
      "            0.009168257005512714,\n",
      "            0.14769558608531952,\n",
      "            -0.054889872670173645,\n",
      "            0.05573338270187378,\n",
      "            -0.03995747119188309,\n",
      "            0.047394853085279465,\n",
      "            0.05579424276947975,\n",
      "            -0.157149538397789,\n",
      "            -0.017441904172301292,\n",
      "            0.13332675397396088,\n",
      "            0.013109325431287289,\n",
      "            0.07472044229507446,\n",
      "            0.08105804771184921,\n",
      "            0.039852529764175415,\n",
      "            0.11567870527505875,\n",
      "            0.046817366033792496,\n",
      "            0.061210066080093384,\n",
      "            0.024165067821741104,\n",
      "            -0.16849441826343536,\n",
      "            0.10990913212299347,\n",
      "            0.08712136745452881,\n",
      "            0.10028145462274551,\n",
      "            -0.15545764565467834,\n",
      "            0.10073105990886688,\n",
      "            0.17663520574569702,\n",
      "            -0.1141301617026329,\n",
      "            -0.06312127411365509,\n",
      "            -0.0763348788022995,\n",
      "            0.068821981549263,\n",
      "            -0.054274462163448334,\n",
      "            0.02012498676776886,\n",
      "            -0.051857538521289825,\n",
      "            -0.05110164359211922,\n",
      "            -0.05582376569509506,\n",
      "            0.16574087738990784,\n",
      "            0.1664290428161621,\n",
      "            -0.13873012363910675,\n",
      "            0.002785141346976161,\n",
      "            -0.12818439304828644,\n",
      "            0.08222752064466476,\n",
      "            -0.017980722710490227,\n",
      "            0.036412227898836136,\n",
      "            -0.034122202545404434,\n",
      "            0.14618632197380066,\n",
      "            0.009035888127982616,\n",
      "            -0.16984443366527557,\n",
      "            -0.17553561925888062,\n",
      "            0.1107255220413208,\n",
      "            -0.09137914329767227,\n",
      "            0.016536831855773926,\n",
      "            -0.15728875994682312,\n",
      "            0.14528754353523254,\n",
      "            0.10535258054733276,\n",
      "            0.15232297778129578,\n",
      "            0.0567525252699852,\n",
      "            0.1674870252609253,\n",
      "            0.14261391758918762,\n",
      "            0.08557569235563278,\n",
      "            0.0862506777048111,\n",
      "            -0.1263681799173355,\n",
      "            -0.09175966680049896,\n",
      "            0.13279767334461212,\n",
      "            -0.15952947735786438,\n",
      "            -0.16806799173355103,\n",
      "            0.14504273235797882,\n",
      "            0.14886486530303955,\n",
      "            -0.04672446846961975,\n",
      "            0.0020489245653152466,\n",
      "            0.12461286783218384,\n",
      "            0.12464836984872818,\n",
      "            -0.17468249797821045,\n",
      "            0.11099263280630112,\n",
      "            0.12221991270780563,\n",
      "            -0.1255994737148285,\n",
      "            0.17217445373535156,\n",
      "            0.028457194566726685,\n",
      "            0.1561041623353958,\n",
      "            -0.1686515063047409,\n",
      "            0.06033052131533623,\n",
      "            -0.10049448907375336,\n",
      "            0.04307296872138977,\n",
      "            -0.10504688322544098,\n",
      "            -0.08681155741214752,\n",
      "            -0.019641486927866936,\n",
      "            -0.14843317866325378,\n",
      "            -0.018961524590849876,\n",
      "            -0.14415813982486725,\n",
      "            0.08087517321109772,\n",
      "            -0.16940151154994965,\n",
      "            0.10188096761703491\n",
      "          ],\n",
      "          [\n",
      "            0.06063729524612427,\n",
      "            0.06834673136472702,\n",
      "            0.03268323466181755,\n",
      "            -0.08001015335321426,\n",
      "            -0.10010537505149841,\n",
      "            0.03048582188785076,\n",
      "            -0.008595524355769157,\n",
      "            0.019638748839497566,\n",
      "            -0.13386289775371552,\n",
      "            0.15573453903198242,\n",
      "            -0.11311312019824982,\n",
      "            0.15268531441688538,\n",
      "            0.13796450197696686,\n",
      "            -0.08638343214988708,\n",
      "            0.01666797138750553,\n",
      "            0.11049304902553558,\n",
      "            -0.06432506442070007,\n",
      "            -0.1307276338338852,\n",
      "            0.02758066914975643,\n",
      "            0.17010854184627533,\n",
      "            0.1141507625579834,\n",
      "            -0.035537730902433395,\n",
      "            -0.09881169348955154,\n",
      "            0.10823676735162735,\n",
      "            0.1277456134557724,\n",
      "            -0.0993114784359932,\n",
      "            0.10250324010848999,\n",
      "            0.14939360320568085,\n",
      "            0.0898466631770134,\n",
      "            0.16275548934936523,\n",
      "            0.08899153769016266,\n",
      "            -0.02736051194369793,\n",
      "            0.03745962679386139,\n",
      "            -0.041460972279310226,\n",
      "            0.15171386301517487,\n",
      "            0.13862252235412598,\n",
      "            -0.1796935796737671,\n",
      "            -0.029825828969478607,\n",
      "            -0.13689397275447845,\n",
      "            -0.11485899239778519,\n",
      "            0.1022200807929039,\n",
      "            -0.1704246699810028,\n",
      "            -0.13033843040466309,\n",
      "            0.12529152631759644,\n",
      "            0.17337356507778168,\n",
      "            0.04586518555879593,\n",
      "            0.15676411986351013,\n",
      "            -0.11831643432378769,\n",
      "            0.05129093676805496,\n",
      "            0.16769392788410187,\n",
      "            -0.02698805183172226,\n",
      "            0.14784690737724304,\n",
      "            -0.1380433589220047,\n",
      "            -0.1398717612028122,\n",
      "            0.002623028354719281,\n",
      "            -0.11714252829551697,\n",
      "            -0.16344086825847626,\n",
      "            -0.07548189908266068,\n",
      "            0.10876353830099106,\n",
      "            0.14058725535869598,\n",
      "            -0.16181062161922455,\n",
      "            0.147444948554039,\n",
      "            0.1423073410987854,\n",
      "            -0.1373220831155777,\n",
      "            -0.07993821799755096,\n",
      "            0.1476936936378479,\n",
      "            0.01527242548763752,\n",
      "            -0.15195852518081665,\n",
      "            -0.037444181740283966,\n",
      "            0.09415363520383835,\n",
      "            -0.0983549952507019,\n",
      "            -0.1076805517077446,\n",
      "            0.15355810523033142,\n",
      "            0.03616168349981308,\n",
      "            -0.09079691767692566,\n",
      "            0.02620287984609604,\n",
      "            0.10687417536973953,\n",
      "            -0.0689038410782814,\n",
      "            -0.08767475932836533,\n",
      "            -0.08973270654678345,\n",
      "            0.16107304394245148,\n",
      "            -0.134202778339386,\n",
      "            0.07023881375789642,\n",
      "            -0.06984538584947586,\n",
      "            -0.028607556596398354,\n",
      "            -0.09550568461418152,\n",
      "            0.06185392662882805,\n",
      "            -0.100895956158638,\n",
      "            0.09835797548294067,\n",
      "            0.07105932384729385,\n",
      "            -0.13043639063835144,\n",
      "            -0.1370326429605484,\n",
      "            0.13542349636554718,\n",
      "            0.11220517009496689,\n",
      "            -0.0964551642537117,\n",
      "            0.117924764752388,\n",
      "            -0.0629367083311081,\n",
      "            0.020506080240011215,\n",
      "            -0.03947581723332405,\n",
      "            -0.15701988339424133,\n",
      "            0.10275376588106155,\n",
      "            -0.05548621714115143,\n",
      "            -0.034512873739004135,\n",
      "            0.107883520424366,\n",
      "            -0.16152355074882507,\n",
      "            -0.0508899986743927,\n",
      "            0.14497457444667816,\n",
      "            0.002780648646876216,\n",
      "            0.14973263442516327,\n",
      "            -0.06999565660953522,\n",
      "            0.06692181527614594,\n",
      "            -0.11878149211406708,\n",
      "            0.16698069870471954,\n",
      "            0.09745711088180542,\n",
      "            -0.11912917345762253,\n",
      "            0.010852162726223469,\n",
      "            -0.13067449629306793,\n",
      "            -0.13601508736610413,\n",
      "            -0.14840081334114075,\n",
      "            0.026192352175712585,\n",
      "            0.05357903614640236,\n",
      "            0.042566508054733276,\n",
      "            -0.12405604869127274,\n",
      "            0.15742149949073792,\n",
      "            -0.04135984554886818,\n",
      "            -0.07976359128952026,\n",
      "            -0.16235867142677307,\n",
      "            -0.13731785118579865\n",
      "          ],\n",
      "          [\n",
      "            -0.03205161914229393,\n",
      "            0.051882579922676086,\n",
      "            0.12017083168029785,\n",
      "            -0.01706322468817234,\n",
      "            -0.018770987167954445,\n",
      "            -0.06009320914745331,\n",
      "            0.04309022054076195,\n",
      "            -0.051992498338222504,\n",
      "            0.054459258913993835,\n",
      "            0.055173810571432114,\n",
      "            -0.09085604548454285,\n",
      "            0.06961901485919952,\n",
      "            0.04312126711010933,\n",
      "            0.09959961473941803,\n",
      "            0.04958193749189377,\n",
      "            -0.0547025203704834,\n",
      "            -0.01341702975332737,\n",
      "            -0.009659742005169392,\n",
      "            0.1288834512233734,\n",
      "            -0.002579357009381056,\n",
      "            -0.16357317566871643,\n",
      "            0.08558078855276108,\n",
      "            0.017750201746821404,\n",
      "            0.044288188219070435,\n",
      "            0.08783137053251266,\n",
      "            0.028678102418780327,\n",
      "            0.07569977641105652,\n",
      "            0.07268460094928741,\n",
      "            -0.019185801967978477,\n",
      "            0.08278559148311615,\n",
      "            0.0021967077627778053,\n",
      "            -0.05957762897014618,\n",
      "            -0.011671796441078186,\n",
      "            0.17128652334213257,\n",
      "            0.15885643661022186,\n",
      "            0.03230849653482437,\n",
      "            0.10854434967041016,\n",
      "            -0.16079899668693542,\n",
      "            0.04893714562058449,\n",
      "            0.139232337474823,\n",
      "            0.07485821098089218,\n",
      "            -0.14980530738830566,\n",
      "            -0.15038347244262695,\n",
      "            -0.02152833715081215,\n",
      "            0.12374057620763779,\n",
      "            -0.03132065385580063,\n",
      "            -0.14289513230323792,\n",
      "            0.12849242985248566,\n",
      "            -0.07852436602115631,\n",
      "            0.004402675665915012,\n",
      "            0.03973693400621414,\n",
      "            0.027485480532050133,\n",
      "            0.012231075204908848,\n",
      "            -0.1297675520181656,\n",
      "            0.05023621767759323,\n",
      "            -0.15594181418418884,\n",
      "            0.049336858093738556,\n",
      "            -0.1459828019142151,\n",
      "            -0.130046084523201,\n",
      "            -0.17157422006130219,\n",
      "            -0.08026693761348724,\n",
      "            -0.005258256569504738,\n",
      "            0.1367245614528656,\n",
      "            0.16094644367694855,\n",
      "            -0.0005622852477245033,\n",
      "            0.011814567260444164,\n",
      "            -0.09555820375680923,\n",
      "            0.02058664709329605,\n",
      "            -0.057592276483774185,\n",
      "            -0.09139982610940933,\n",
      "            -0.007887006737291813,\n",
      "            -0.0036744202952831984,\n",
      "            0.1634880155324936,\n",
      "            0.041949979960918427,\n",
      "            -0.04294603690505028,\n",
      "            0.046068813651800156,\n",
      "            0.05232083424925804,\n",
      "            0.1954941600561142,\n",
      "            -0.07538944482803345,\n",
      "            0.07346389442682266,\n",
      "            -0.040431879460811615,\n",
      "            -0.0946076512336731,\n",
      "            0.17130684852600098,\n",
      "            0.10082381963729858,\n",
      "            -0.1068650558590889,\n",
      "            0.06330467760562897,\n",
      "            -0.10886208713054657,\n",
      "            -0.18178896605968475,\n",
      "            0.023882493376731873,\n",
      "            0.02884773164987564,\n",
      "            -0.08321737498044968,\n",
      "            -0.14260852336883545,\n",
      "            0.050773631781339645,\n",
      "            -0.18934135138988495,\n",
      "            -0.15515978634357452,\n",
      "            -0.07840140908956528,\n",
      "            -0.0705956295132637,\n",
      "            0.16221536695957184,\n",
      "            0.022042440250515938,\n",
      "            -0.10019738227128983,\n",
      "            0.024133002385497093,\n",
      "            -0.05689459294080734,\n",
      "            -0.040838953107595444,\n",
      "            -0.08522527664899826,\n",
      "            0.019899684935808182,\n",
      "            -0.15510137379169464,\n",
      "            0.16426609456539154,\n",
      "            -0.06690054386854172,\n",
      "            -0.023144302889704704,\n",
      "            0.17586874961853027,\n",
      "            0.017764991149306297,\n",
      "            0.08051331341266632,\n",
      "            0.17937996983528137,\n",
      "            0.18473096191883087,\n",
      "            0.06342114508152008,\n",
      "            -0.00809952337294817,\n",
      "            -0.10576245933771133,\n",
      "            0.03161834925413132,\n",
      "            0.11560142785310745,\n",
      "            -0.13569289445877075,\n",
      "            0.00641272496432066,\n",
      "            -0.01816575601696968,\n",
      "            0.04161960259079933,\n",
      "            -0.11341851204633713,\n",
      "            0.17131564021110535,\n",
      "            -0.0752975270152092,\n",
      "            0.044618912041187286,\n",
      "            0.11920298635959625\n",
      "          ],\n",
      "          [\n",
      "            0.06883031129837036,\n",
      "            -0.01017487607896328,\n",
      "            -0.0724102109670639,\n",
      "            -0.13356877863407135,\n",
      "            0.036811210215091705,\n",
      "            0.10642905533313751,\n",
      "            0.14993013441562653,\n",
      "            -0.022720348089933395,\n",
      "            0.03875361382961273,\n",
      "            0.07544625550508499,\n",
      "            -0.0639888122677803,\n",
      "            0.05102069303393364,\n",
      "            0.14827251434326172,\n",
      "            -0.07682591676712036,\n",
      "            -0.06787917017936707,\n",
      "            -0.022417232394218445,\n",
      "            -0.0751248449087143,\n",
      "            -0.008096868172287941,\n",
      "            0.10261766612529755,\n",
      "            0.09165407717227936,\n",
      "            -0.08753246814012527,\n",
      "            0.16271856427192688,\n",
      "            -0.11279554665088654,\n",
      "            -0.09239529818296432,\n",
      "            0.037445444613695145,\n",
      "            0.09217297285795212,\n",
      "            0.10503004491329193,\n",
      "            0.06336775422096252,\n",
      "            0.004872757475823164,\n",
      "            0.02004266157746315,\n",
      "            -0.06497165560722351,\n",
      "            -0.08543233573436737,\n",
      "            0.1595911681652069,\n",
      "            0.013563976623117924,\n",
      "            0.03408132493495941,\n",
      "            -0.1248503252863884,\n",
      "            0.06673167645931244,\n",
      "            -0.07410658150911331,\n",
      "            -0.025909487158060074,\n",
      "            0.17519265413284302,\n",
      "            -0.10953933000564575,\n",
      "            0.011027932167053223,\n",
      "            0.1337154060602188,\n",
      "            -0.1326923370361328,\n",
      "            -0.1380978524684906,\n",
      "            0.1673230677843094,\n",
      "            -0.03718522563576698,\n",
      "            -0.086214579641819,\n",
      "            -0.056153371930122375,\n",
      "            -0.08225081861019135,\n",
      "            -0.06820429116487503,\n",
      "            0.04887879267334938,\n",
      "            0.06321419775485992,\n",
      "            -0.05201540142297745,\n",
      "            0.13296863436698914,\n",
      "            -0.08842973411083221,\n",
      "            -0.025832369923591614,\n",
      "            -0.14097094535827637,\n",
      "            -0.035122036933898926,\n",
      "            -0.16142591834068298,\n",
      "            -0.08453484624624252,\n",
      "            -0.16836895048618317,\n",
      "            -0.10712293535470963,\n",
      "            0.004301135428249836,\n",
      "            0.06969484686851501,\n",
      "            0.03924070671200752,\n",
      "            0.15432918071746826,\n",
      "            -0.17070640623569489,\n",
      "            0.08627910912036896,\n",
      "            -0.1407518833875656,\n",
      "            0.047707702964544296,\n",
      "            0.04218735918402672,\n",
      "            0.12336346507072449,\n",
      "            0.0957893580198288,\n",
      "            -0.16451069712638855,\n",
      "            0.07881785929203033,\n",
      "            0.03793957829475403,\n",
      "            -0.13151924312114716,\n",
      "            0.0003298866213299334,\n",
      "            -0.02118891105055809,\n",
      "            -0.14457479119300842,\n",
      "            -0.032447248697280884,\n",
      "            -0.011573359370231628,\n",
      "            0.05723005160689354,\n",
      "            0.005531935952603817,\n",
      "            0.046333782374858856,\n",
      "            0.041405364871025085,\n",
      "            -0.06597095727920532,\n",
      "            0.14899194240570068,\n",
      "            -0.013904007151722908,\n",
      "            0.14412422478199005,\n",
      "            -0.06026184558868408,\n",
      "            -0.11253537982702255,\n",
      "            -0.15478338301181793,\n",
      "            -0.17545676231384277,\n",
      "            0.08888201415538788,\n",
      "            0.14633773267269135,\n",
      "            0.03938378393650055,\n",
      "            -0.14809221029281616,\n",
      "            0.12332095205783844,\n",
      "            -0.0329812616109848,\n",
      "            0.006959244608879089,\n",
      "            0.16014403104782104,\n",
      "            0.06410402059555054,\n",
      "            0.005269914865493774,\n",
      "            0.03826715052127838,\n",
      "            0.020316582173109055,\n",
      "            -0.0211779922246933,\n",
      "            0.12590263783931732,\n",
      "            -0.07153007388114929,\n",
      "            -0.13448448479175568,\n",
      "            0.012126078829169273,\n",
      "            0.03213756904006004,\n",
      "            -0.08103330433368683,\n",
      "            0.054698389023542404,\n",
      "            -0.028203358873724937,\n",
      "            -0.005114094819873571,\n",
      "            -0.07433822005987167,\n",
      "            -0.010563931427896023,\n",
      "            -0.07786643505096436,\n",
      "            0.0712382048368454,\n",
      "            0.0030582936014980078,\n",
      "            -0.02487916871905327,\n",
      "            0.05842284485697746,\n",
      "            0.07679089158773422,\n",
      "            0.07487949728965759,\n",
      "            -0.13529586791992188,\n",
      "            -0.06823650747537613\n",
      "          ],\n",
      "          [\n",
      "            -0.133698508143425,\n",
      "            0.06353496760129929,\n",
      "            0.11128119379281998,\n",
      "            -0.15278244018554688,\n",
      "            -0.017715124413371086,\n",
      "            0.1583489328622818,\n",
      "            -0.02039260044693947,\n",
      "            0.15353275835514069,\n",
      "            0.045477449893951416,\n",
      "            0.11401599645614624,\n",
      "            0.04405079782009125,\n",
      "            0.08314129710197449,\n",
      "            -0.04653151333332062,\n",
      "            0.12980873882770538,\n",
      "            -0.1686166375875473,\n",
      "            -0.11144111305475235,\n",
      "            -0.039654623717069626,\n",
      "            -0.02280312590301037,\n",
      "            -0.14738841354846954,\n",
      "            0.19289815425872803,\n",
      "            0.10320501029491425,\n",
      "            -0.09795410931110382,\n",
      "            -0.008221989497542381,\n",
      "            -0.053033020347356796,\n",
      "            0.14805248379707336,\n",
      "            0.17489543557167053,\n",
      "            0.03227216750383377,\n",
      "            0.16584627330303192,\n",
      "            -0.09217534214258194,\n",
      "            -0.06620652973651886,\n",
      "            0.09817968308925629,\n",
      "            0.18457332253456116,\n",
      "            0.22151553630828857,\n",
      "            -0.1732790619134903,\n",
      "            0.17218057811260223,\n",
      "            0.09488808363676071,\n",
      "            -0.08584368973970413,\n",
      "            -0.12058228254318237,\n",
      "            -0.15348981320858002,\n",
      "            0.17799507081508636,\n",
      "            0.0007578528020530939,\n",
      "            0.002094476018100977,\n",
      "            -0.03168635442852974,\n",
      "            -0.1423535943031311,\n",
      "            0.08545636385679245,\n",
      "            0.1800355613231659,\n",
      "            0.08076217770576477,\n",
      "            -0.00794490147382021,\n",
      "            0.07682085782289505,\n",
      "            0.00031575525645166636,\n",
      "            0.18866822123527527,\n",
      "            0.056075096130371094,\n",
      "            0.009326105006039143,\n",
      "            0.045266102999448776,\n",
      "            0.13857628405094147,\n",
      "            -0.011792775243520737,\n",
      "            -0.014852851629257202,\n",
      "            0.18556150794029236,\n",
      "            0.10748060792684555,\n",
      "            -0.1022782176733017,\n",
      "            0.06088580563664436,\n",
      "            0.028912346810102463,\n",
      "            -0.23591852188110352,\n",
      "            -0.08146820962429047,\n",
      "            -0.1249995008111,\n",
      "            0.03380103409290314,\n",
      "            0.10304500162601471,\n",
      "            0.22400499880313873,\n",
      "            -0.0030476253014057875,\n",
      "            -0.04812268540263176,\n",
      "            -0.138472318649292,\n",
      "            -0.041986867785453796,\n",
      "            0.17425982654094696,\n",
      "            0.2015913724899292,\n",
      "            0.164600670337677,\n",
      "            0.18881705403327942,\n",
      "            -0.11063994467258453,\n",
      "            -0.15941929817199707,\n",
      "            -0.05705993250012398,\n",
      "            0.05532233044505119,\n",
      "            0.16409388184547424,\n",
      "            0.11711623519659042,\n",
      "            -0.0659802109003067,\n",
      "            0.11210553348064423,\n",
      "            0.08387823402881622,\n",
      "            0.15088285505771637,\n",
      "            0.1300387680530548,\n",
      "            -0.15213313698768616,\n",
      "            0.010704251006245613,\n",
      "            -0.09799996763467789,\n",
      "            -0.003351077437400818,\n",
      "            0.20116440951824188,\n",
      "            -0.1540878862142563,\n",
      "            0.2537153363227844,\n",
      "            -0.021276328712701797,\n",
      "            0.2634252905845642,\n",
      "            -0.023092815652489662,\n",
      "            0.05043429136276245,\n",
      "            0.09202472865581512,\n",
      "            0.107222780585289,\n",
      "            0.15751582384109497,\n",
      "            -0.14867082238197327,\n",
      "            0.049003757536411285,\n",
      "            0.11668651551008224,\n",
      "            0.06792403757572174,\n",
      "            -0.15776817500591278,\n",
      "            0.04252064228057861,\n",
      "            0.13850508630275726,\n",
      "            -0.0948481634259224,\n",
      "            0.18776196241378784,\n",
      "            0.05146852508187294,\n",
      "            0.11413867026567459,\n",
      "            -0.13393189013004303,\n",
      "            -0.15629881620407104,\n",
      "            -0.1346667855978012,\n",
      "            -0.07692906260490417,\n",
      "            0.100776307284832,\n",
      "            0.08086211234331131,\n",
      "            0.16847041249275208,\n",
      "            0.07099830359220505,\n",
      "            -0.0033220890909433365,\n",
      "            0.06342796981334686,\n",
      "            -0.024595635011792183,\n",
      "            -0.02792680636048317,\n",
      "            0.1291654407978058,\n",
      "            0.04806496575474739,\n",
      "            0.2177339345216751,\n",
      "            0.047321632504463196\n",
      "          ],\n",
      "          [\n",
      "            0.1042909100651741,\n",
      "            0.056158483028411865,\n",
      "            0.07730524986982346,\n",
      "            -0.1544891744852066,\n",
      "            -0.07034886628389359,\n",
      "            0.14411859214305878,\n",
      "            0.11302769929170609,\n",
      "            -0.1243722215294838,\n",
      "            -0.10310354083776474,\n",
      "            0.06530959159135818,\n",
      "            -0.09859837591648102,\n",
      "            -0.05097372457385063,\n",
      "            0.19613227248191833,\n",
      "            0.08764354139566422,\n",
      "            -0.10505165904760361,\n",
      "            -0.12205739319324493,\n",
      "            -0.029239313676953316,\n",
      "            0.13001060485839844,\n",
      "            -0.14578720927238464,\n",
      "            -0.017079578712582588,\n",
      "            0.12941744923591614,\n",
      "            -0.0916048139333725,\n",
      "            -0.028203237801790237,\n",
      "            0.06819894909858704,\n",
      "            0.059896938502788544,\n",
      "            0.2094750851392746,\n",
      "            0.175035759806633,\n",
      "            -0.07719198614358902,\n",
      "            -0.013193534687161446,\n",
      "            0.09541046619415283,\n",
      "            0.08900623768568039,\n",
      "            0.16359984874725342,\n",
      "            0.24347734451293945,\n",
      "            -0.13467101752758026,\n",
      "            -0.09356874227523804,\n",
      "            -0.0009273799369111657,\n",
      "            0.11658300459384918,\n",
      "            -0.16214139759540558,\n",
      "            -0.03591994568705559,\n",
      "            0.006519913207739592,\n",
      "            0.13689039647579193,\n",
      "            0.16414280235767365,\n",
      "            -0.015157029964029789,\n",
      "            0.1373840719461441,\n",
      "            -0.13050134479999542,\n",
      "            0.14655020833015442,\n",
      "            -0.18202093243598938,\n",
      "            -0.013559719547629356,\n",
      "            0.07623933255672455,\n",
      "            0.17041301727294922,\n",
      "            0.1810254603624344,\n",
      "            -0.10296550393104553,\n",
      "            0.20523841679096222,\n",
      "            -0.016947025433182716,\n",
      "            0.012741753831505775,\n",
      "            0.21994931995868683,\n",
      "            0.093508280813694,\n",
      "            -0.1001221165060997,\n",
      "            -0.12311208248138428,\n",
      "            0.16405035555362701,\n",
      "            0.2044975757598877,\n",
      "            -0.13508929312229156,\n",
      "            -0.1944555938243866,\n",
      "            0.017479214817285538,\n",
      "            -0.010091681964695454,\n",
      "            0.11698500066995621,\n",
      "            -0.16500598192214966,\n",
      "            0.07775011658668518,\n",
      "            0.031248224899172783,\n",
      "            -0.11597880721092224,\n",
      "            0.052021324634552,\n",
      "            0.0951150432229042,\n",
      "            -0.15985007584095,\n",
      "            -0.025067375972867012,\n",
      "            -0.0054159644059836864,\n",
      "            0.1482676863670349,\n",
      "            0.13403820991516113,\n",
      "            0.03510477393865585,\n",
      "            0.05766955018043518,\n",
      "            0.01663024164736271,\n",
      "            0.14043991267681122,\n",
      "            -0.07091184705495834,\n",
      "            0.15748251974582672,\n",
      "            -0.008984696120023727,\n",
      "            0.08398124575614929,\n",
      "            -0.0014264064375311136,\n",
      "            -0.001137157785706222,\n",
      "            -0.024460894986987114,\n",
      "            0.077645443379879,\n",
      "            -0.055689889937639236,\n",
      "            -0.06368692964315414,\n",
      "            0.01286375429481268,\n",
      "            -0.0879831463098526,\n",
      "            -0.07518090307712555,\n",
      "            0.16322092711925507,\n",
      "            0.07034249603748322,\n",
      "            -0.21427446603775024,\n",
      "            0.10100340098142624,\n",
      "            -0.030297953635454178,\n",
      "            0.18681105971336365,\n",
      "            0.04616430401802063,\n",
      "            0.1472575068473816,\n",
      "            -0.1601191908121109,\n",
      "            0.05800826475024223,\n",
      "            0.1304420679807663,\n",
      "            -0.039118632674217224,\n",
      "            0.10153374075889587,\n",
      "            0.1563986986875534,\n",
      "            0.019265733659267426,\n",
      "            0.1239151805639267,\n",
      "            -0.1754942536354065,\n",
      "            0.11218266934156418,\n",
      "            0.11769348382949829,\n",
      "            -0.17433451116085052,\n",
      "            -0.06435271352529526,\n",
      "            0.21685101091861725,\n",
      "            -0.14179490506649017,\n",
      "            0.05308559536933899,\n",
      "            0.09952397644519806,\n",
      "            -0.13854266703128815,\n",
      "            -0.12684732675552368,\n",
      "            -0.05854300409555435,\n",
      "            0.02164645493030548,\n",
      "            -0.14710547029972076,\n",
      "            0.006698025390505791,\n",
      "            -0.10929591208696365,\n",
      "            0.2207348793745041,\n",
      "            0.10833317786455154\n",
      "          ],\n",
      "          [\n",
      "            -0.07905484735965729,\n",
      "            -0.1068669930100441,\n",
      "            0.011891867034137249,\n",
      "            0.08687275648117065,\n",
      "            0.19262923300266266,\n",
      "            0.14049898087978363,\n",
      "            0.0607232041656971,\n",
      "            -0.10412323474884033,\n",
      "            0.15949617326259613,\n",
      "            0.0782824158668518,\n",
      "            -0.00847630575299263,\n",
      "            0.021628636866807938,\n",
      "            -0.002976966556161642,\n",
      "            0.12893983721733093,\n",
      "            0.02763349749147892,\n",
      "            0.07412110269069672,\n",
      "            -0.1721852421760559,\n",
      "            -0.18630795180797577,\n",
      "            -0.16381730139255524,\n",
      "            -0.05407941713929176,\n",
      "            -0.03912842646241188,\n",
      "            0.08524227142333984,\n",
      "            0.17298287153244019,\n",
      "            0.02040654793381691,\n",
      "            0.1312963366508484,\n",
      "            0.138202965259552,\n",
      "            0.13879647850990295,\n",
      "            -0.09908889979124069,\n",
      "            0.09663781523704529,\n",
      "            0.1836976259946823,\n",
      "            0.0777958407998085,\n",
      "            0.009484903886914253,\n",
      "            -0.05256670340895653,\n",
      "            0.026810795068740845,\n",
      "            -0.0031771063804626465,\n",
      "            0.03689437732100487,\n",
      "            0.030235538259148598,\n",
      "            -0.06448990106582642,\n",
      "            0.16376787424087524,\n",
      "            -0.15874925255775452,\n",
      "            0.19643250107765198,\n",
      "            0.10279065370559692,\n",
      "            -0.14698684215545654,\n",
      "            -0.12346740067005157,\n",
      "            -0.12892447412014008,\n",
      "            0.0694606825709343,\n",
      "            -0.20758192241191864,\n",
      "            0.09186238795518875,\n",
      "            0.028986556455492973,\n",
      "            -0.006114133168011904,\n",
      "            0.03080318309366703,\n",
      "            0.08670420199632645,\n",
      "            -0.0972316712141037,\n",
      "            0.08768245577812195,\n",
      "            -0.12277619540691376,\n",
      "            -0.021750375628471375,\n",
      "            0.12083830684423447,\n",
      "            -0.05154220014810562,\n",
      "            0.13933119177818298,\n",
      "            0.19571955502033234,\n",
      "            0.08112110942602158,\n",
      "            0.05470739305019379,\n",
      "            -0.14083969593048096,\n",
      "            0.15568311512470245,\n",
      "            -0.10533284395933151,\n",
      "            0.1785055547952652,\n",
      "            -0.025404591113328934,\n",
      "            0.21193334460258484,\n",
      "            -0.1357581615447998,\n",
      "            0.17487983405590057,\n",
      "            0.12847557663917542,\n",
      "            -0.07515811175107956,\n",
      "            -0.038600802421569824,\n",
      "            0.07599491626024246,\n",
      "            0.1503562182188034,\n",
      "            0.15788991749286652,\n",
      "            -0.08606569468975067,\n",
      "            0.0320463702082634,\n",
      "            0.07101380825042725,\n",
      "            -0.12898816168308258,\n",
      "            -0.13257072865962982,\n",
      "            -0.15254804491996765,\n",
      "            0.17866331338882446,\n",
      "            0.14439870417118073,\n",
      "            -0.004240884445607662,\n",
      "            0.10671201348304749,\n",
      "            -0.1698697805404663,\n",
      "            -0.06615934520959854,\n",
      "            0.07676879316568375,\n",
      "            -0.022428318858146667,\n",
      "            -0.07103118300437927,\n",
      "            0.2087910771369934,\n",
      "            -0.061731576919555664,\n",
      "            -0.07568316906690598,\n",
      "            0.10043661296367645,\n",
      "            0.2103060483932495,\n",
      "            -0.1913299262523651,\n",
      "            0.08192984014749527,\n",
      "            0.23353224992752075,\n",
      "            0.057188332080841064,\n",
      "            -0.0695510134100914,\n",
      "            0.12465664744377136,\n",
      "            0.004592082463204861,\n",
      "            0.14025330543518066,\n",
      "            -0.11016515642404556,\n",
      "            0.03330768644809723,\n",
      "            -0.0806102454662323,\n",
      "            -0.1429116278886795,\n",
      "            0.058297041803598404,\n",
      "            0.14724968373775482,\n",
      "            0.11804839968681335,\n",
      "            0.08665885776281357,\n",
      "            -0.056561850011348724,\n",
      "            0.08651932328939438,\n",
      "            -0.021039007231593132,\n",
      "            0.13781747221946716,\n",
      "            -0.12332891672849655,\n",
      "            -0.05017941817641258,\n",
      "            -0.019001493230462074,\n",
      "            -0.06140769645571709,\n",
      "            -0.17355868220329285,\n",
      "            -0.06420055031776428,\n",
      "            -0.11997780203819275,\n",
      "            -0.017047803848981857,\n",
      "            0.12930236756801605,\n",
      "            -0.030655957758426666,\n",
      "            0.06852718442678452,\n",
      "            0.10506618022918701\n",
      "          ],\n",
      "          [\n",
      "            -0.013419132679700851,\n",
      "            -0.03195914998650551,\n",
      "            0.1515631526708603,\n",
      "            -0.15165172517299652,\n",
      "            0.11475703120231628,\n",
      "            0.1069028303027153,\n",
      "            -0.06354683637619019,\n",
      "            0.0378304161131382,\n",
      "            -0.14955246448516846,\n",
      "            -0.05084076523780823,\n",
      "            -0.02672131545841694,\n",
      "            0.032053664326667786,\n",
      "            0.014447725377976894,\n",
      "            -0.14087429642677307,\n",
      "            0.032643456012010574,\n",
      "            0.10267217457294464,\n",
      "            0.1291065216064453,\n",
      "            -0.17563508450984955,\n",
      "            -0.10377462208271027,\n",
      "            -0.16744017601013184,\n",
      "            0.07890520244836807,\n",
      "            -0.15913890302181244,\n",
      "            0.13635289669036865,\n",
      "            -0.13898490369319916,\n",
      "            0.09935014694929123,\n",
      "            0.08403266221284866,\n",
      "            -0.03813811391592026,\n",
      "            0.0027255003806203604,\n",
      "            -0.15150560438632965,\n",
      "            0.016323545947670937,\n",
      "            0.0014645763440057635,\n",
      "            -0.006313307210803032,\n",
      "            0.09910085052251816,\n",
      "            -0.0660574808716774,\n",
      "            -0.03619895875453949,\n",
      "            0.06790348887443542,\n",
      "            -0.10601762682199478,\n",
      "            -0.12697328627109528,\n",
      "            0.0883917585015297,\n",
      "            -0.0705307126045227,\n",
      "            0.10383326560258865,\n",
      "            -0.09333986788988113,\n",
      "            0.14573130011558533,\n",
      "            -0.09038593620061874,\n",
      "            0.05860074609518051,\n",
      "            0.09350942820310593,\n",
      "            -0.1286846101284027,\n",
      "            -0.12210146337747574,\n",
      "            0.061485663056373596,\n",
      "            0.1737491488456726,\n",
      "            -0.007343478500843048,\n",
      "            0.05683893710374832,\n",
      "            -0.027055300772190094,\n",
      "            0.05923664942383766,\n",
      "            0.10056374967098236,\n",
      "            -0.012190639041364193,\n",
      "            -0.1104954183101654,\n",
      "            0.06600335985422134,\n",
      "            -0.08932877331972122,\n",
      "            -0.11646772921085358,\n",
      "            0.04723069816827774,\n",
      "            -0.11215139180421829,\n",
      "            -0.048870813101530075,\n",
      "            0.05047836899757385,\n",
      "            0.0741695687174797,\n",
      "            -0.04188232868909836,\n",
      "            -0.15842735767364502,\n",
      "            -0.042895495891571045,\n",
      "            0.16044779121875763,\n",
      "            -0.057733360677957535,\n",
      "            0.05304040387272835,\n",
      "            0.038127366453409195,\n",
      "            -0.10399927198886871,\n",
      "            0.0815495178103447,\n",
      "            -0.10494232177734375,\n",
      "            0.14779751002788544,\n",
      "            0.05669091269373894,\n",
      "            0.023638255894184113,\n",
      "            0.10121427476406097,\n",
      "            -0.03249240294098854,\n",
      "            -0.0538446381688118,\n",
      "            -0.16064006090164185,\n",
      "            -0.01315181702375412,\n",
      "            0.06013520434498787,\n",
      "            -0.17587874829769135,\n",
      "            0.16640329360961914,\n",
      "            0.03652510419487953,\n",
      "            -0.13428373634815216,\n",
      "            -0.015974843874573708,\n",
      "            -0.16312308609485626,\n",
      "            0.11460336297750473,\n",
      "            0.021268149837851524,\n",
      "            -0.095865398645401,\n",
      "            -0.15912291407585144,\n",
      "            -0.022579887881875038,\n",
      "            -0.05953256040811539,\n",
      "            -0.1556745171546936,\n",
      "            0.08704601973295212,\n",
      "            -0.07385582476854324,\n",
      "            0.044827818870544434,\n",
      "            -0.06329605728387833,\n",
      "            -0.1589861363172531,\n",
      "            -0.14049050211906433,\n",
      "            0.12447904795408249,\n",
      "            0.1027975082397461,\n",
      "            0.02923537790775299,\n",
      "            0.04019302874803543,\n",
      "            -0.1684514433145523,\n",
      "            0.10136542469263077,\n",
      "            0.005986134521663189,\n",
      "            0.09590043127536774,\n",
      "            -0.04147985205054283,\n",
      "            -0.13329963386058807,\n",
      "            -0.17442408204078674,\n",
      "            -0.05759871006011963,\n",
      "            0.050028178840875626,\n",
      "            0.016650527715682983,\n",
      "            -0.14380331337451935,\n",
      "            -0.06905970722436905,\n",
      "            0.12520702183246613,\n",
      "            0.007399727590382099,\n",
      "            0.10201191157102585,\n",
      "            0.11955658346414566,\n",
      "            -0.0019103948725387454,\n",
      "            -0.14911822974681854,\n",
      "            0.04483607038855553,\n",
      "            0.06357842683792114,\n",
      "            0.09656412899494171\n",
      "          ],\n",
      "          [\n",
      "            -0.013569578528404236,\n",
      "            -0.17499923706054688,\n",
      "            0.254204124212265,\n",
      "            -0.16349732875823975,\n",
      "            -0.16825751960277557,\n",
      "            0.2203892022371292,\n",
      "            0.017783217132091522,\n",
      "            -0.1051277369260788,\n",
      "            -0.15365099906921387,\n",
      "            -0.01558700855821371,\n",
      "            -0.049519944936037064,\n",
      "            0.08516571670770645,\n",
      "            0.18861474096775055,\n",
      "            0.03410793095827103,\n",
      "            -0.09157788753509521,\n",
      "            -0.1372508704662323,\n",
      "            0.03842846676707268,\n",
      "            0.11034334450960159,\n",
      "            0.05077066272497177,\n",
      "            0.19494087994098663,\n",
      "            0.04089919477701187,\n",
      "            -0.027517490088939667,\n",
      "            0.010004005394876003,\n",
      "            -0.15842294692993164,\n",
      "            -0.055252693593502045,\n",
      "            0.16559875011444092,\n",
      "            0.22342270612716675,\n",
      "            0.10318498313426971,\n",
      "            0.06880489736795425,\n",
      "            -0.13103315234184265,\n",
      "            0.11828034371137619,\n",
      "            0.19598492980003357,\n",
      "            0.11777061969041824,\n",
      "            -0.14243900775909424,\n",
      "            0.12350316345691681,\n",
      "            0.10305619239807129,\n",
      "            0.014731117524206638,\n",
      "            0.12274612486362457,\n",
      "            -0.05784754827618599,\n",
      "            0.21925899386405945,\n",
      "            0.020808620378375053,\n",
      "            0.14526550471782684,\n",
      "            -0.11263364553451538,\n",
      "            -0.1548626571893692,\n",
      "            0.10091730207204819,\n",
      "            0.11402425915002823,\n",
      "            -0.243446484208107,\n",
      "            -0.06805095821619034,\n",
      "            -0.14052261412143707,\n",
      "            0.17941345274448395,\n",
      "            0.22331082820892334,\n",
      "            0.20324841141700745,\n",
      "            0.06476909667253494,\n",
      "            -0.09406253695487976,\n",
      "            0.07535246014595032,\n",
      "            0.17085866630077362,\n",
      "            0.1339912712574005,\n",
      "            0.15222643315792084,\n",
      "            0.06823138892650604,\n",
      "            0.23249119520187378,\n",
      "            0.18651948869228363,\n",
      "            0.10998012125492096,\n",
      "            -0.005947466474026442,\n",
      "            0.1576441526412964,\n",
      "            -0.2303759902715683,\n",
      "            0.10065650939941406,\n",
      "            0.14230971038341522,\n",
      "            0.050270453095436096,\n",
      "            0.07646051049232483,\n",
      "            0.14012108743190765,\n",
      "            0.05432886630296707,\n",
      "            0.16631852090358734,\n",
      "            0.06767221540212631,\n",
      "            0.10117797553539276,\n",
      "            -0.04124709218740463,\n",
      "            -0.04593919962644577,\n",
      "            -0.12464074045419693,\n",
      "            0.09280291944742203,\n",
      "            -0.08846598863601685,\n",
      "            0.23979473114013672,\n",
      "            -0.0012215826427564025,\n",
      "            -0.04281442612409592,\n",
      "            0.055594321340322495,\n",
      "            -0.062610924243927,\n",
      "            -0.11924485862255096,\n",
      "            0.009674974717199802,\n",
      "            0.16897259652614594,\n",
      "            0.09109921753406525,\n",
      "            0.11340581625699997,\n",
      "            0.02793363481760025,\n",
      "            -0.18317466974258423,\n",
      "            -0.06407302618026733,\n",
      "            -0.036561038345098495,\n",
      "            0.1702166348695755,\n",
      "            0.12055286020040512,\n",
      "            0.27615076303482056,\n",
      "            -0.08232542872428894,\n",
      "            0.08642672002315521,\n",
      "            0.05855717882514,\n",
      "            -0.1246357336640358,\n",
      "            0.03758277744054794,\n",
      "            -0.06195174530148506,\n",
      "            -0.03300909698009491,\n",
      "            0.013451699167490005,\n",
      "            -0.026636304333806038,\n",
      "            -0.11924219876527786,\n",
      "            -0.2168291062116623,\n",
      "            -0.06960383802652359,\n",
      "            0.15352767705917358,\n",
      "            -0.005046849604696035,\n",
      "            -0.1644895225763321,\n",
      "            0.1328059583902359,\n",
      "            -0.1596207618713379,\n",
      "            -0.18036246299743652,\n",
      "            0.17997679114341736,\n",
      "            -0.02051512338221073,\n",
      "            0.02085084468126297,\n",
      "            -0.21037177741527557,\n",
      "            0.1051875501871109,\n",
      "            -0.15540196001529694,\n",
      "            0.07970693707466125,\n",
      "            -0.1869582086801529,\n",
      "            0.027174226939678192,\n",
      "            0.1456909328699112,\n",
      "            -0.07289879024028778,\n",
      "            0.14742900431156158,\n",
      "            0.023193368688225746,\n",
      "            -0.1690824180841446\n",
      "          ],\n",
      "          [\n",
      "            0.14044572412967682,\n",
      "            -0.1526409536600113,\n",
      "            0.0035658564884215593,\n",
      "            -0.13673390448093414,\n",
      "            -0.058040544390678406,\n",
      "            0.14002516865730286,\n",
      "            0.005982386413961649,\n",
      "            0.1786383092403412,\n",
      "            0.1382255107164383,\n",
      "            -0.07883092761039734,\n",
      "            0.07605081796646118,\n",
      "            -0.03566008061170578,\n",
      "            -0.09103186428546906,\n",
      "            0.14221414923667908,\n",
      "            0.1650748997926712,\n",
      "            -0.006172657012939453,\n",
      "            0.15641705691814423,\n",
      "            -0.05442310869693756,\n",
      "            -0.023874761536717415,\n",
      "            0.12325755506753922,\n",
      "            0.08578174561262131,\n",
      "            -0.12826655805110931,\n",
      "            0.07064587622880936,\n",
      "            0.0008561513386666775,\n",
      "            -0.08098169416189194,\n",
      "            -0.08464725315570831,\n",
      "            -0.01904984377324581,\n",
      "            0.06725779920816422,\n",
      "            0.10732989758253098,\n",
      "            0.007462203036993742,\n",
      "            -0.051592934876680374,\n",
      "            -0.0015185201773419976,\n",
      "            0.1180500015616417,\n",
      "            0.05093049257993698,\n",
      "            0.04627572000026703,\n",
      "            0.021038340404629707,\n",
      "            -0.017095619812607765,\n",
      "            0.1503072828054428,\n",
      "            -0.16301217675209045,\n",
      "            -0.14564968645572662,\n",
      "            0.07637245208024979,\n",
      "            -0.03832000866532326,\n",
      "            0.0025649708695709705,\n",
      "            -0.12428224831819534,\n",
      "            -0.15843847393989563,\n",
      "            0.0836762934923172,\n",
      "            0.12219821661710739,\n",
      "            0.11779280751943588,\n",
      "            -0.08144991099834442,\n",
      "            -0.002993789967149496,\n",
      "            -0.1021704226732254,\n",
      "            0.04057738929986954,\n",
      "            0.11151628196239471,\n",
      "            0.13187702000141144,\n",
      "            0.04353545233607292,\n",
      "            -0.05331200733780861,\n",
      "            0.08784829080104828,\n",
      "            0.06874870508909225,\n",
      "            0.16281616687774658,\n",
      "            0.0775148794054985,\n",
      "            0.12151303142309189,\n",
      "            -0.11808031797409058,\n",
      "            0.17746561765670776,\n",
      "            -0.07793684303760529,\n",
      "            0.0073420461267232895,\n",
      "            0.0974075123667717,\n",
      "            -0.14486214518547058,\n",
      "            -0.12871594727039337,\n",
      "            -0.05290070176124573,\n",
      "            0.080850750207901,\n",
      "            -0.162845641374588,\n",
      "            0.030765270814299583,\n",
      "            0.14333496987819672,\n",
      "            -0.098966084420681,\n",
      "            -0.15542443096637726,\n",
      "            -0.19615235924720764,\n",
      "            0.08151756972074509,\n",
      "            0.03491556644439697,\n",
      "            -0.0016685271402820945,\n",
      "            -0.005856458563357592,\n",
      "            0.01561497151851654,\n",
      "            -0.03293420374393463,\n",
      "            -0.02726844884455204,\n",
      "            -0.01430997159332037,\n",
      "            -0.14127296209335327,\n",
      "            0.10061652213335037,\n",
      "            0.15101365745067596,\n",
      "            -0.09653354436159134,\n",
      "            -0.03129635006189346,\n",
      "            0.05524037033319473,\n",
      "            0.16428698599338531,\n",
      "            -0.16853801906108856,\n",
      "            0.030868737027049065,\n",
      "            0.11838096380233765,\n",
      "            0.0010806912323459983,\n",
      "            0.04173325002193451,\n",
      "            0.05098577216267586,\n",
      "            -0.07250736653804779,\n",
      "            -0.08802090585231781,\n",
      "            -0.08896536380052567,\n",
      "            0.1867736428976059,\n",
      "            -0.07724978774785995,\n",
      "            -0.17397424578666687,\n",
      "            0.11040494590997696,\n",
      "            0.1736714094877243,\n",
      "            -0.1549469381570816,\n",
      "            -0.048877645283937454,\n",
      "            -0.07074679434299469,\n",
      "            0.07584553211927414,\n",
      "            0.025659698992967606,\n",
      "            -0.025661731138825417,\n",
      "            0.14204518496990204,\n",
      "            -0.0022624810226261616,\n",
      "            0.06504008919000626,\n",
      "            0.1126372218132019,\n",
      "            0.03385106101632118,\n",
      "            0.01366312988102436,\n",
      "            -0.01124071329832077,\n",
      "            -0.15899141132831573,\n",
      "            0.1141156554222107,\n",
      "            0.08772081881761551,\n",
      "            -0.142730712890625,\n",
      "            -0.08586035668849945,\n",
      "            -0.09338556975126266,\n",
      "            -0.13516254723072052,\n",
      "            0.10759350657463074,\n",
      "            -0.10267560929059982,\n",
      "            -0.0956665650010109\n",
      "          ],\n",
      "          [\n",
      "            0.11705896258354187,\n",
      "            0.06723422557115555,\n",
      "            0.0278791431337595,\n",
      "            0.03609762340784073,\n",
      "            -0.0034184404648840427,\n",
      "            0.019821174442768097,\n",
      "            0.13421832025051117,\n",
      "            -0.06024869531393051,\n",
      "            -0.04745188727974892,\n",
      "            -0.017864027991890907,\n",
      "            -0.12416684627532959,\n",
      "            -0.07551951706409454,\n",
      "            -0.04690464213490486,\n",
      "            -0.10679604858160019,\n",
      "            0.1177309975028038,\n",
      "            0.12644825875759125,\n",
      "            0.004607244860380888,\n",
      "            -0.08570107817649841,\n",
      "            0.16030475497245789,\n",
      "            -0.03868124634027481,\n",
      "            0.11196411401033401,\n",
      "            -0.04398423060774803,\n",
      "            -0.10117241740226746,\n",
      "            -0.023345110937952995,\n",
      "            -0.02379736118018627,\n",
      "            -0.10591894388198853,\n",
      "            -0.024178288877010345,\n",
      "            0.03011460416018963,\n",
      "            -0.005244259722530842,\n",
      "            0.009668540209531784,\n",
      "            0.05422414466738701,\n",
      "            0.048401929438114166,\n",
      "            0.07746143639087677,\n",
      "            0.14983226358890533,\n",
      "            -0.11187775433063507,\n",
      "            0.05114114284515381,\n",
      "            0.10666420310735703,\n",
      "            0.03706410899758339,\n",
      "            -0.13802561163902283,\n",
      "            -0.16587834060192108,\n",
      "            0.13433323800563812,\n",
      "            0.12955671548843384,\n",
      "            -0.11727754026651382,\n",
      "            0.05339443311095238,\n",
      "            -0.03617072477936745,\n",
      "            -0.16907738149166107,\n",
      "            0.003357558511197567,\n",
      "            0.02684379741549492,\n",
      "            -0.0936453640460968,\n",
      "            -0.0905337929725647,\n",
      "            -0.09675877541303635,\n",
      "            -0.062212999910116196,\n",
      "            0.10658638924360275,\n",
      "            -0.14063182473182678,\n",
      "            0.05775715410709381,\n",
      "            -0.07612357288599014,\n",
      "            0.0029142932035028934,\n",
      "            -0.054109763354063034,\n",
      "            0.12108495086431503,\n",
      "            0.045808058232069016,\n",
      "            0.15148264169692993,\n",
      "            0.11391274631023407,\n",
      "            -0.08793196082115173,\n",
      "            -0.08262074738740921,\n",
      "            -0.08386412262916565,\n",
      "            0.10395700484514236,\n",
      "            0.17419667541980743,\n",
      "            -0.079620860517025,\n",
      "            0.17030887305736542,\n",
      "            0.0632544457912445,\n",
      "            0.0641745999455452,\n",
      "            0.057504478842020035,\n",
      "            -0.11351887881755829,\n",
      "            -0.11338966339826584,\n",
      "            -0.14579913020133972,\n",
      "            0.04004859924316406,\n",
      "            -0.1693996787071228,\n",
      "            -0.1211373507976532,\n",
      "            -0.003882640041410923,\n",
      "            -0.06839673221111298,\n",
      "            -0.1638069897890091,\n",
      "            0.056122180074453354,\n",
      "            0.06906372308731079,\n",
      "            -0.13075979053974152,\n",
      "            0.08284220844507217,\n",
      "            -0.0011715823784470558,\n",
      "            -0.03437953069806099,\n",
      "            0.17064182460308075,\n",
      "            -0.11044774949550629,\n",
      "            0.1497461050748825,\n",
      "            -0.12204646319150925,\n",
      "            0.1392544060945511,\n",
      "            0.035547737032175064,\n",
      "            0.12793253362178802,\n",
      "            0.054264213889837265,\n",
      "            0.025060465559363365,\n",
      "            0.10331635177135468,\n",
      "            -0.07162841409444809,\n",
      "            -0.09155149012804031,\n",
      "            -0.07219309359788895,\n",
      "            -0.0032459988724440336,\n",
      "            -0.03978986293077469,\n",
      "            0.15799403190612793,\n",
      "            0.05887436866760254,\n",
      "            0.13735432922840118,\n",
      "            0.04334963858127594,\n",
      "            -0.13621823489665985,\n",
      "            -0.06932393461465836,\n",
      "            0.12791787087917328,\n",
      "            -0.05070874094963074,\n",
      "            0.045705877244472504,\n",
      "            -0.008680139668285847,\n",
      "            -0.08352141827344894,\n",
      "            -0.16558101773262024,\n",
      "            -0.031165553256869316,\n",
      "            0.08573716878890991,\n",
      "            0.03763788938522339,\n",
      "            -0.008335859514772892,\n",
      "            0.153804749250412,\n",
      "            0.16008047759532928,\n",
      "            -0.07827980071306229,\n",
      "            0.14211149513721466,\n",
      "            -0.0487421415746212,\n",
      "            -0.15758702158927917,\n",
      "            -0.1712374985218048,\n",
      "            0.10441349446773529,\n",
      "            -0.10194841772317886,\n",
      "            0.062397606670856476\n",
      "          ],\n",
      "          [\n",
      "            0.19114451110363007,\n",
      "            -0.10719236731529236,\n",
      "            -0.24630288779735565,\n",
      "            -0.14063739776611328,\n",
      "            -0.10700216889381409,\n",
      "            0.1541389375925064,\n",
      "            -0.1042480319738388,\n",
      "            0.08581043034791946,\n",
      "            -0.013911571353673935,\n",
      "            0.08291944116353989,\n",
      "            -0.0998266190290451,\n",
      "            0.12423722445964813,\n",
      "            0.05758729577064514,\n",
      "            0.09395836293697357,\n",
      "            -0.06537806987762451,\n",
      "            -0.06774140894412994,\n",
      "            -0.01728319562971592,\n",
      "            -0.1087365597486496,\n",
      "            0.1106342077255249,\n",
      "            -0.1744534820318222,\n",
      "            -0.2065243124961853,\n",
      "            -0.13030403852462769,\n",
      "            0.06779471039772034,\n",
      "            0.14651645720005035,\n",
      "            -0.10473231226205826,\n",
      "            -0.014440582133829594,\n",
      "            -0.20309796929359436,\n",
      "            -0.09872674942016602,\n",
      "            -0.07436322420835495,\n",
      "            -0.12241847068071365,\n",
      "            0.10211296379566193,\n",
      "            -0.11334257572889328,\n",
      "            -0.11985035985708237,\n",
      "            -0.06985768675804138,\n",
      "            -0.092509426176548,\n",
      "            0.10937462002038956,\n",
      "            0.03331291303038597,\n",
      "            0.09869357943534851,\n",
      "            -0.043559957295656204,\n",
      "            0.12193676084280014,\n",
      "            -0.03323577344417572,\n",
      "            -0.028758026659488678,\n",
      "            -0.03340580314397812,\n",
      "            -0.1011626124382019,\n",
      "            -0.10670316219329834,\n",
      "            -0.1059350073337555,\n",
      "            0.26021552085876465,\n",
      "            -0.0021996540017426014,\n",
      "            -0.0767446905374527,\n",
      "            -0.07703323662281036,\n",
      "            -0.007782858796417713,\n",
      "            -0.07742542028427124,\n",
      "            0.08276238292455673,\n",
      "            0.1266712248325348,\n",
      "            0.05062631890177727,\n",
      "            -0.08786281943321228,\n",
      "            0.05134269595146179,\n",
      "            -0.08001004159450531,\n",
      "            0.09847331047058105,\n",
      "            0.03582306206226349,\n",
      "            -0.05961376801133156,\n",
      "            -0.025840241461992264,\n",
      "            0.1331903487443924,\n",
      "            -0.10211003571748734,\n",
      "            0.1543145626783371,\n",
      "            -0.08976147323846817,\n",
      "            0.08233499526977539,\n",
      "            -0.09941698610782623,\n",
      "            -0.06333403289318085,\n",
      "            -0.054581645876169205,\n",
      "            0.09289538115262985,\n",
      "            -0.18514607846736908,\n",
      "            0.055120665580034256,\n",
      "            -0.2345307320356369,\n",
      "            -0.16616256535053253,\n",
      "            0.1118752732872963,\n",
      "            -0.017993062734603882,\n",
      "            0.18116053938865662,\n",
      "            -0.08236324787139893,\n",
      "            -0.08286570757627487,\n",
      "            -0.08509626239538193,\n",
      "            -0.16904525458812714,\n",
      "            -0.188272163271904,\n",
      "            0.12849682569503784,\n",
      "            0.01017245277762413,\n",
      "            -0.0022776927798986435,\n",
      "            0.061562132090330124,\n",
      "            0.010806132107973099,\n",
      "            -0.11448271572589874,\n",
      "            0.11712914705276489,\n",
      "            -0.019571950659155846,\n",
      "            -0.1315888613462448,\n",
      "            -0.07600676268339157,\n",
      "            -0.1880144476890564,\n",
      "            0.06289073079824448,\n",
      "            -0.11076531559228897,\n",
      "            0.2452501654624939,\n",
      "            -0.2931630611419678,\n",
      "            0.031074313446879387,\n",
      "            -0.10644305497407913,\n",
      "            0.06644169986248016,\n",
      "            -0.11671417951583862,\n",
      "            -0.1747361719608307,\n",
      "            0.01685516908764839,\n",
      "            0.15042021870613098,\n",
      "            -0.13928964734077454,\n",
      "            0.08834390342235565,\n",
      "            0.1508122831583023,\n",
      "            -0.07148411124944687,\n",
      "            0.08004029095172882,\n",
      "            -0.1043161004781723,\n",
      "            -0.008540541864931583,\n",
      "            -0.015170353464782238,\n",
      "            0.20453716814517975,\n",
      "            -0.03616922348737717,\n",
      "            0.06207997724413872,\n",
      "            0.12900297343730927,\n",
      "            -0.0822046771645546,\n",
      "            -0.023381680250167847,\n",
      "            -0.07463627308607101,\n",
      "            -0.1316845417022705,\n",
      "            0.11188896745443344,\n",
      "            -0.19174028933048248,\n",
      "            -0.16895617544651031,\n",
      "            0.08278355002403259,\n",
      "            -0.04511113092303276,\n",
      "            0.04015302658081055,\n",
      "            -0.030367311090230942\n",
      "          ],\n",
      "          [\n",
      "            0.04490233585238457,\n",
      "            -0.0664810985326767,\n",
      "            0.21200017631053925,\n",
      "            -0.1338752806186676,\n",
      "            0.14423027634620667,\n",
      "            0.06054035946726799,\n",
      "            -0.09676741063594818,\n",
      "            0.035561274737119675,\n",
      "            -0.1592392772436142,\n",
      "            0.06703265756368637,\n",
      "            -0.12626372277736664,\n",
      "            -0.029734568670392036,\n",
      "            0.19630110263824463,\n",
      "            0.10940098762512207,\n",
      "            0.05222324654459953,\n",
      "            -0.16979338228702545,\n",
      "            -0.013951526954770088,\n",
      "            -0.033737294375896454,\n",
      "            -0.04508243873715401,\n",
      "            -0.0028245432768017054,\n",
      "            0.15013308823108673,\n",
      "            -0.07718174904584885,\n",
      "            -0.032234031707048416,\n",
      "            0.003534469520673156,\n",
      "            0.1695077270269394,\n",
      "            -0.055142201483249664,\n",
      "            -0.14180971682071686,\n",
      "            -0.006338676903396845,\n",
      "            0.03960657864809036,\n",
      "            0.0723530575633049,\n",
      "            -0.15394306182861328,\n",
      "            0.18724510073661804,\n",
      "            -0.11513177305459976,\n",
      "            0.028418181464076042,\n",
      "            0.14767040312290192,\n",
      "            -0.004272479098290205,\n",
      "            -0.05021879822015762,\n",
      "            0.15092666447162628,\n",
      "            0.08618021756410599,\n",
      "            0.17963922023773193,\n",
      "            0.22657272219657898,\n",
      "            0.0493229515850544,\n",
      "            -0.02096523717045784,\n",
      "            0.17613548040390015,\n",
      "            0.08511313050985336,\n",
      "            0.0765567496418953,\n",
      "            -0.014186214655637741,\n",
      "            0.08088906109333038,\n",
      "            0.15698359906673431,\n",
      "            0.10941709578037262,\n",
      "            -0.07517778873443604,\n",
      "            0.047705311328172684,\n",
      "            -0.11266704648733139,\n",
      "            -0.13334138691425323,\n",
      "            -0.013593057170510292,\n",
      "            0.23421163856983185,\n",
      "            -0.06154124066233635,\n",
      "            -0.005771650467067957,\n",
      "            -0.10289318114519119,\n",
      "            0.0554695725440979,\n",
      "            -0.03154434636235237,\n",
      "            -0.11054502427577972,\n",
      "            -0.09259697794914246,\n",
      "            0.1242019459605217,\n",
      "            -0.2230045050382614,\n",
      "            0.04147949442267418,\n",
      "            0.07619648426771164,\n",
      "            0.1564093679189682,\n",
      "            0.08852890878915787,\n",
      "            0.1508144736289978,\n",
      "            -0.09796561300754547,\n",
      "            0.15192911028862,\n",
      "            -0.14075610041618347,\n",
      "            0.11239883303642273,\n",
      "            -0.016236038878560066,\n",
      "            0.13022936880588531,\n",
      "            0.15196001529693604,\n",
      "            -0.05162317678332329,\n",
      "            0.08046101033687592,\n",
      "            -0.001035370514728129,\n",
      "            0.1914338618516922,\n",
      "            0.1443672925233841,\n",
      "            -0.024934858083724976,\n",
      "            0.08987677097320557,\n",
      "            0.13426212966442108,\n",
      "            0.20582643151283264,\n",
      "            0.071602463722229,\n",
      "            -0.06983279436826706,\n",
      "            0.2123701423406601,\n",
      "            -0.041843049228191376,\n",
      "            0.026417123153805733,\n",
      "            0.04838677868247032,\n",
      "            -0.0673043355345726,\n",
      "            -0.00321812741458416,\n",
      "            0.1426599621772766,\n",
      "            0.15579794347286224,\n",
      "            -0.11069846898317337,\n",
      "            0.09240224957466125,\n",
      "            0.17944885790348053,\n",
      "            -0.050142351537942886,\n",
      "            -0.054913513362407684,\n",
      "            0.09575765579938889,\n",
      "            0.09590369462966919,\n",
      "            -0.07753492146730423,\n",
      "            -0.10095309466123581,\n",
      "            0.018999159336090088,\n",
      "            -0.0930456593632698,\n",
      "            -0.07064367085695267,\n",
      "            -0.1410713642835617,\n",
      "            0.1252625435590744,\n",
      "            -0.017280643805861473,\n",
      "            -0.09046917408704758,\n",
      "            0.04257195442914963,\n",
      "            -0.04869775101542473,\n",
      "            0.12787547707557678,\n",
      "            -0.10949256271123886,\n",
      "            0.128143772482872,\n",
      "            -0.177147775888443,\n",
      "            0.0722949430346489,\n",
      "            0.036351341754198074,\n",
      "            -0.0009386599995195866,\n",
      "            0.07301024347543716,\n",
      "            0.17043542861938477,\n",
      "            -0.04747448489069939,\n",
      "            0.024157298728823662,\n",
      "            -0.027193456888198853,\n",
      "            0.07629498094320297,\n",
      "            0.008118392899632454\n",
      "          ],\n",
      "          [\n",
      "            0.016466284170746803,\n",
      "            0.1509115844964981,\n",
      "            0.09001584351062775,\n",
      "            0.04517320170998573,\n",
      "            -0.09739083796739578,\n",
      "            -0.14699777960777283,\n",
      "            0.013253550045192242,\n",
      "            0.12197978049516678,\n",
      "            -0.024178937077522278,\n",
      "            -0.03236515074968338,\n",
      "            0.09467101842164993,\n",
      "            -0.048244476318359375,\n",
      "            0.005992553196847439,\n",
      "            0.029665393754839897,\n",
      "            -0.10760463774204254,\n",
      "            0.15285460650920868,\n",
      "            -0.12163686007261276,\n",
      "            0.00792605895549059,\n",
      "            0.1606416255235672,\n",
      "            -0.008822763338685036,\n",
      "            0.004103650338947773,\n",
      "            -0.017634691670536995,\n",
      "            -0.017143303528428078,\n",
      "            0.07696301490068436,\n",
      "            -0.019912730902433395,\n",
      "            0.05042039975523949,\n",
      "            0.05811958387494087,\n",
      "            0.12480729073286057,\n",
      "            -0.0006482073222286999,\n",
      "            0.0362069234251976,\n",
      "            -0.07805764675140381,\n",
      "            0.13587629795074463,\n",
      "            0.03272758424282074,\n",
      "            -0.04403385519981384,\n",
      "            0.0685419887304306,\n",
      "            -0.10436099767684937,\n",
      "            -0.03754562884569168,\n",
      "            -0.1347675919532776,\n",
      "            -0.16301321983337402,\n",
      "            0.015149866230785847,\n",
      "            -0.19571775197982788,\n",
      "            -0.06631997972726822,\n",
      "            -0.17169320583343506,\n",
      "            0.11752813309431076,\n",
      "            0.002002549357712269,\n",
      "            0.0357046015560627,\n",
      "            -0.007604595273733139,\n",
      "            -0.0024727745912969112,\n",
      "            0.037571411579847336,\n",
      "            -0.015554053708910942,\n",
      "            -0.023793624714016914,\n",
      "            0.08019652217626572,\n",
      "            0.04919153451919556,\n",
      "            0.18677814304828644,\n",
      "            0.17495952546596527,\n",
      "            -0.1905645877122879,\n",
      "            0.11369753628969193,\n",
      "            -0.11249835044145584,\n",
      "            -0.03541138768196106,\n",
      "            -0.04133066534996033,\n",
      "            0.08793351799249649,\n",
      "            -0.08102172613143921,\n",
      "            0.054747406393289566,\n",
      "            -0.187972292304039,\n",
      "            0.20230689644813538,\n",
      "            -0.01582307554781437,\n",
      "            -0.018812425434589386,\n",
      "            -0.04825610667467117,\n",
      "            0.13291700184345245,\n",
      "            0.06376916915178299,\n",
      "            -0.18396782875061035,\n",
      "            -0.02037680335342884,\n",
      "            0.17228497564792633,\n",
      "            -0.13323241472244263,\n",
      "            0.09467615932226181,\n",
      "            0.09599389880895615,\n",
      "            -0.008897772058844566,\n",
      "            0.13602708280086517,\n",
      "            -0.14321358501911163,\n",
      "            0.034729741513729095,\n",
      "            -0.18608880043029785,\n",
      "            -0.14249232411384583,\n",
      "            -0.04595193639397621,\n",
      "            0.15444283187389374,\n",
      "            -0.08233346045017242,\n",
      "            0.0196066964417696,\n",
      "            -0.17185775935649872,\n",
      "            -0.044427137821912766,\n",
      "            -0.1366320252418518,\n",
      "            0.06101791560649872,\n",
      "            0.1269373744726181,\n",
      "            -0.08782035857439041,\n",
      "            0.11207704246044159,\n",
      "            -0.12549374997615814,\n",
      "            -0.028637435287237167,\n",
      "            -0.1284290999174118,\n",
      "            0.059457533061504364,\n",
      "            -0.16067487001419067,\n",
      "            -0.011686544865369797,\n",
      "            -0.013120947405695915,\n",
      "            0.17010653018951416,\n",
      "            0.15731123089790344,\n",
      "            -0.1787712424993515,\n",
      "            0.07737261056900024,\n",
      "            -0.03558291867375374,\n",
      "            0.03981725871562958,\n",
      "            0.19884969294071198,\n",
      "            -0.1235785111784935,\n",
      "            0.06631384789943695,\n",
      "            -0.011414376087486744,\n",
      "            -0.021768804639577866,\n",
      "            0.11862151324748993,\n",
      "            0.13523873686790466,\n",
      "            -0.11161462217569351,\n",
      "            -0.022204602137207985,\n",
      "            -0.1368628442287445,\n",
      "            -0.13517695665359497,\n",
      "            -0.14594054222106934,\n",
      "            -0.12044630199670792,\n",
      "            0.04677063226699829,\n",
      "            -0.10650394856929779,\n",
      "            0.15708908438682556,\n",
      "            0.15501639246940613,\n",
      "            0.0014722200576215982,\n",
      "            0.18820026516914368,\n",
      "            -0.08647990226745605,\n",
      "            -0.02638041041791439,\n",
      "            0.15753847360610962\n",
      "          ],\n",
      "          [\n",
      "            0.04740335792303085,\n",
      "            -0.07513854652643204,\n",
      "            -0.027080202475190163,\n",
      "            0.08903501182794571,\n",
      "            -0.004662750754505396,\n",
      "            -0.029704637825489044,\n",
      "            0.18184210360050201,\n",
      "            -0.019461963325738907,\n",
      "            -0.033834900707006454,\n",
      "            -0.13250871002674103,\n",
      "            -0.08591856807470322,\n",
      "            -0.008455220609903336,\n",
      "            0.010892020538449287,\n",
      "            0.08275433629751205,\n",
      "            0.11745434254407883,\n",
      "            -0.11949706077575684,\n",
      "            0.10175060480833054,\n",
      "            0.09738604724407196,\n",
      "            -0.03498532250523567,\n",
      "            0.09988968819379807,\n",
      "            0.04689802974462509,\n",
      "            -0.007162576541304588,\n",
      "            0.09488556534051895,\n",
      "            0.08487778902053833,\n",
      "            0.16634894907474518,\n",
      "            -0.028041565790772438,\n",
      "            -0.1758817881345749,\n",
      "            0.1658197045326233,\n",
      "            0.013324267230927944,\n",
      "            -0.1580471247434616,\n",
      "            0.1614948809146881,\n",
      "            -0.08319967240095139,\n",
      "            0.09629658609628677,\n",
      "            0.1651161164045334,\n",
      "            -0.17080464959144592,\n",
      "            0.03038070723414421,\n",
      "            -0.1695198118686676,\n",
      "            -0.053742364048957825,\n",
      "            -0.04462491720914841,\n",
      "            -0.06856909394264221,\n",
      "            0.015194125473499298,\n",
      "            0.155622661113739,\n",
      "            -0.153142049908638,\n",
      "            -3.0002851417521015e-05,\n",
      "            -0.10975035279989243,\n",
      "            -0.030205314978957176,\n",
      "            0.079794742166996,\n",
      "            -0.04728692024946213,\n",
      "            0.09112519770860672,\n",
      "            0.053399667143821716,\n",
      "            -0.11099778860807419,\n",
      "            -0.12356334924697876,\n",
      "            0.07973487675189972,\n",
      "            0.14436066150665283,\n",
      "            -0.08683019876480103,\n",
      "            0.061412036418914795,\n",
      "            -0.1682789921760559,\n",
      "            -0.16041699051856995,\n",
      "            -0.12435484677553177,\n",
      "            -0.12883570790290833,\n",
      "            0.1758129894733429,\n",
      "            0.07102842628955841,\n",
      "            0.14187869429588318,\n",
      "            -0.04431626945734024,\n",
      "            -0.06981024891138077,\n",
      "            -0.0949118360877037,\n",
      "            -0.12618543207645416,\n",
      "            -0.12199511379003525,\n",
      "            0.13693957030773163,\n",
      "            0.03846351057291031,\n",
      "            0.1454656422138214,\n",
      "            -0.1484878957271576,\n",
      "            -0.0850016176700592,\n",
      "            0.030138587579131126,\n",
      "            -0.16141311824321747,\n",
      "            -0.14217643439769745,\n",
      "            -0.06900472193956375,\n",
      "            -0.04560743644833565,\n",
      "            0.02387743443250656,\n",
      "            0.09989431500434875,\n",
      "            -0.12750229239463806,\n",
      "            -0.12861065566539764,\n",
      "            -0.07820164412260056,\n",
      "            -0.03558216989040375,\n",
      "            -0.13192185759544373,\n",
      "            0.1565445363521576,\n",
      "            -0.11669619381427765,\n",
      "            0.057171065360307693,\n",
      "            0.1724996715784073,\n",
      "            0.0362786203622818,\n",
      "            -0.08962231129407883,\n",
      "            0.06585626304149628,\n",
      "            -0.029820920899510384,\n",
      "            0.051821861416101456,\n",
      "            0.08506851643323898,\n",
      "            0.11273475736379623,\n",
      "            0.10882403701543808,\n",
      "            0.11528128385543823,\n",
      "            -0.1587994247674942,\n",
      "            0.015314829535782337,\n",
      "            0.05753934755921364,\n",
      "            -0.05415835231542587,\n",
      "            0.12904158234596252,\n",
      "            -0.17373323440551758,\n",
      "            -0.06362506747245789,\n",
      "            -0.09524087607860565,\n",
      "            0.02026771567761898,\n",
      "            0.12016656249761581,\n",
      "            -0.03547995910048485,\n",
      "            -0.0015562762273475528,\n",
      "            0.01056533120572567,\n",
      "            -0.04627593234181404,\n",
      "            0.056614454835653305,\n",
      "            0.12875856459140778,\n",
      "            -0.07882995158433914,\n",
      "            0.14157389104366302,\n",
      "            -0.1389511078596115,\n",
      "            -0.024426380172371864,\n",
      "            -0.05589160695672035,\n",
      "            0.06524960696697235,\n",
      "            -0.01626606471836567,\n",
      "            0.10051177442073822,\n",
      "            -0.1581377536058426,\n",
      "            -0.14944961667060852,\n",
      "            0.06197940185666084,\n",
      "            0.04331062734127045,\n",
      "            -0.12030407041311264,\n",
      "            0.18814221024513245\n",
      "          ],\n",
      "          [\n",
      "            -0.0012989103561267257,\n",
      "            0.011742032133042812,\n",
      "            0.17686745524406433,\n",
      "            0.06810873001813889,\n",
      "            -0.058789752423763275,\n",
      "            0.060122743248939514,\n",
      "            0.001655255095101893,\n",
      "            -0.1433839499950409,\n",
      "            -0.11788603663444519,\n",
      "            -0.06595385074615479,\n",
      "            0.03611236438155174,\n",
      "            -0.06972359865903854,\n",
      "            -0.091517373919487,\n",
      "            -0.015342667698860168,\n",
      "            0.1085432767868042,\n",
      "            -0.022440075874328613,\n",
      "            0.004063552711158991,\n",
      "            -0.11161830276250839,\n",
      "            -0.03437245637178421,\n",
      "            0.13915537297725677,\n",
      "            0.10647209733724594,\n",
      "            0.012995650060474873,\n",
      "            -0.08150855451822281,\n",
      "            0.08921302855014801,\n",
      "            -0.042421258985996246,\n",
      "            0.17076244950294495,\n",
      "            0.15882015228271484,\n",
      "            -0.022220708429813385,\n",
      "            -0.011175607331097126,\n",
      "            -0.021650591865181923,\n",
      "            -0.09632321447134018,\n",
      "            0.04316771775484085,\n",
      "            0.13339240849018097,\n",
      "            0.04211227968335152,\n",
      "            -0.04728066921234131,\n",
      "            0.051652416586875916,\n",
      "            0.15742500126361847,\n",
      "            -0.13254445791244507,\n",
      "            0.13707321882247925,\n",
      "            0.16520409286022186,\n",
      "            -0.010915718972682953,\n",
      "            -0.012177259661257267,\n",
      "            -0.055678825825452805,\n",
      "            -0.12901927530765533,\n",
      "            0.1134246215224266,\n",
      "            -0.04723703861236572,\n",
      "            0.05492807552218437,\n",
      "            0.06846525520086288,\n",
      "            0.08981052786111832,\n",
      "            0.103424072265625,\n",
      "            -0.061855703592300415,\n",
      "            -0.1260441392660141,\n",
      "            -0.06244507431983948,\n",
      "            -0.1276537925004959,\n",
      "            -0.0065850382670760155,\n",
      "            -0.017095176503062248,\n",
      "            0.16696715354919434,\n",
      "            -0.14194253087043762,\n",
      "            0.06829068809747696,\n",
      "            -0.02281445451080799,\n",
      "            0.12165800482034683,\n",
      "            0.029531503096222878,\n",
      "            0.028388723731040955,\n",
      "            -0.08551008254289627,\n",
      "            -0.05758064240217209,\n",
      "            0.17409095168113708,\n",
      "            -0.016626747325062752,\n",
      "            0.17752626538276672,\n",
      "            -0.09347420930862427,\n",
      "            -0.04471917822957039,\n",
      "            -0.14873434603214264,\n",
      "            -0.012035475112497807,\n",
      "            -0.021146677434444427,\n",
      "            0.09910164028406143,\n",
      "            0.09341586381196976,\n",
      "            -0.015166941098868847,\n",
      "            0.15862922370433807,\n",
      "            0.10076350718736649,\n",
      "            -0.03557658568024635,\n",
      "            0.18398454785346985,\n",
      "            -0.06524255871772766,\n",
      "            -0.16074120998382568,\n",
      "            -0.06559809297323227,\n",
      "            0.04249566048383713,\n",
      "            0.09289124608039856,\n",
      "            -0.12241124361753464,\n",
      "            0.04517838731408119,\n",
      "            0.1717536747455597,\n",
      "            0.15035077929496765,\n",
      "            0.1281326413154602,\n",
      "            0.08389071375131607,\n",
      "            0.02893136814236641,\n",
      "            0.17029958963394165,\n",
      "            -0.11745914071798325,\n",
      "            0.017197322100400925,\n",
      "            -0.0891554057598114,\n",
      "            -0.021450553089380264,\n",
      "            -0.11184004694223404,\n",
      "            0.015562488697469234,\n",
      "            0.05995412543416023,\n",
      "            0.1219160258769989,\n",
      "            0.13847361505031586,\n",
      "            0.06333163380622864,\n",
      "            0.014321948401629925,\n",
      "            0.15592625737190247,\n",
      "            -0.1679033488035202,\n",
      "            -0.08461105823516846,\n",
      "            -0.09635713696479797,\n",
      "            0.03738866373896599,\n",
      "            0.027674635872244835,\n",
      "            0.12218590080738068,\n",
      "            -0.16967259347438812,\n",
      "            0.08143866062164307,\n",
      "            0.1247963011264801,\n",
      "            0.09646189957857132,\n",
      "            0.18763886392116547,\n",
      "            0.14806880056858063,\n",
      "            0.025064175948500633,\n",
      "            0.10492779314517975,\n",
      "            0.011479627341032028,\n",
      "            0.16543267667293549,\n",
      "            -0.13437196612358093,\n",
      "            0.01095788087695837,\n",
      "            0.1467362493276596,\n",
      "            -0.04789845645427704,\n",
      "            0.09345439821481705,\n",
      "            -0.04281887784600258,\n",
      "            -0.05824611708521843\n",
      "          ],\n",
      "          [\n",
      "            -0.09824588894844055,\n",
      "            -0.008474248461425304,\n",
      "            -0.23598195612430573,\n",
      "            0.08506663888692856,\n",
      "            -0.026040643453598022,\n",
      "            -0.09415420144796371,\n",
      "            0.05053352192044258,\n",
      "            -0.008764862082898617,\n",
      "            0.16953638195991516,\n",
      "            -0.14421233534812927,\n",
      "            0.13161447644233704,\n",
      "            -0.14114297926425934,\n",
      "            0.006254254840314388,\n",
      "            0.16330716013908386,\n",
      "            0.020226242020726204,\n",
      "            0.13265462219715118,\n",
      "            0.13963724672794342,\n",
      "            0.016609003767371178,\n",
      "            -0.0039078835397958755,\n",
      "            -0.21444688737392426,\n",
      "            -0.18193751573562622,\n",
      "            0.12454497069120407,\n",
      "            0.007371614687144756,\n",
      "            -0.15200379490852356,\n",
      "            -0.010551265440881252,\n",
      "            0.12252471596002579,\n",
      "            -0.1382865011692047,\n",
      "            0.030500764027237892,\n",
      "            0.18986068665981293,\n",
      "            -0.16089147329330444,\n",
      "            0.06658811122179031,\n",
      "            0.07623299211263657,\n",
      "            -0.11293654888868332,\n",
      "            0.03619292005896568,\n",
      "            -0.10268038511276245,\n",
      "            -0.10548548400402069,\n",
      "            -0.017055055126547813,\n",
      "            -0.017962554469704628,\n",
      "            0.03141998499631882,\n",
      "            0.13519766926765442,\n",
      "            -0.12532052397727966,\n",
      "            -0.07451080530881882,\n",
      "            0.13242976367473602,\n",
      "            -0.03788619861006737,\n",
      "            -0.012071391567587852,\n",
      "            -0.08259687572717667,\n",
      "            0.190673366189003,\n",
      "            0.15023690462112427,\n",
      "            0.1485840529203415,\n",
      "            -0.20798540115356445,\n",
      "            0.011819181963801384,\n",
      "            -0.06370033323764801,\n",
      "            -0.10804586857557297,\n",
      "            -0.07364245504140854,\n",
      "            -0.13183552026748657,\n",
      "            0.012514985166490078,\n",
      "            -0.004370999056845903,\n",
      "            -0.0766916424036026,\n",
      "            -0.1304546743631363,\n",
      "            -0.08530622720718384,\n",
      "            0.08787918090820312,\n",
      "            0.0606907457113266,\n",
      "            0.18057316541671753,\n",
      "            0.020947905257344246,\n",
      "            0.15048697590827942,\n",
      "            -0.144670769572258,\n",
      "            0.04245683178305626,\n",
      "            0.047357894480228424,\n",
      "            -0.05812764912843704,\n",
      "            0.0025783819146454334,\n",
      "            -0.011123747564852238,\n",
      "            0.1527070850133896,\n",
      "            0.159718319773674,\n",
      "            -0.14956678450107574,\n",
      "            -0.12828315794467926,\n",
      "            -0.05334586277604103,\n",
      "            -0.08922143280506134,\n",
      "            0.1088738888502121,\n",
      "            0.06912221759557724,\n",
      "            -0.13798020780086517,\n",
      "            -0.14573697745800018,\n",
      "            0.02462407574057579,\n",
      "            0.05965336784720421,\n",
      "            0.17355668544769287,\n",
      "            -0.030330050736665726,\n",
      "            -0.16450025141239166,\n",
      "            0.04465270787477493,\n",
      "            -0.22477898001670837,\n",
      "            -0.23528392612934113,\n",
      "            -0.1296166479587555,\n",
      "            -0.018270233646035194,\n",
      "            -0.1267484873533249,\n",
      "            -0.14614050090312958,\n",
      "            -0.12276846915483475,\n",
      "            -0.01965901628136635,\n",
      "            0.047950517386198044,\n",
      "            0.24670113623142242,\n",
      "            -0.1934041827917099,\n",
      "            -0.21028219163417816,\n",
      "            0.10011941939592361,\n",
      "            0.19918864965438843,\n",
      "            -0.0447441041469574,\n",
      "            0.12530916929244995,\n",
      "            -0.016833586618304253,\n",
      "            0.0799000933766365,\n",
      "            -0.12538981437683105,\n",
      "            0.21628423035144806,\n",
      "            -0.014348967000842094,\n",
      "            -0.10444822907447815,\n",
      "            -0.04468541964888573,\n",
      "            0.012303854338824749,\n",
      "            0.10861262679100037,\n",
      "            -0.008121538907289505,\n",
      "            0.20604147017002106,\n",
      "            -0.13058768212795258,\n",
      "            0.1050909087061882,\n",
      "            -0.10168556123971939,\n",
      "            0.22423969209194183,\n",
      "            0.06773152202367783,\n",
      "            0.19033467769622803,\n",
      "            0.005773412529379129,\n",
      "            -0.09931076318025589,\n",
      "            -0.06214875355362892,\n",
      "            0.08372729271650314,\n",
      "            0.17040996253490448,\n",
      "            -0.15098199248313904,\n",
      "            0.1589364856481552,\n",
      "            -0.05854099616408348\n",
      "          ],\n",
      "          [\n",
      "            0.0031943561043590307,\n",
      "            -0.10992353409528732,\n",
      "            -0.08262354135513306,\n",
      "            -0.0038337260484695435,\n",
      "            0.14034585654735565,\n",
      "            0.09422905743122101,\n",
      "            -0.039749667048454285,\n",
      "            0.13880258798599243,\n",
      "            -0.1331729292869568,\n",
      "            -0.115261971950531,\n",
      "            0.0012125223875045776,\n",
      "            0.03718122839927673,\n",
      "            -0.028921082615852356,\n",
      "            0.002314329147338867,\n",
      "            -0.11620518565177917,\n",
      "            -0.1483033001422882,\n",
      "            -0.06365427374839783,\n",
      "            0.0073224944062530994,\n",
      "            0.15149672329425812,\n",
      "            -0.15274202823638916,\n",
      "            0.08998562395572662,\n",
      "            0.15837816894054413,\n",
      "            0.13851122558116913,\n",
      "            0.08092288672924042,\n",
      "            -0.02078232169151306,\n",
      "            0.023639315739274025,\n",
      "            -0.10647474229335785,\n",
      "            0.025263234972953796,\n",
      "            -0.10511395335197449,\n",
      "            -0.04832367226481438,\n",
      "            -0.043510034680366516,\n",
      "            0.04464097321033478,\n",
      "            0.004659965634346008,\n",
      "            0.04636186733841896,\n",
      "            0.05023542046546936,\n",
      "            -0.13234110176563263,\n",
      "            -0.027339354157447815,\n",
      "            -0.12858723104000092,\n",
      "            0.03203649818897247,\n",
      "            -0.10273130238056183,\n",
      "            0.021362006664276123,\n",
      "            -0.1075381189584732,\n",
      "            0.09696804732084274,\n",
      "            0.12389657646417618,\n",
      "            -0.05931662768125534,\n",
      "            -0.10004426538944244,\n",
      "            0.10345541685819626,\n",
      "            0.009814523160457611,\n",
      "            0.005669466219842434,\n",
      "            -0.055338867008686066,\n",
      "            -0.03957236558198929,\n",
      "            0.17087416350841522,\n",
      "            -0.008873939514160156,\n",
      "            -0.11944392323493958,\n",
      "            0.1695253998041153,\n",
      "            -0.12258405238389969,\n",
      "            0.04569612443447113,\n",
      "            0.053659602999687195,\n",
      "            0.136757031083107,\n",
      "            -0.020237773656845093,\n",
      "            -0.029812991619110107,\n",
      "            -0.1275937706232071,\n",
      "            -0.053129952400922775,\n",
      "            -0.041425786912441254,\n",
      "            0.05159379169344902,\n",
      "            -0.14921072125434875,\n",
      "            -0.1174730435013771,\n",
      "            -0.1460309475660324,\n",
      "            0.010351389646530151,\n",
      "            -0.15371452271938324,\n",
      "            -0.11000571399927139,\n",
      "            -0.012587718665599823,\n",
      "            0.056912366300821304,\n",
      "            0.16966037452220917,\n",
      "            -0.11659436672925949,\n",
      "            0.13875587284564972,\n",
      "            -0.049771565943956375,\n",
      "            -0.09755947440862656,\n",
      "            -0.12390924990177155,\n",
      "            -0.15343749523162842,\n",
      "            -0.10080333054065704,\n",
      "            0.13858069479465485,\n",
      "            0.03480903059244156,\n",
      "            0.036701053380966187,\n",
      "            -0.04683889448642731,\n",
      "            -0.14184542000293732,\n",
      "            -0.1661563664674759,\n",
      "            0.04868999123573303,\n",
      "            -0.1586659848690033,\n",
      "            -0.07053999602794647,\n",
      "            0.041059937328100204,\n",
      "            -0.1329864114522934,\n",
      "            0.1629853993654251,\n",
      "            0.012090280652046204,\n",
      "            -0.08042377233505249,\n",
      "            -0.11916161328554153,\n",
      "            -0.02931249514222145,\n",
      "            -0.018631353974342346,\n",
      "            0.1602744609117508,\n",
      "            -0.0657191127538681,\n",
      "            -0.1431785523891449,\n",
      "            -0.15471699833869934,\n",
      "            -0.13015198707580566,\n",
      "            -0.0037665367126464844,\n",
      "            0.0655069500207901,\n",
      "            -0.15359824895858765,\n",
      "            -0.06405600160360336,\n",
      "            -0.06439878046512604,\n",
      "            0.15837718546390533,\n",
      "            0.10597680509090424,\n",
      "            -0.13378003239631653,\n",
      "            -0.14186152815818787,\n",
      "            0.1262538880109787,\n",
      "            0.04539952799677849,\n",
      "            -0.09456586092710495,\n",
      "            -0.10454457998275757,\n",
      "            -0.052483148872852325,\n",
      "            0.14319205284118652,\n",
      "            -0.16055943071842194,\n",
      "            -0.14088982343673706,\n",
      "            0.04660126566886902,\n",
      "            0.0195834431797266,\n",
      "            -0.06228732690215111,\n",
      "            -0.03712748736143112,\n",
      "            -0.001316031557507813,\n",
      "            0.14633505046367645,\n",
      "            0.1614997237920761,\n",
      "            -0.009687036275863647\n",
      "          ],\n",
      "          [\n",
      "            -0.1570090800523758,\n",
      "            0.06086109206080437,\n",
      "            -0.05639461800456047,\n",
      "            -0.14150182902812958,\n",
      "            -0.11579713970422745,\n",
      "            -0.08513981103897095,\n",
      "            0.09328820556402206,\n",
      "            -0.1741514801979065,\n",
      "            0.10099134594202042,\n",
      "            0.053708646446466446,\n",
      "            -0.03354176878929138,\n",
      "            -0.13742418587207794,\n",
      "            0.0725061371922493,\n",
      "            0.13213910162448883,\n",
      "            -0.12832514941692352,\n",
      "            0.11673803627490997,\n",
      "            -0.08388520777225494,\n",
      "            0.0605488047003746,\n",
      "            0.0028503171633929014,\n",
      "            -0.08421847969293594,\n",
      "            -0.14934608340263367,\n",
      "            -0.014560267329216003,\n",
      "            0.02282322756946087,\n",
      "            -0.015619136393070221,\n",
      "            -0.0330892875790596,\n",
      "            0.055239226669073105,\n",
      "            -0.010277468711137772,\n",
      "            0.08981412649154663,\n",
      "            -0.09826051443815231,\n",
      "            -0.1370168775320053,\n",
      "            -0.004904668778181076,\n",
      "            -0.155477836728096,\n",
      "            0.03334754705429077,\n",
      "            -0.08760584145784378,\n",
      "            0.030247271060943604,\n",
      "            -0.14069528877735138,\n",
      "            -0.10072793066501617,\n",
      "            0.07254395633935928,\n",
      "            -0.09381551295518875,\n",
      "            0.08483231067657471,\n",
      "            0.12775054574012756,\n",
      "            0.04271828383207321,\n",
      "            0.1736144870519638,\n",
      "            -0.16482391953468323,\n",
      "            0.07914499938488007,\n",
      "            -0.08453863859176636,\n",
      "            -0.1447766125202179,\n",
      "            -0.11913900822401047,\n",
      "            0.13333192467689514,\n",
      "            -0.08186017721891403,\n",
      "            -0.08578210324048996,\n",
      "            -0.04384906217455864,\n",
      "            0.15515102446079254,\n",
      "            0.007722982205450535,\n",
      "            -0.0021879293490201235,\n",
      "            -0.10880102217197418,\n",
      "            -0.07835116982460022,\n",
      "            -0.1407316029071808,\n",
      "            0.025291144847869873,\n",
      "            0.0509573370218277,\n",
      "            0.17406849563121796,\n",
      "            0.09155125916004181,\n",
      "            -0.0821504145860672,\n",
      "            0.030963804572820663,\n",
      "            0.10348738729953766,\n",
      "            -0.004573393147438765,\n",
      "            -0.010488673113286495,\n",
      "            -0.030373701825737953,\n",
      "            0.14557193219661713,\n",
      "            0.002098352648317814,\n",
      "            0.11557365953922272,\n",
      "            0.04595771059393883,\n",
      "            -0.13882093131542206,\n",
      "            -0.1628420501947403,\n",
      "            0.12698648869991302,\n",
      "            -0.15351970493793488,\n",
      "            0.01974831521511078,\n",
      "            0.03962773084640503,\n",
      "            -0.06699497997760773,\n",
      "            -0.15527674555778503,\n",
      "            -0.17620104551315308,\n",
      "            0.047084741294384,\n",
      "            0.010848268866539001,\n",
      "            -0.14181779325008392,\n",
      "            -0.15070506930351257,\n",
      "            0.13953018188476562,\n",
      "            -0.1633249819278717,\n",
      "            0.0805254727602005,\n",
      "            0.11654112488031387,\n",
      "            -0.08498825877904892,\n",
      "            -0.04912634566426277,\n",
      "            0.02424902282655239,\n",
      "            -0.10827943682670593,\n",
      "            -0.12379056215286255,\n",
      "            0.1465802639722824,\n",
      "            0.12314881384372711,\n",
      "            0.17466656863689423,\n",
      "            -0.005523680243641138,\n",
      "            0.025669671595096588,\n",
      "            0.08966967463493347,\n",
      "            0.04827796667814255,\n",
      "            0.12008310854434967,\n",
      "            0.020200811326503754,\n",
      "            0.06995262950658798,\n",
      "            0.06538766622543335,\n",
      "            -0.12286746501922607,\n",
      "            -0.00404425198212266,\n",
      "            0.13825730979442596,\n",
      "            -0.025932837277650833,\n",
      "            0.020552610978484154,\n",
      "            0.141142800450325,\n",
      "            -0.03588143736124039,\n",
      "            -0.028035877272486687,\n",
      "            0.027156276628375053,\n",
      "            -0.060975730419158936,\n",
      "            0.09980377554893494,\n",
      "            0.0504806749522686,\n",
      "            0.03264516219496727,\n",
      "            -0.025222843512892723,\n",
      "            0.07454121112823486,\n",
      "            -0.05466863512992859,\n",
      "            0.10211587697267532,\n",
      "            -0.1082797572016716,\n",
      "            0.046202048659324646,\n",
      "            0.1499919295310974,\n",
      "            -0.12811794877052307,\n",
      "            -0.1433011144399643,\n",
      "            -0.11665615439414978\n",
      "          ],\n",
      "          [\n",
      "            0.20813436806201935,\n",
      "            0.06340246647596359,\n",
      "            0.07821375131607056,\n",
      "            -0.0717541053891182,\n",
      "            -0.16511127352714539,\n",
      "            -0.15223731100559235,\n",
      "            0.01543734036386013,\n",
      "            0.11933950334787369,\n",
      "            -0.030675578862428665,\n",
      "            0.11487511545419693,\n",
      "            -0.06958610564470291,\n",
      "            0.034055765718221664,\n",
      "            -0.06525348126888275,\n",
      "            -0.07892361283302307,\n",
      "            -0.11937496066093445,\n",
      "            0.0012635588645935059,\n",
      "            0.20594558119773865,\n",
      "            0.08502405136823654,\n",
      "            0.06108202412724495,\n",
      "            -0.20446647703647614,\n",
      "            -0.12915048003196716,\n",
      "            0.16791050136089325,\n",
      "            -0.006400873884558678,\n",
      "            -0.03544546291232109,\n",
      "            -0.1307593584060669,\n",
      "            0.02605224773287773,\n",
      "            0.12036339193582535,\n",
      "            -0.07578365504741669,\n",
      "            0.1491081267595291,\n",
      "            -0.06852474808692932,\n",
      "            -0.10081808269023895,\n",
      "            -0.12518590688705444,\n",
      "            0.12014656513929367,\n",
      "            0.2048606425523758,\n",
      "            0.00014856457710266113,\n",
      "            -0.05837279185652733,\n",
      "            -0.20541174709796906,\n",
      "            -0.08920840173959732,\n",
      "            -0.19773107767105103,\n",
      "            0.014655386097729206,\n",
      "            -0.22806884348392487,\n",
      "            -0.05734578147530556,\n",
      "            -0.0681847333908081,\n",
      "            -0.2208116203546524,\n",
      "            0.08792015165090561,\n",
      "            -0.1160954013466835,\n",
      "            0.18771906197071075,\n",
      "            0.11072837561368942,\n",
      "            -0.14850714802742004,\n",
      "            0.057149045169353485,\n",
      "            -0.21162152290344238,\n",
      "            0.024680206552147865,\n",
      "            -0.1549563854932785,\n",
      "            0.11252257227897644,\n",
      "            0.04392607510089874,\n",
      "            -0.2331378161907196,\n",
      "            0.15835417807102203,\n",
      "            0.0049191368743777275,\n",
      "            0.08121224492788315,\n",
      "            0.04944891110062599,\n",
      "            0.009661177173256874,\n",
      "            -0.12659679353237152,\n",
      "            0.12357096374034882,\n",
      "            0.08882816135883331,\n",
      "            0.25864264369010925,\n",
      "            0.13863730430603027,\n",
      "            -0.15221820771694183,\n",
      "            -0.20697763562202454,\n",
      "            0.13094621896743774,\n",
      "            0.16763058304786682,\n",
      "            -0.1415839046239853,\n",
      "            -0.03206929564476013,\n",
      "            -0.02357284352183342,\n",
      "            -0.1549006551504135,\n",
      "            -0.04386037215590477,\n",
      "            0.03401290625333786,\n",
      "            0.12174328416585922,\n",
      "            0.14569847285747528,\n",
      "            -0.09270377457141876,\n",
      "            -0.1726394146680832,\n",
      "            -0.01784726418554783,\n",
      "            -0.02618403173983097,\n",
      "            0.027555150911211967,\n",
      "            -0.10228511691093445,\n",
      "            -0.07762313634157181,\n",
      "            -0.18397782742977142,\n",
      "            -0.04358780011534691,\n",
      "            0.11177636682987213,\n",
      "            -0.2614893913269043,\n",
      "            0.07053469866514206,\n",
      "            -0.16196542978286743,\n",
      "            -0.2123972475528717,\n",
      "            -0.16495707631111145,\n",
      "            -0.19587408006191254,\n",
      "            0.08925211429595947,\n",
      "            -0.1664683073759079,\n",
      "            0.08653218299150467,\n",
      "            -0.01773313619196415,\n",
      "            0.037526506930589676,\n",
      "            -0.15680859982967377,\n",
      "            0.12969593703746796,\n",
      "            -0.008257175795733929,\n",
      "            -0.09855200350284576,\n",
      "            -0.16345223784446716,\n",
      "            0.13377496600151062,\n",
      "            -0.17574557662010193,\n",
      "            0.08234962075948715,\n",
      "            -0.11385633051395416,\n",
      "            -0.17402467131614685,\n",
      "            -0.05837690085172653,\n",
      "            0.11129216849803925,\n",
      "            0.05443057045340538,\n",
      "            0.03201482445001602,\n",
      "            0.22921176254749298,\n",
      "            -0.16341906785964966,\n",
      "            -0.13753823935985565,\n",
      "            -0.19578300416469574,\n",
      "            -0.0002347873814869672,\n",
      "            -0.2235047072172165,\n",
      "            0.10697503387928009,\n",
      "            0.06303054094314575,\n",
      "            -0.0005707137170247734,\n",
      "            0.06383391469717026,\n",
      "            -0.17721429467201233,\n",
      "            0.253898948431015,\n",
      "            0.026746392250061035,\n",
      "            0.1905045062303543,\n",
      "            0.17392811179161072\n",
      "          ],\n",
      "          [\n",
      "            -0.1592632234096527,\n",
      "            0.0527983196079731,\n",
      "            0.11579887568950653,\n",
      "            -0.1464720219373703,\n",
      "            0.08305197954177856,\n",
      "            -0.06404685229063034,\n",
      "            -0.09630017727613449,\n",
      "            -0.08512946963310242,\n",
      "            -0.01762882061302662,\n",
      "            -0.08512488752603531,\n",
      "            0.0707656666636467,\n",
      "            0.14883315563201904,\n",
      "            0.1671207845211029,\n",
      "            -0.05371938645839691,\n",
      "            0.025298617780208588,\n",
      "            -0.13189548254013062,\n",
      "            0.15267671644687653,\n",
      "            0.027825141325592995,\n",
      "            0.15690168738365173,\n",
      "            -0.011218550615012646,\n",
      "            0.048828672617673874,\n",
      "            -0.1097654402256012,\n",
      "            -0.07206043601036072,\n",
      "            -0.10424749553203583,\n",
      "            0.23651544749736786,\n",
      "            0.24704809486865997,\n",
      "            0.19205689430236816,\n",
      "            -0.039248157292604446,\n",
      "            -0.14496402442455292,\n",
      "            0.10309912264347076,\n",
      "            0.13324348628520966,\n",
      "            -0.08203675597906113,\n",
      "            -0.10558898746967316,\n",
      "            -0.07288520038127899,\n",
      "            0.08404727280139923,\n",
      "            0.11963602155447006,\n",
      "            0.17191928625106812,\n",
      "            0.080402672290802,\n",
      "            -0.008347028866410255,\n",
      "            0.1979660987854004,\n",
      "            0.08515281975269318,\n",
      "            0.17580705881118774,\n",
      "            0.08509978652000427,\n",
      "            0.16467967629432678,\n",
      "            -0.05007541552186012,\n",
      "            0.17583802342414856,\n",
      "            -0.15825514495372772,\n",
      "            -0.13899460434913635,\n",
      "            -0.140309140086174,\n",
      "            -0.05429864302277565,\n",
      "            0.17134995758533478,\n",
      "            0.1209043487906456,\n",
      "            -0.01688394881784916,\n",
      "            0.04562493786215782,\n",
      "            0.1079396978020668,\n",
      "            0.19624796509742737,\n",
      "            0.03555793687701225,\n",
      "            0.10422925651073456,\n",
      "            -0.024334462359547615,\n",
      "            0.17043383419513702,\n",
      "            0.03518761321902275,\n",
      "            -0.1568266898393631,\n",
      "            0.07727017253637314,\n",
      "            -0.1478348672389984,\n",
      "            -0.014682939276099205,\n",
      "            0.06288490444421768,\n",
      "            -0.14227569103240967,\n",
      "            0.013928580097854137,\n",
      "            -0.16958272457122803,\n",
      "            -0.08833779394626617,\n",
      "            0.07186180353164673,\n",
      "            0.20343787968158722,\n",
      "            -0.021847272291779518,\n",
      "            0.14408770203590393,\n",
      "            -0.03802402317523956,\n",
      "            0.1217755675315857,\n",
      "            0.1377662718296051,\n",
      "            -0.05526354908943176,\n",
      "            0.17089539766311646,\n",
      "            0.10748844593763351,\n",
      "            0.10905350744724274,\n",
      "            -0.05757422745227814,\n",
      "            0.12464519590139389,\n",
      "            0.10513588041067123,\n",
      "            0.144438236951828,\n",
      "            -0.07267563045024872,\n",
      "            -0.10293912887573242,\n",
      "            -0.07695810496807098,\n",
      "            -0.04102565720677376,\n",
      "            -0.1454361230134964,\n",
      "            0.07477807998657227,\n",
      "            0.061033692210912704,\n",
      "            -0.14819201827049255,\n",
      "            -0.09761977195739746,\n",
      "            0.22264444828033447,\n",
      "            0.0814216136932373,\n",
      "            -0.22836098074913025,\n",
      "            0.0014887592988088727,\n",
      "            0.058951932936906815,\n",
      "            0.12007641047239304,\n",
      "            -0.01473953202366829,\n",
      "            0.13166449964046478,\n",
      "            0.15469346940517426,\n",
      "            0.15334945917129517,\n",
      "            0.0811619907617569,\n",
      "            -0.015835702419281006,\n",
      "            0.011852988041937351,\n",
      "            -0.128180593252182,\n",
      "            0.105950728058815,\n",
      "            -0.06277209520339966,\n",
      "            0.11298093944787979,\n",
      "            -0.08257705718278885,\n",
      "            -0.07363062351942062,\n",
      "            -0.18100477755069733,\n",
      "            0.009701869450509548,\n",
      "            -0.1206650584936142,\n",
      "            -0.009040845558047295,\n",
      "            -0.17525769770145416,\n",
      "            0.22640842199325562,\n",
      "            -0.09891575574874878,\n",
      "            -0.1308172345161438,\n",
      "            0.012517204508185387,\n",
      "            0.10356166958808899,\n",
      "            0.15940265357494354,\n",
      "            -0.011283108033239841,\n",
      "            0.08938529342412949,\n",
      "            0.10131160914897919,\n",
      "            -0.15027233958244324\n",
      "          ],\n",
      "          [\n",
      "            0.1710071563720703,\n",
      "            -0.042300138622522354,\n",
      "            0.01602162793278694,\n",
      "            -0.14532922208309174,\n",
      "            -0.005989555735141039,\n",
      "            0.13380026817321777,\n",
      "            0.17884033918380737,\n",
      "            0.005921072326600552,\n",
      "            0.13613468408584595,\n",
      "            -0.040024708956480026,\n",
      "            0.16597139835357666,\n",
      "            0.11718907207250595,\n",
      "            0.12614315748214722,\n",
      "            -0.04000489041209221,\n",
      "            0.07138802856206894,\n",
      "            -0.04227025806903839,\n",
      "            0.00790388137102127,\n",
      "            0.17820873856544495,\n",
      "            -0.11313934624195099,\n",
      "            0.07820826023817062,\n",
      "            -0.10302648693323135,\n",
      "            -0.16546352207660675,\n",
      "            -0.04841297119855881,\n",
      "            -0.14214341342449188,\n",
      "            0.09161029011011124,\n",
      "            0.0028738887049257755,\n",
      "            -0.15052436292171478,\n",
      "            0.07966890186071396,\n",
      "            -0.047990117222070694,\n",
      "            0.055166952311992645,\n",
      "            0.1256180703639984,\n",
      "            0.13121028244495392,\n",
      "            -0.0067011150531470776,\n",
      "            -0.1480257660150528,\n",
      "            0.04546186327934265,\n",
      "            0.11119692772626877,\n",
      "            -0.05797600746154785,\n",
      "            -0.13607576489448547,\n",
      "            0.13650988042354584,\n",
      "            0.0529806949198246,\n",
      "            0.17865224182605743,\n",
      "            0.07865528762340546,\n",
      "            0.11949748545885086,\n",
      "            -0.15065407752990723,\n",
      "            -0.08098022639751434,\n",
      "            -0.15645405650138855,\n",
      "            -0.0193022508174181,\n",
      "            -0.070688396692276,\n",
      "            0.06791738420724869,\n",
      "            -0.1595355123281479,\n",
      "            -0.060572363436222076,\n",
      "            0.047541823238134384,\n",
      "            -0.03855106607079506,\n",
      "            0.15825016796588898,\n",
      "            0.01408395916223526,\n",
      "            -0.09911920875310898,\n",
      "            0.12436303496360779,\n",
      "            0.009951489977538586,\n",
      "            -0.023271137848496437,\n",
      "            0.028175203129649162,\n",
      "            0.13802176713943481,\n",
      "            0.0398857481777668,\n",
      "            -0.060041721910238266,\n",
      "            0.10929234325885773,\n",
      "            0.08015719056129456,\n",
      "            -0.0493692010641098,\n",
      "            -0.015099423006176949,\n",
      "            -0.03260617330670357,\n",
      "            -0.1485435664653778,\n",
      "            -0.01672491431236267,\n",
      "            0.006602918263524771,\n",
      "            0.08492953330278397,\n",
      "            -0.042228810489177704,\n",
      "            0.03141031786799431,\n",
      "            -0.12906447052955627,\n",
      "            0.0489785373210907,\n",
      "            -0.1509777307510376,\n",
      "            0.10248716175556183,\n",
      "            0.011687995865941048,\n",
      "            0.044436611235141754,\n",
      "            -0.11008002609014511,\n",
      "            0.018613314256072044,\n",
      "            0.008691715076565742,\n",
      "            0.044518012553453445,\n",
      "            -0.029892517253756523,\n",
      "            -0.16345107555389404,\n",
      "            0.1319573074579239,\n",
      "            -0.09675252437591553,\n",
      "            -0.03835628181695938,\n",
      "            -0.03145149350166321,\n",
      "            -0.11782591044902802,\n",
      "            0.07384084165096283,\n",
      "            0.10431555658578873,\n",
      "            -0.10414174944162369,\n",
      "            0.08342041820287704,\n",
      "            -0.13125327229499817,\n",
      "            0.06029428169131279,\n",
      "            -0.021563004702329636,\n",
      "            0.020233409479260445,\n",
      "            0.1343296766281128,\n",
      "            0.006567871198058128,\n",
      "            0.17159897089004517,\n",
      "            -0.14361658692359924,\n",
      "            0.1582002341747284,\n",
      "            0.11912617832422256,\n",
      "            -0.02589203417301178,\n",
      "            0.054985858500003815,\n",
      "            -0.16661648452281952,\n",
      "            0.09338024258613586,\n",
      "            0.08484802395105362,\n",
      "            0.04242954030632973,\n",
      "            -0.06191769242286682,\n",
      "            -0.12484129518270493,\n",
      "            -0.13991278409957886,\n",
      "            0.17951251566410065,\n",
      "            0.025846797972917557,\n",
      "            -0.02281092293560505,\n",
      "            0.12461359053850174,\n",
      "            -0.07560912519693375,\n",
      "            -0.1034339964389801,\n",
      "            0.12836389243602753,\n",
      "            0.1611890196800232,\n",
      "            0.07127649337053299,\n",
      "            -0.1732439547777176,\n",
      "            -0.017698898911476135,\n",
      "            0.16992352902889252,\n",
      "            -0.12568846344947815,\n",
      "            0.12510262429714203\n",
      "          ],\n",
      "          [\n",
      "            0.011372231878340244,\n",
      "            0.13539759814739227,\n",
      "            -0.06129607558250427,\n",
      "            -0.05742011219263077,\n",
      "            -0.17494839429855347,\n",
      "            -0.1043691635131836,\n",
      "            -0.08653011918067932,\n",
      "            -0.0860893577337265,\n",
      "            -0.08983305841684341,\n",
      "            -0.027000263333320618,\n",
      "            0.10545535385608673,\n",
      "            0.12103577703237534,\n",
      "            0.03663661703467369,\n",
      "            -0.027897462248802185,\n",
      "            0.07407208532094955,\n",
      "            0.07764934003353119,\n",
      "            -0.10329069197177887,\n",
      "            0.038352951407432556,\n",
      "            0.12999597191810608,\n",
      "            0.0343160443007946,\n",
      "            0.08444301038980484,\n",
      "            0.039110422134399414,\n",
      "            0.16389496624469757,\n",
      "            -0.07036104053258896,\n",
      "            0.06843603402376175,\n",
      "            0.02236069366335869,\n",
      "            -0.024453982710838318,\n",
      "            0.15387149155139923,\n",
      "            0.027401389554142952,\n",
      "            -0.07191918045282364,\n",
      "            0.08727657794952393,\n",
      "            0.024497078731656075,\n",
      "            -0.16745217144489288,\n",
      "            0.0869339182972908,\n",
      "            -0.013007059693336487,\n",
      "            0.050542376935482025,\n",
      "            -0.11309131234884262,\n",
      "            0.12683556973934174,\n",
      "            -0.09966042637825012,\n",
      "            0.0041080862283706665,\n",
      "            -0.06420233845710754,\n",
      "            0.13647937774658203,\n",
      "            -0.06908281147480011,\n",
      "            -0.049204807728528976,\n",
      "            -0.03409022092819214,\n",
      "            -0.01912897452712059,\n",
      "            0.04399517551064491,\n",
      "            -0.015127645805478096,\n",
      "            0.10169366747140884,\n",
      "            0.13881132006645203,\n",
      "            -0.05567552149295807,\n",
      "            0.16035079956054688,\n",
      "            -0.11079166829586029,\n",
      "            0.006512319669127464,\n",
      "            -0.16138723492622375,\n",
      "            -0.150741845369339,\n",
      "            0.10056255757808685,\n",
      "            -0.15611018240451813,\n",
      "            0.15353219211101532,\n",
      "            -0.13345153629779816,\n",
      "            -0.06785750389099121,\n",
      "            -0.003688812255859375,\n",
      "            -0.07716212421655655,\n",
      "            0.09636038541793823,\n",
      "            -0.014568195678293705,\n",
      "            -0.09736926108598709,\n",
      "            -0.1516209840774536,\n",
      "            -0.056519173085689545,\n",
      "            -0.04874514043331146,\n",
      "            -0.0811675488948822,\n",
      "            0.12391868233680725,\n",
      "            -0.09937817603349686,\n",
      "            -0.11456495523452759,\n",
      "            0.07559890300035477,\n",
      "            0.08308450132608414,\n",
      "            0.011537408456206322,\n",
      "            0.007843519560992718,\n",
      "            -0.10563291609287262,\n",
      "            0.1683190017938614,\n",
      "            0.14002490043640137,\n",
      "            -0.10249264538288116,\n",
      "            0.13095343112945557,\n",
      "            0.11977588385343552,\n",
      "            -0.10516739636659622,\n",
      "            -0.06211310997605324,\n",
      "            -0.1329686939716339,\n",
      "            -0.13488444685935974,\n",
      "            0.09150570631027222,\n",
      "            -0.1338336020708084,\n",
      "            0.02879650704562664,\n",
      "            0.1406923085451126,\n",
      "            -0.11119887232780457,\n",
      "            0.021466657519340515,\n",
      "            0.1564951241016388,\n",
      "            -0.03790055215358734,\n",
      "            0.16720698773860931,\n",
      "            -0.15897242724895477,\n",
      "            -0.09127800166606903,\n",
      "            0.08220748603343964,\n",
      "            -0.1511719524860382,\n",
      "            -0.14303208887577057,\n",
      "            -0.16775880753993988,\n",
      "            0.02074650302529335,\n",
      "            -0.04153101518750191,\n",
      "            -0.14350071549415588,\n",
      "            0.14205147325992584,\n",
      "            -0.05157097056508064,\n",
      "            -0.0213223397731781,\n",
      "            -0.05000239610671997,\n",
      "            -0.03976404666900635,\n",
      "            0.045125871896743774,\n",
      "            -0.0928000882267952,\n",
      "            -0.03342108055949211,\n",
      "            0.11291304975748062,\n",
      "            0.08589179068803787,\n",
      "            -0.010397301986813545,\n",
      "            0.11601287871599197,\n",
      "            0.08622314780950546,\n",
      "            0.14322078227996826,\n",
      "            -0.16385416686534882,\n",
      "            -0.016535671427845955,\n",
      "            0.14401839673519135,\n",
      "            0.0020775904413312674,\n",
      "            0.08009438961744308,\n",
      "            -0.00832467619329691,\n",
      "            -0.08987617492675781,\n",
      "            0.008970266208052635,\n",
      "            -0.16324108839035034\n",
      "          ],\n",
      "          [\n",
      "            -0.01771547645330429,\n",
      "            0.143990620970726,\n",
      "            -0.11011099815368652,\n",
      "            -0.07530811429023743,\n",
      "            -0.1057523563504219,\n",
      "            -0.038730207830667496,\n",
      "            0.07584455609321594,\n",
      "            0.16300544142723083,\n",
      "            0.0016273846849799156,\n",
      "            -0.1109212189912796,\n",
      "            -0.03214313089847565,\n",
      "            0.027400383725762367,\n",
      "            0.08088892698287964,\n",
      "            0.06002339720726013,\n",
      "            0.007742375135421753,\n",
      "            0.06842721998691559,\n",
      "            -0.034514419734478,\n",
      "            0.04150310903787613,\n",
      "            0.13172104954719543,\n",
      "            -0.09781413525342941,\n",
      "            0.07293166220188141,\n",
      "            -0.0698942244052887,\n",
      "            -0.06824731081724167,\n",
      "            -0.13739657402038574,\n",
      "            -0.030799224972724915,\n",
      "            -0.09704837203025818,\n",
      "            0.035625435411930084,\n",
      "            0.01677480712532997,\n",
      "            -0.10687705874443054,\n",
      "            -0.1332322061061859,\n",
      "            -0.05177559703588486,\n",
      "            0.10557608306407928,\n",
      "            -0.050638798624277115,\n",
      "            -0.040914490818977356,\n",
      "            -0.08414331823587418,\n",
      "            -0.0881892666220665,\n",
      "            -0.021228596568107605,\n",
      "            -0.027071263641119003,\n",
      "            0.06160813570022583,\n",
      "            -0.06299416720867157,\n",
      "            0.10010786354541779,\n",
      "            -0.13582612574100494,\n",
      "            -0.025698762387037277,\n",
      "            0.053647853434085846,\n",
      "            0.007340297102928162,\n",
      "            -0.1501622200012207,\n",
      "            -0.06781766563653946,\n",
      "            -0.07930387556552887,\n",
      "            0.055200476199388504,\n",
      "            -0.1256251037120819,\n",
      "            0.14251117408275604,\n",
      "            -0.062204424291849136,\n",
      "            0.0672600194811821,\n",
      "            -0.0354243665933609,\n",
      "            0.008901465684175491,\n",
      "            -0.09592553973197937,\n",
      "            -0.0948488712310791,\n",
      "            -0.08231329172849655,\n",
      "            -0.03000260889530182,\n",
      "            -0.08357109874486923,\n",
      "            -0.06256993114948273,\n",
      "            -0.04908548295497894,\n",
      "            -0.025099072605371475,\n",
      "            0.02544507384300232,\n",
      "            0.06284063309431076,\n",
      "            0.14788882434368134,\n",
      "            -0.064618781208992,\n",
      "            -0.061229411512613297,\n",
      "            0.10181669890880585,\n",
      "            0.012720892205834389,\n",
      "            -0.025075389072299004,\n",
      "            -0.012606225907802582,\n",
      "            -0.1521127074956894,\n",
      "            -0.12448684126138687,\n",
      "            -0.12131915986537933,\n",
      "            -0.14957553148269653,\n",
      "            0.1255795657634735,\n",
      "            -0.11865399777889252,\n",
      "            -0.14506858587265015,\n",
      "            -0.16003865003585815,\n",
      "            0.09912581741809845,\n",
      "            0.11768774688243866,\n",
      "            0.15555702149868011,\n",
      "            0.10192346572875977,\n",
      "            -0.0031576433684676886,\n",
      "            -0.15901997685432434,\n",
      "            -0.027340715751051903,\n",
      "            -0.07081285119056702,\n",
      "            -0.08124192804098129,\n",
      "            0.17607854306697845,\n",
      "            0.017543718218803406,\n",
      "            -0.15602965652942657,\n",
      "            -0.07576759904623032,\n",
      "            -0.1500532627105713,\n",
      "            -0.03499588742852211,\n",
      "            0.13773150742053986,\n",
      "            0.014387336559593678,\n",
      "            0.15794675052165985,\n",
      "            0.13651719689369202,\n",
      "            -0.05341017246246338,\n",
      "            0.08693794161081314,\n",
      "            -0.14696863293647766,\n",
      "            -0.011117520742118359,\n",
      "            0.11546691507101059,\n",
      "            0.03382539749145508,\n",
      "            0.10752789676189423,\n",
      "            -0.009082246571779251,\n",
      "            -0.05000677704811096,\n",
      "            0.07278664410114288,\n",
      "            -0.1129951998591423,\n",
      "            -0.010912910103797913,\n",
      "            -0.1051521971821785,\n",
      "            -0.11493997275829315,\n",
      "            0.02062741480767727,\n",
      "            -0.11313023418188095,\n",
      "            0.15393655002117157,\n",
      "            -0.16197359561920166,\n",
      "            0.11041542142629623,\n",
      "            -0.14045368134975433,\n",
      "            -0.1734236627817154,\n",
      "            0.08103297650814056,\n",
      "            0.166811004281044,\n",
      "            0.01956971548497677,\n",
      "            0.08272616565227509,\n",
      "            0.13912293314933777,\n",
      "            -0.0872218906879425,\n",
      "            -0.02036249078810215,\n",
      "            0.1707460880279541\n",
      "          ],\n",
      "          [\n",
      "            -0.13008169829845428,\n",
      "            0.16447536647319794,\n",
      "            -0.0678701251745224,\n",
      "            -0.061753690242767334,\n",
      "            0.015455201268196106,\n",
      "            0.16933123767375946,\n",
      "            -0.14592595398426056,\n",
      "            -0.04737769067287445,\n",
      "            0.1159854382276535,\n",
      "            0.025355160236358643,\n",
      "            -0.04044993221759796,\n",
      "            -0.1593834012746811,\n",
      "            0.01268063485622406,\n",
      "            0.02037644386291504,\n",
      "            -0.1413668543100357,\n",
      "            -0.09765665233135223,\n",
      "            -0.051778919994831085,\n",
      "            -0.04356542229652405,\n",
      "            0.006021946668624878,\n",
      "            -0.028079748153686523,\n",
      "            -0.009003490209579468,\n",
      "            0.02469143271446228,\n",
      "            0.061684995889663696,\n",
      "            -0.09444151818752289,\n",
      "            -0.08579213917255402,\n",
      "            -0.0405205637216568,\n",
      "            -0.17154891788959503,\n",
      "            0.04000768065452576,\n",
      "            -0.1654307246208191,\n",
      "            -0.12109552323818207,\n",
      "            0.1202973872423172,\n",
      "            -0.163898766040802,\n",
      "            0.016869619488716125,\n",
      "            0.10920675098896027,\n",
      "            -0.02756647765636444,\n",
      "            -0.1299324482679367,\n",
      "            -0.07421461492776871,\n",
      "            0.03879065811634064,\n",
      "            -0.07690459489822388,\n",
      "            -0.13101334869861603,\n",
      "            -0.0032251179218292236,\n",
      "            0.01598377525806427,\n",
      "            0.00383836030960083,\n",
      "            0.1256077140569687,\n",
      "            -0.019445374608039856,\n",
      "            -0.17060913145542145,\n",
      "            -0.02392461895942688,\n",
      "            0.1264399141073227,\n",
      "            -0.02449515461921692,\n",
      "            -0.00786009430885315,\n",
      "            -0.039752110838890076,\n",
      "            -0.11206189543008804,\n",
      "            -0.08384786546230316,\n",
      "            0.021766483783721924,\n",
      "            0.006979435682296753,\n",
      "            0.1408752053976059,\n",
      "            -0.012009277939796448,\n",
      "            -0.15870729088783264,\n",
      "            0.1327403038740158,\n",
      "            0.15276841819286346,\n",
      "            -0.0577433705329895,\n",
      "            -0.0825352892279625,\n",
      "            -0.1688140630722046,\n",
      "            -0.09595602005720139,\n",
      "            -0.056666307151317596,\n",
      "            0.12082113325595856,\n",
      "            -0.16116175055503845,\n",
      "            0.029618188738822937,\n",
      "            -0.006602644920349121,\n",
      "            0.07371412217617035,\n",
      "            0.12497670948505402,\n",
      "            0.10896448791027069,\n",
      "            0.08865855634212494,\n",
      "            -0.1185699999332428,\n",
      "            0.009304970502853394,\n",
      "            0.1465284377336502,\n",
      "            -0.13429611921310425,\n",
      "            -0.05573621392250061,\n",
      "            0.13505734503269196,\n",
      "            -0.08943375945091248,\n",
      "            -0.11981746554374695,\n",
      "            -0.0702439621090889,\n",
      "            0.1666414588689804,\n",
      "            0.05321279168128967,\n",
      "            0.031561702489852905,\n",
      "            -0.024381443858146667,\n",
      "            0.07725290954113007,\n",
      "            0.1604965478181839,\n",
      "            -0.1600779891014099,\n",
      "            -0.12286214530467987,\n",
      "            -0.10919886082410812,\n",
      "            0.09434829652309418,\n",
      "            -0.05818262696266174,\n",
      "            -0.07059108465909958,\n",
      "            0.0911790281534195,\n",
      "            0.012371405959129333,\n",
      "            -0.012555286288261414,\n",
      "            -0.11805061995983124,\n",
      "            -0.16831980645656586,\n",
      "            0.07558886706829071,\n",
      "            -0.0840269923210144,\n",
      "            -0.062069959938526154,\n",
      "            0.09024088084697723,\n",
      "            -0.1681300550699234,\n",
      "            0.06860107183456421,\n",
      "            -0.15364030003547668,\n",
      "            0.11802111566066742,\n",
      "            -0.10704267024993896,\n",
      "            -0.0709700658917427,\n",
      "            -0.162245512008667,\n",
      "            -0.11377592384815216,\n",
      "            -0.019692569971084595,\n",
      "            -0.1767660230398178,\n",
      "            -0.053834713995456696,\n",
      "            0.17299465835094452,\n",
      "            -0.15466709434986115,\n",
      "            0.02246697247028351,\n",
      "            0.034891098737716675,\n",
      "            0.1609567552804947,\n",
      "            0.17531393468379974,\n",
      "            -0.01701401174068451,\n",
      "            -0.12258507311344147,\n",
      "            0.0017643570899963379,\n",
      "            0.13563396036624908,\n",
      "            -0.12811626493930817,\n",
      "            -0.0079183429479599,\n",
      "            0.15870554745197296,\n",
      "            -0.043429791927337646\n",
      "          ],\n",
      "          [\n",
      "            0.07353905588388443,\n",
      "            0.13850387930870056,\n",
      "            0.1498906910419464,\n",
      "            0.09076002985239029,\n",
      "            -0.0987718254327774,\n",
      "            -0.09849099814891815,\n",
      "            -0.005670567508786917,\n",
      "            0.06437547504901886,\n",
      "            0.14215566217899323,\n",
      "            0.1351310759782791,\n",
      "            0.10534743964672089,\n",
      "            0.16124095022678375,\n",
      "            0.060350798070430756,\n",
      "            -0.06433110684156418,\n",
      "            0.016844479367136955,\n",
      "            -0.003506407141685486,\n",
      "            -0.011489901691675186,\n",
      "            0.02102602645754814,\n",
      "            -0.08613722026348114,\n",
      "            -0.04483921080827713,\n",
      "            -0.0729195848107338,\n",
      "            -0.02892884612083435,\n",
      "            0.11127936840057373,\n",
      "            -0.09936413168907166,\n",
      "            0.2538525462150574,\n",
      "            -0.09055151790380478,\n",
      "            -0.07780835777521133,\n",
      "            0.1028340756893158,\n",
      "            -0.15618133544921875,\n",
      "            0.17129269242286682,\n",
      "            0.17830339074134827,\n",
      "            0.19332255423069,\n",
      "            0.2062659114599228,\n",
      "            0.053698789328336716,\n",
      "            -0.10906428098678589,\n",
      "            -0.019500931724905968,\n",
      "            -0.09549392014741898,\n",
      "            0.14039526879787445,\n",
      "            0.040250588208436966,\n",
      "            -0.09438852965831757,\n",
      "            -0.021686282008886337,\n",
      "            0.0328608900308609,\n",
      "            0.099688820540905,\n",
      "            -0.026741374284029007,\n",
      "            -0.009977676905691624,\n",
      "            -0.09412763267755508,\n",
      "            0.10531947761774063,\n",
      "            -0.005906061269342899,\n",
      "            -0.15764185786247253,\n",
      "            -0.1675916612148285,\n",
      "            0.13579300045967102,\n",
      "            0.18856580555438995,\n",
      "            -0.030009876936674118,\n",
      "            0.09330026805400848,\n",
      "            0.18539008498191833,\n",
      "            -0.11504773050546646,\n",
      "            -0.0021112298127263784,\n",
      "            -0.009404183365404606,\n",
      "            0.16058847308158875,\n",
      "            0.17332977056503296,\n",
      "            0.151601180434227,\n",
      "            0.05768459290266037,\n",
      "            -0.11568604409694672,\n",
      "            0.01061177160590887,\n",
      "            -0.013442731462419033,\n",
      "            -0.0025895435828715563,\n",
      "            0.022292492911219597,\n",
      "            0.08376049250364304,\n",
      "            0.045766159892082214,\n",
      "            -0.056570570915937424,\n",
      "            0.09579338878393173,\n",
      "            -0.1275399625301361,\n",
      "            0.00039673468563705683,\n",
      "            0.1707894206047058,\n",
      "            0.09183663129806519,\n",
      "            -0.05017922818660736,\n",
      "            -0.04202856495976448,\n",
      "            0.0016344512114301324,\n",
      "            0.05980239436030388,\n",
      "            -0.03560349717736244,\n",
      "            0.20974738895893097,\n",
      "            0.05218296870589256,\n",
      "            0.1216701865196228,\n",
      "            -0.018412383273243904,\n",
      "            0.08098357915878296,\n",
      "            0.07264211028814316,\n",
      "            -0.007501676212996244,\n",
      "            -0.03466194123029709,\n",
      "            -0.05427753925323486,\n",
      "            -0.12462116777896881,\n",
      "            0.006229644641280174,\n",
      "            -0.007126483600586653,\n",
      "            0.06476932018995285,\n",
      "            0.06753397732973099,\n",
      "            -0.08374286442995071,\n",
      "            -0.02545054815709591,\n",
      "            -0.2029150128364563,\n",
      "            0.12298466265201569,\n",
      "            -0.12146810442209244,\n",
      "            -0.06638065725564957,\n",
      "            -0.06828733533620834,\n",
      "            0.06102069839835167,\n",
      "            -0.0428549088537693,\n",
      "            0.1728087067604065,\n",
      "            0.12911778688430786,\n",
      "            0.13965661823749542,\n",
      "            0.12456431239843369,\n",
      "            0.1734887808561325,\n",
      "            0.07220876961946487,\n",
      "            0.02203359268605709,\n",
      "            -0.09268802404403687,\n",
      "            0.1395575851202011,\n",
      "            -0.19381727278232574,\n",
      "            -0.10037735104560852,\n",
      "            -0.055724602192640305,\n",
      "            0.054388899356126785,\n",
      "            0.11424984037876129,\n",
      "            -0.06206353008747101,\n",
      "            -0.05142005905508995,\n",
      "            0.14953964948654175,\n",
      "            0.11600585281848907,\n",
      "            -0.1270662397146225,\n",
      "            0.01487061008810997,\n",
      "            -0.053496673703193665,\n",
      "            0.08810466527938843,\n",
      "            -0.1299683302640915,\n",
      "            -0.10069292038679123,\n",
      "            -0.17865505814552307\n",
      "          ],\n",
      "          [\n",
      "            0.160269096493721,\n",
      "            -0.06521864235401154,\n",
      "            -0.15773163735866547,\n",
      "            0.007545039989054203,\n",
      "            0.008719714358448982,\n",
      "            -0.11639239639043808,\n",
      "            0.008958792313933372,\n",
      "            0.19607850909233093,\n",
      "            -0.07522346079349518,\n",
      "            0.020680949091911316,\n",
      "            -0.027203239500522614,\n",
      "            0.12937700748443604,\n",
      "            -0.09438660740852356,\n",
      "            -0.034353043884038925,\n",
      "            -0.1332588791847229,\n",
      "            0.09929753839969635,\n",
      "            -0.08646652847528458,\n",
      "            0.03650740906596184,\n",
      "            0.14177295565605164,\n",
      "            -0.04225410521030426,\n",
      "            -0.10840365290641785,\n",
      "            0.13161996006965637,\n",
      "            0.028841618448495865,\n",
      "            -0.06982176005840302,\n",
      "            -0.0904470756649971,\n",
      "            -0.16145013272762299,\n",
      "            0.12128228694200516,\n",
      "            -0.14504940807819366,\n",
      "            0.022356798872351646,\n",
      "            0.05308232083916664,\n",
      "            0.16064903140068054,\n",
      "            -0.11388222873210907,\n",
      "            -0.030304929241538048,\n",
      "            0.20847080647945404,\n",
      "            0.015819236636161804,\n",
      "            -0.14180085062980652,\n",
      "            0.017520522698760033,\n",
      "            -0.09010478854179382,\n",
      "            -0.1973365843296051,\n",
      "            -0.0063676368445158005,\n",
      "            0.12472083419561386,\n",
      "            -0.1838991940021515,\n",
      "            -0.024386411532759666,\n",
      "            -0.11287382245063782,\n",
      "            0.11554629355669022,\n",
      "            -0.027396809309720993,\n",
      "            0.13241365551948547,\n",
      "            -0.05559476837515831,\n",
      "            0.09162715822458267,\n",
      "            0.04697180911898613,\n",
      "            0.06120834872126579,\n",
      "            -0.14950506389141083,\n",
      "            0.11811414361000061,\n",
      "            -0.120478555560112,\n",
      "            0.14287717640399933,\n",
      "            -0.19161376357078552,\n",
      "            -0.1468210518360138,\n",
      "            0.174281045794487,\n",
      "            -0.09120207279920578,\n",
      "            -0.16697603464126587,\n",
      "            -0.093137226998806,\n",
      "            0.02624281495809555,\n",
      "            0.02840510569512844,\n",
      "            -0.17918582260608673,\n",
      "            0.1618790477514267,\n",
      "            -0.1417124718427658,\n",
      "            0.16514290869235992,\n",
      "            0.05772810056805611,\n",
      "            -0.11601033806800842,\n",
      "            0.06758558750152588,\n",
      "            -0.1616651564836502,\n",
      "            0.009510884992778301,\n",
      "            -0.043838582932949066,\n",
      "            -0.0316133052110672,\n",
      "            0.12069565057754517,\n",
      "            0.053319983184337616,\n",
      "            -0.19507230818271637,\n",
      "            -0.04985245689749718,\n",
      "            0.02941821701824665,\n",
      "            -0.19014900922775269,\n",
      "            0.17084337770938873,\n",
      "            -0.11727125197649002,\n",
      "            0.12162603437900543,\n",
      "            0.047068510204553604,\n",
      "            0.0142754465341568,\n",
      "            -0.023742109537124634,\n",
      "            0.15121029317378998,\n",
      "            0.0794207975268364,\n",
      "            0.14715446531772614,\n",
      "            0.11472751200199127,\n",
      "            0.13718435168266296,\n",
      "            -0.062435369938611984,\n",
      "            -0.12546223402023315,\n",
      "            -0.046824339777231216,\n",
      "            0.14603224396705627,\n",
      "            0.10886145383119583,\n",
      "            0.18462182581424713,\n",
      "            -0.1517903208732605,\n",
      "            -0.047479160130023956,\n",
      "            -0.1446419507265091,\n",
      "            0.19991403818130493,\n",
      "            -0.12423627823591232,\n",
      "            -0.11812332272529602,\n",
      "            -0.08956911414861679,\n",
      "            -0.11609909683465958,\n",
      "            -0.01372992992401123,\n",
      "            -0.029382742941379547,\n",
      "            -0.17518562078475952,\n",
      "            0.14801275730133057,\n",
      "            0.12206879258155823,\n",
      "            -0.04193790256977081,\n",
      "            0.22209380567073822,\n",
      "            -0.04326958581805229,\n",
      "            0.012604773044586182,\n",
      "            0.02621687576174736,\n",
      "            -0.15930001437664032,\n",
      "            0.09870544821023941,\n",
      "            -0.05342002213001251,\n",
      "            -0.05294674634933472,\n",
      "            0.19794602692127228,\n",
      "            -0.018169352784752846,\n",
      "            0.051919300109148026,\n",
      "            0.12383319437503815,\n",
      "            -0.021348673850297928,\n",
      "            0.08289743959903717,\n",
      "            -0.07643672823905945,\n",
      "            0.06024283543229103,\n",
      "            0.029140455648303032\n",
      "          ]\n",
      "        ],\n",
      "        \"bias\": [\n",
      "          0.022532861679792404,\n",
      "          -0.008544926531612873,\n",
      "          -0.0024023649748414755,\n",
      "          0.052216608077287674,\n",
      "          -0.010179257951676846,\n",
      "          0.05869818106293678,\n",
      "          0.036779846996068954,\n",
      "          0.04147520288825035,\n",
      "          -0.0059929508715868,\n",
      "          0.0674053356051445,\n",
      "          -0.005549479741603136,\n",
      "          -0.006569490302354097,\n",
      "          0.028577562421560287,\n",
      "          0.013036402873694897,\n",
      "          -0.019164659082889557,\n",
      "          -0.0357515923678875,\n",
      "          0.0029702577739953995,\n",
      "          0.05805646628141403,\n",
      "          -0.0034280135296285152,\n",
      "          -0.004695003852248192,\n",
      "          0.09635303914546967,\n",
      "          -0.003911382984369993,\n",
      "          0.06090816482901573,\n",
      "          0.007855461910367012,\n",
      "          -0.025825487449765205,\n",
      "          0.0013165613636374474,\n",
      "          0.018573731184005737,\n",
      "          0.0,\n",
      "          -0.007584558334201574,\n",
      "          0.01250439416617155,\n",
      "          0.026972301304340363,\n",
      "          0.0038921001832932234,\n",
      "          -0.0009552250849083066,\n",
      "          0.06242164224386215,\n",
      "          -2.535715975682251e-05,\n",
      "          -0.00678465748205781,\n",
      "          0.044773124158382416,\n",
      "          -0.02834230661392212,\n",
      "          -0.02664891630411148,\n",
      "          -0.007691300939768553,\n",
      "          7.879398617660627e-05,\n",
      "          0.08567751944065094,\n",
      "          0.06875360757112503,\n",
      "          -0.01446205098181963,\n",
      "          -0.017502887174487114,\n",
      "          -0.020410316064953804,\n",
      "          -0.030178694054484367,\n",
      "          -0.012422431260347366,\n",
      "          0.03654389828443527,\n",
      "          0.029119078069925308,\n",
      "          -0.002528602024540305,\n",
      "          -0.0033458895049989223,\n",
      "          -0.029371216893196106,\n",
      "          0.042651623487472534,\n",
      "          -4.815114152734168e-06,\n",
      "          -4.833989078179002e-05,\n",
      "          0.05580221861600876,\n",
      "          0.0405927412211895,\n",
      "          0.10994056612253189,\n",
      "          -0.0003844964667223394,\n",
      "          -0.002037125639617443,\n",
      "          0.0,\n",
      "          -0.00816764123737812,\n",
      "          0.03377212584018707\n",
      "        ],\n",
      "        \"activation\": \"relu\"\n",
      "      },\n",
      "      {\n",
      "        \"matrix\": [\n",
      "          [\n",
      "            0.1098332330584526,\n",
      "            -0.1743934601545334,\n",
      "            0.02761848084628582,\n",
      "            0.29466885328292847,\n",
      "            0.11918555945158005,\n",
      "            -0.19514892995357513,\n",
      "            -0.18204577267169952,\n",
      "            0.10595692694187164,\n",
      "            -0.16014115512371063,\n",
      "            0.1432909369468689,\n",
      "            0.11641819030046463,\n",
      "            0.201338991522789,\n",
      "            -0.028737619519233704,\n",
      "            -0.10038722306489944,\n",
      "            0.19606870412826538,\n",
      "            -0.007970775477588177,\n",
      "            0.1636197417974472,\n",
      "            -0.0688382014632225,\n",
      "            -0.02880549430847168,\n",
      "            0.1754978746175766,\n",
      "            0.15481896698474884,\n",
      "            0.001236651442013681,\n",
      "            0.058179717510938644,\n",
      "            -0.011531597003340721,\n",
      "            0.1739450842142105,\n",
      "            0.1511915773153305,\n",
      "            -0.26143378019332886,\n",
      "            -0.2330508828163147,\n",
      "            0.015624039806425571,\n",
      "            0.28155839443206787,\n",
      "            -0.05460469052195549,\n",
      "            0.15871742367744446,\n",
      "            0.1839095801115036,\n",
      "            -0.06541077047586441,\n",
      "            0.2087295651435852,\n",
      "            -0.09358417987823486,\n",
      "            0.3370242416858673,\n",
      "            0.2201668918132782,\n",
      "            0.13832178711891174,\n",
      "            0.13884319365024567,\n",
      "            -0.16551941633224487,\n",
      "            0.13917769491672516,\n",
      "            -0.007915846072137356,\n",
      "            0.11853919178247452,\n",
      "            0.015501150861382484,\n",
      "            0.21061217784881592,\n",
      "            -0.12563958764076233,\n",
      "            -0.03591885417699814,\n",
      "            -0.18519814312458038,\n",
      "            -0.11894997954368591,\n",
      "            -0.04472353309392929,\n",
      "            0.14938437938690186,\n",
      "            -0.14572225511074066,\n",
      "            -0.2220364362001419,\n",
      "            -0.19160312414169312,\n",
      "            -0.20987693965435028,\n",
      "            -0.12267152965068817,\n",
      "            0.08834976702928543,\n",
      "            -0.08103864639997482,\n",
      "            0.11735522001981735,\n",
      "            0.22898119688034058,\n",
      "            0.23143458366394043,\n",
      "            0.18127493560314178,\n",
      "            0.14192654192447662\n",
      "          ],\n",
      "          [\n",
      "            -0.024768097326159477,\n",
      "            0.19242125749588013,\n",
      "            -0.12676852941513062,\n",
      "            0.0684991255402565,\n",
      "            -0.04472245275974274,\n",
      "            0.010465617291629314,\n",
      "            0.22429658472537994,\n",
      "            -0.21077659726142883,\n",
      "            -0.12788242101669312,\n",
      "            -0.06301216036081314,\n",
      "            -0.0926838293671608,\n",
      "            -0.10794103145599365,\n",
      "            0.1775265634059906,\n",
      "            -0.050121139734983444,\n",
      "            -0.13394218683242798,\n",
      "            -0.10478020459413528,\n",
      "            -0.189311683177948,\n",
      "            -0.08809726685285568,\n",
      "            0.1601903885602951,\n",
      "            0.16883091628551483,\n",
      "            -0.03659972548484802,\n",
      "            -0.15346510708332062,\n",
      "            0.11217770725488663,\n",
      "            -0.23453031480312347,\n",
      "            -0.21447418630123138,\n",
      "            -0.19951565563678741,\n",
      "            0.01957188919186592,\n",
      "            0.21173083782196045,\n",
      "            0.12389060854911804,\n",
      "            -0.01067834161221981,\n",
      "            -0.12730693817138672,\n",
      "            -0.013874102383852005,\n",
      "            -0.1954030841588974,\n",
      "            -0.07640927284955978,\n",
      "            -0.08023477345705032,\n",
      "            -0.0997103750705719,\n",
      "            0.18364669382572174,\n",
      "            0.22007864713668823,\n",
      "            0.012953435070812702,\n",
      "            0.11985836178064346,\n",
      "            -0.05037064105272293,\n",
      "            0.1268460750579834,\n",
      "            -0.1809946745634079,\n",
      "            -0.059373192489147186,\n",
      "            -0.1858721226453781,\n",
      "            0.19276481866836548,\n",
      "            0.042330700904130936,\n",
      "            -0.2251138687133789,\n",
      "            0.037168681621551514,\n",
      "            0.10727806389331818,\n",
      "            -0.03295299410820007,\n",
      "            -0.09642746299505234,\n",
      "            0.11653624475002289,\n",
      "            -0.10993536561727524,\n",
      "            -0.015734195709228516,\n",
      "            0.0637226328253746,\n",
      "            -0.0840386375784874,\n",
      "            0.22582970559597015,\n",
      "            -0.11360657215118408,\n",
      "            -0.19870425760746002,\n",
      "            0.0514565110206604,\n",
      "            0.0036661624908447266,\n",
      "            0.1199020966887474,\n",
      "            -0.19509708881378174\n",
      "          ],\n",
      "          [\n",
      "            -0.26841914653778076,\n",
      "            -0.16783873736858368,\n",
      "            0.1475629061460495,\n",
      "            0.0016619482776150107,\n",
      "            0.18691493570804596,\n",
      "            0.126002699136734,\n",
      "            -0.15375159680843353,\n",
      "            -0.10958918929100037,\n",
      "            -0.1486595720052719,\n",
      "            -0.12336883693933487,\n",
      "            0.07153762876987457,\n",
      "            0.22408729791641235,\n",
      "            0.13748754560947418,\n",
      "            -0.03197798132896423,\n",
      "            0.0726156234741211,\n",
      "            0.20904651284217834,\n",
      "            0.1341702938079834,\n",
      "            -0.20014949142932892,\n",
      "            -0.12612248957157135,\n",
      "            -0.16824927926063538,\n",
      "            0.14087209105491638,\n",
      "            -0.20257581770420074,\n",
      "            -0.20075620710849762,\n",
      "            0.1238880529999733,\n",
      "            0.13468241691589355,\n",
      "            0.08142724633216858,\n",
      "            -0.12983667850494385,\n",
      "            0.12950283288955688,\n",
      "            -0.10944139212369919,\n",
      "            0.06502094864845276,\n",
      "            0.06455236673355103,\n",
      "            0.10532453656196594,\n",
      "            0.011742603965103626,\n",
      "            -0.22811560332775116,\n",
      "            -0.05883112922310829,\n",
      "            0.041491445153951645,\n",
      "            -0.03862503916025162,\n",
      "            0.06370045989751816,\n",
      "            -0.1422959417104721,\n",
      "            -0.15665295720100403,\n",
      "            -0.23261183500289917,\n",
      "            0.1461312621831894,\n",
      "            0.23729251325130463,\n",
      "            0.1280319094657898,\n",
      "            0.20338214933872223,\n",
      "            -0.22604499757289886,\n",
      "            0.15153421461582184,\n",
      "            0.21691687405109406,\n",
      "            -0.08233002573251724,\n",
      "            -0.05063110217452049,\n",
      "            -0.1919446587562561,\n",
      "            -0.08455614000558853,\n",
      "            -0.06855632364749908,\n",
      "            0.10710462927818298,\n",
      "            0.13278377056121826,\n",
      "            0.16573886573314667,\n",
      "            0.20273198187351227,\n",
      "            -0.1898423433303833,\n",
      "            0.09388967603445053,\n",
      "            0.20117127895355225,\n",
      "            0.012927412986755371,\n",
      "            0.04757887125015259,\n",
      "            -0.24682623147964478,\n",
      "            0.0683952122926712\n",
      "          ],\n",
      "          [\n",
      "            -0.06114930287003517,\n",
      "            0.22983351349830627,\n",
      "            -0.09945470839738846,\n",
      "            -0.03189340606331825,\n",
      "            0.008448519743978977,\n",
      "            0.2699085772037506,\n",
      "            -0.11045117676258087,\n",
      "            -0.0919070690870285,\n",
      "            0.2227308303117752,\n",
      "            0.028331467881798744,\n",
      "            -0.21481212973594666,\n",
      "            0.02362820878624916,\n",
      "            0.1024402379989624,\n",
      "            -0.12349799275398254,\n",
      "            0.03914385288953781,\n",
      "            -0.08190687745809555,\n",
      "            -0.2312266230583191,\n",
      "            -0.13321320712566376,\n",
      "            -0.17536631226539612,\n",
      "            -0.0639946386218071,\n",
      "            0.19433358311653137,\n",
      "            -0.06941281259059906,\n",
      "            0.07990209758281708,\n",
      "            -0.12769955396652222,\n",
      "            -0.14239126443862915,\n",
      "            0.18504147231578827,\n",
      "            -0.15305253863334656,\n",
      "            -0.11998951435089111,\n",
      "            0.1833612471818924,\n",
      "            0.00834495946764946,\n",
      "            0.15569941699504852,\n",
      "            -0.061390284448862076,\n",
      "            0.10118841379880905,\n",
      "            -0.1951950341463089,\n",
      "            -0.21431350708007812,\n",
      "            -0.15203726291656494,\n",
      "            0.06401687860488892,\n",
      "            -0.20270338654518127,\n",
      "            0.2005196213722229,\n",
      "            -0.19534572958946228,\n",
      "            0.049728333950042725,\n",
      "            0.08531161397695541,\n",
      "            -0.08166451752185822,\n",
      "            0.1160137727856636,\n",
      "            -0.2129102349281311,\n",
      "            0.0034372122026979923,\n",
      "            -0.12375976890325546,\n",
      "            0.017666177824139595,\n",
      "            0.07753625512123108,\n",
      "            -0.1996370255947113,\n",
      "            0.24268265068531036,\n",
      "            -0.0460854172706604,\n",
      "            -0.23866452276706696,\n",
      "            0.15979646146297455,\n",
      "            0.08972370624542236,\n",
      "            0.16500715911388397,\n",
      "            0.1710767149925232,\n",
      "            -0.2427530139684677,\n",
      "            0.13922031223773956,\n",
      "            0.21095478534698486,\n",
      "            0.08068191260099411,\n",
      "            -0.018649518489837646,\n",
      "            -0.05043250322341919,\n",
      "            -0.007388363592326641\n",
      "          ],\n",
      "          [\n",
      "            -0.22228531539440155,\n",
      "            0.11118999868631363,\n",
      "            0.2099693864583969,\n",
      "            0.15619976818561554,\n",
      "            0.11119920760393143,\n",
      "            0.17012149095535278,\n",
      "            0.1329517513513565,\n",
      "            -0.10105767101049423,\n",
      "            -0.018132710829377174,\n",
      "            0.23887045681476593,\n",
      "            0.16834956407546997,\n",
      "            -0.22012808918952942,\n",
      "            -0.20935563743114471,\n",
      "            -0.09961412101984024,\n",
      "            0.07045717537403107,\n",
      "            -0.13903312385082245,\n",
      "            -0.07322336733341217,\n",
      "            -0.0197916179895401,\n",
      "            -0.06836210191249847,\n",
      "            -0.10491129010915756,\n",
      "            0.2500426471233368,\n",
      "            -0.1374734789133072,\n",
      "            0.10750535130500793,\n",
      "            -0.18689008057117462,\n",
      "            -0.08040710538625717,\n",
      "            -0.1690499633550644,\n",
      "            0.21470433473587036,\n",
      "            -0.15456312894821167,\n",
      "            -0.05407584458589554,\n",
      "            0.000750992854591459,\n",
      "            0.004766372032463551,\n",
      "            -0.20888136327266693,\n",
      "            -0.08600620925426483,\n",
      "            -0.1848316341638565,\n",
      "            0.15459400415420532,\n",
      "            -0.08843754231929779,\n",
      "            -0.2684124708175659,\n",
      "            0.0867004245519638,\n",
      "            0.11665496230125427,\n",
      "            0.11691980063915253,\n",
      "            -0.2184899002313614,\n",
      "            -0.09103509783744812,\n",
      "            0.14276091754436493,\n",
      "            -0.2813987731933594,\n",
      "            0.19112887978553772,\n",
      "            -0.30260559916496277,\n",
      "            -0.02679610624909401,\n",
      "            -0.20944741368293762,\n",
      "            0.05938474088907242,\n",
      "            -0.015653792768716812,\n",
      "            0.14625908434391022,\n",
      "            0.16828978061676025,\n",
      "            -0.07296377420425415,\n",
      "            0.30726414918899536,\n",
      "            0.1731013059616089,\n",
      "            0.03761252015829086,\n",
      "            0.3951144814491272,\n",
      "            0.1767333447933197,\n",
      "            0.14073383808135986,\n",
      "            0.009448530152440071,\n",
      "            -0.184211865067482,\n",
      "            0.16380751132965088,\n",
      "            0.047042153775691986,\n",
      "            0.21880200505256653\n",
      "          ],\n",
      "          [\n",
      "            0.15941622853279114,\n",
      "            0.07190924882888794,\n",
      "            -0.18839454650878906,\n",
      "            0.14431358873844147,\n",
      "            0.010389694012701511,\n",
      "            0.23084038496017456,\n",
      "            -0.1786399632692337,\n",
      "            0.009064528159797192,\n",
      "            -0.24671238660812378,\n",
      "            -0.02629014104604721,\n",
      "            -0.05300136283040047,\n",
      "            0.14455777406692505,\n",
      "            0.23443257808685303,\n",
      "            -0.09595110267400742,\n",
      "            0.05376553535461426,\n",
      "            -0.05347941070795059,\n",
      "            0.04957282543182373,\n",
      "            -0.23904190957546234,\n",
      "            0.19152362644672394,\n",
      "            0.18424023687839508,\n",
      "            -0.13946199417114258,\n",
      "            0.03487234562635422,\n",
      "            0.0718122199177742,\n",
      "            -0.11514578759670258,\n",
      "            -0.03807847574353218,\n",
      "            -0.2348300814628601,\n",
      "            0.1338847130537033,\n",
      "            -0.13205647468566895,\n",
      "            0.20599447190761566,\n",
      "            -0.21311019361019135,\n",
      "            0.06675609946250916,\n",
      "            -0.024333959445357323,\n",
      "            0.11935358494520187,\n",
      "            -0.21956439316272736,\n",
      "            0.10986518859863281,\n",
      "            0.06201742962002754,\n",
      "            -0.1715984344482422,\n",
      "            0.23950085043907166,\n",
      "            -0.10361865162849426,\n",
      "            -0.012911077588796616,\n",
      "            -0.011926472187042236,\n",
      "            -0.1384906768798828,\n",
      "            0.15261249244213104,\n",
      "            0.24918948113918304,\n",
      "            0.12118381261825562,\n",
      "            -0.10182386636734009,\n",
      "            -0.0026646375190466642,\n",
      "            0.18719884753227234,\n",
      "            0.2116451859474182,\n",
      "            -0.19521643221378326,\n",
      "            0.03178800642490387,\n",
      "            -0.16619066894054413,\n",
      "            -0.22777153551578522,\n",
      "            0.029307493939995766,\n",
      "            -0.12989366054534912,\n",
      "            -0.22477635741233826,\n",
      "            -0.20143380761146545,\n",
      "            -0.13911044597625732,\n",
      "            0.07829280942678452,\n",
      "            0.20619064569473267,\n",
      "            0.0688113197684288,\n",
      "            0.01164865493774414,\n",
      "            0.13630710542201996,\n",
      "            0.025966888293623924\n",
      "          ],\n",
      "          [\n",
      "            -0.17343956232070923,\n",
      "            -0.18421822786331177,\n",
      "            -0.0011478662490844727,\n",
      "            0.03206528723239899,\n",
      "            0.0383760929107666,\n",
      "            0.14872115850448608,\n",
      "            -0.2199183702468872,\n",
      "            -0.03534315153956413,\n",
      "            -0.2240445613861084,\n",
      "            0.2451401799917221,\n",
      "            -0.05997443199157715,\n",
      "            0.044249277561903,\n",
      "            0.09199754148721695,\n",
      "            -0.03238266706466675,\n",
      "            -0.20035678148269653,\n",
      "            0.04287904500961304,\n",
      "            -0.14873862266540527,\n",
      "            -0.22291404008865356,\n",
      "            0.061058759689331055,\n",
      "            -0.16479843854904175,\n",
      "            -0.00026977446395903826,\n",
      "            -0.011476099491119385,\n",
      "            0.14483416080474854,\n",
      "            -0.007512528914958239,\n",
      "            -0.03596764802932739,\n",
      "            -0.1925678253173828,\n",
      "            0.16044342517852783,\n",
      "            -0.19639939069747925,\n",
      "            0.005384087562561035,\n",
      "            -0.0816127136349678,\n",
      "            -0.14133745431900024,\n",
      "            -0.24508190155029297,\n",
      "            -0.06332826614379883,\n",
      "            0.0171967763453722,\n",
      "            0.09210538864135742,\n",
      "            0.15818309783935547,\n",
      "            -0.046217918395996094,\n",
      "            -0.16329419612884521,\n",
      "            -0.1420900821685791,\n",
      "            -0.061321921646595,\n",
      "            -0.023643553256988525,\n",
      "            -0.0619632862508297,\n",
      "            -0.058709517121315,\n",
      "            0.22998028993606567,\n",
      "            -0.10677576065063477,\n",
      "            -0.09625524282455444,\n",
      "            0.002432827604934573,\n",
      "            -0.17754977941513062,\n",
      "            -0.08708403259515762,\n",
      "            -0.16090911626815796,\n",
      "            -0.24964433908462524,\n",
      "            -0.22250837087631226,\n",
      "            0.1574687957763672,\n",
      "            -0.07155579328536987,\n",
      "            -0.15511953830718994,\n",
      "            -0.20025736093521118,\n",
      "            -0.02105160988867283,\n",
      "            -0.15213322639465332,\n",
      "            0.09878549724817276,\n",
      "            -0.11062479019165039,\n",
      "            -0.1962888240814209,\n",
      "            -0.17517513036727905,\n",
      "            0.1370985507965088,\n",
      "            0.20666275918483734\n",
      "          ],\n",
      "          [\n",
      "            -0.09920234978199005,\n",
      "            0.1745941936969757,\n",
      "            -0.2205066680908203,\n",
      "            0.15063536167144775,\n",
      "            0.05578120797872543,\n",
      "            -0.2086848020553589,\n",
      "            -0.14394602179527283,\n",
      "            -0.2411111742258072,\n",
      "            0.07083147019147873,\n",
      "            0.16439667344093323,\n",
      "            -0.11509043723344803,\n",
      "            -0.0017277657752856612,\n",
      "            -0.183193638920784,\n",
      "            0.04098782315850258,\n",
      "            -0.21327006816864014,\n",
      "            -0.09206518530845642,\n",
      "            -0.15824425220489502,\n",
      "            -0.2477477788925171,\n",
      "            -0.11448954045772552,\n",
      "            -0.18610000610351562,\n",
      "            0.1365879327058792,\n",
      "            -0.018737200647592545,\n",
      "            0.2270088940858841,\n",
      "            -0.06410511583089828,\n",
      "            -0.004897326231002808,\n",
      "            -0.04141930863261223,\n",
      "            0.0637885183095932,\n",
      "            0.1605241298675537,\n",
      "            -0.02108617126941681,\n",
      "            0.21604348719120026,\n",
      "            0.17729540169239044,\n",
      "            0.1396549493074417,\n",
      "            -0.18947063386440277,\n",
      "            -0.15134036540985107,\n",
      "            -0.08155205100774765,\n",
      "            -0.02375349961221218,\n",
      "            -0.18085773289203644,\n",
      "            0.17027555406093597,\n",
      "            -0.20532691478729248,\n",
      "            -0.12185472995042801,\n",
      "            -0.011098952032625675,\n",
      "            0.07717788964509964,\n",
      "            -0.12902341783046722,\n",
      "            0.24622508883476257,\n",
      "            0.12139385938644409,\n",
      "            -0.18536831438541412,\n",
      "            0.17936456203460693,\n",
      "            0.2114318609237671,\n",
      "            -0.1918019950389862,\n",
      "            -0.10660100728273392,\n",
      "            0.10443796962499619,\n",
      "            -0.2504674196243286,\n",
      "            0.0024171462282538414,\n",
      "            -0.004632942844182253,\n",
      "            0.14826732873916626,\n",
      "            -0.20908264815807343,\n",
      "            -0.12448783963918686,\n",
      "            -0.16133694350719452,\n",
      "            -0.2033444344997406,\n",
      "            0.24132925271987915,\n",
      "            0.16053521633148193,\n",
      "            -0.10666966438293457,\n",
      "            0.18613411486148834,\n",
      "            0.033163171261548996\n",
      "          ],\n",
      "          [\n",
      "            0.22088003158569336,\n",
      "            0.15118420124053955,\n",
      "            -0.023540575057268143,\n",
      "            0.2038494199514389,\n",
      "            -0.22210898995399475,\n",
      "            0.18047431111335754,\n",
      "            -0.02882716804742813,\n",
      "            0.19977517426013947,\n",
      "            -0.2134736180305481,\n",
      "            0.1751902550458908,\n",
      "            -0.06836134940385818,\n",
      "            -0.01873028092086315,\n",
      "            -0.08186674863100052,\n",
      "            0.01899200864136219,\n",
      "            -0.19263231754302979,\n",
      "            -0.10510805249214172,\n",
      "            -0.18338029086589813,\n",
      "            0.23274394869804382,\n",
      "            0.2348659783601761,\n",
      "            0.17872001230716705,\n",
      "            0.004697291646152735,\n",
      "            -0.1356724053621292,\n",
      "            0.0667944923043251,\n",
      "            0.159682959318161,\n",
      "            0.19674226641654968,\n",
      "            0.20862612128257751,\n",
      "            0.10808759927749634,\n",
      "            0.23769861459732056,\n",
      "            -0.1592552363872528,\n",
      "            0.22566956281661987,\n",
      "            -0.2310243546962738,\n",
      "            -0.08993127197027206,\n",
      "            0.1466251015663147,\n",
      "            0.2295360267162323,\n",
      "            -0.0052380310371518135,\n",
      "            0.05846811458468437,\n",
      "            0.08948305994272232,\n",
      "            0.13621269166469574,\n",
      "            -0.18988919258117676,\n",
      "            0.07412829250097275,\n",
      "            0.20349431037902832,\n",
      "            -0.03244805708527565,\n",
      "            -0.11796223372220993,\n",
      "            -0.05444982275366783,\n",
      "            0.0538904182612896,\n",
      "            0.3081393241882324,\n",
      "            0.08558345586061478,\n",
      "            0.21393902599811554,\n",
      "            -0.20314887166023254,\n",
      "            0.04529796913266182,\n",
      "            -0.08831777423620224,\n",
      "            -0.1154778003692627,\n",
      "            0.2534322142601013,\n",
      "            0.1703919619321823,\n",
      "            -0.062292516231536865,\n",
      "            0.1466810703277588,\n",
      "            -0.31159964203834534,\n",
      "            0.10834881663322449,\n",
      "            -0.026776310056447983,\n",
      "            0.009840314276516438,\n",
      "            0.03987899795174599,\n",
      "            -0.23755836486816406,\n",
      "            -0.183510422706604,\n",
      "            -0.2191687375307083\n",
      "          ],\n",
      "          [\n",
      "            -0.06599026173353195,\n",
      "            -0.11258397996425629,\n",
      "            0.0488472618162632,\n",
      "            0.0876176729798317,\n",
      "            0.22128434479236603,\n",
      "            -0.14698609709739685,\n",
      "            -0.21161499619483948,\n",
      "            -0.011010185815393925,\n",
      "            0.246762216091156,\n",
      "            -0.23418761789798737,\n",
      "            -0.069045290350914,\n",
      "            0.012619495391845703,\n",
      "            -0.08182768523693085,\n",
      "            0.0924036055803299,\n",
      "            -0.09166473150253296,\n",
      "            -0.12438438087701797,\n",
      "            -0.10128939151763916,\n",
      "            0.08003424108028412,\n",
      "            -0.21764394640922546,\n",
      "            -0.025065109133720398,\n",
      "            -0.11854714155197144,\n",
      "            0.02352818474173546,\n",
      "            -0.24906688928604126,\n",
      "            -0.0698295384645462,\n",
      "            0.000451424130005762,\n",
      "            -0.03149169310927391,\n",
      "            -0.21487711369991302,\n",
      "            0.15879809856414795,\n",
      "            0.04006245732307434,\n",
      "            0.10929485410451889,\n",
      "            0.041020553559064865,\n",
      "            -0.006180741358548403,\n",
      "            -0.11019456386566162,\n",
      "            -0.06847462058067322,\n",
      "            0.24984818696975708,\n",
      "            -0.12570425868034363,\n",
      "            0.14425423741340637,\n",
      "            -0.09076675027608871,\n",
      "            0.054881513118743896,\n",
      "            -0.23321707546710968,\n",
      "            -0.036848284304142,\n",
      "            -0.1933373212814331,\n",
      "            -0.20770227909088135,\n",
      "            0.04085201397538185,\n",
      "            0.166708841919899,\n",
      "            0.010435660369694233,\n",
      "            0.015699297189712524,\n",
      "            0.16069479286670685,\n",
      "            -0.019458958879113197,\n",
      "            0.05989581346511841,\n",
      "            0.07605258375406265,\n",
      "            -0.05648856610059738,\n",
      "            0.24910111725330353,\n",
      "            0.16445600986480713,\n",
      "            0.052717626094818115,\n",
      "            -0.049445152282714844,\n",
      "            0.12723663449287415,\n",
      "            0.13302233815193176,\n",
      "            0.07683660835027695,\n",
      "            0.09369860589504242,\n",
      "            -0.1647905707359314,\n",
      "            0.14713555574417114,\n",
      "            -0.09186992794275284,\n",
      "            -0.0936262235045433\n",
      "          ],\n",
      "          [\n",
      "            0.034654319286346436,\n",
      "            0.009994745254516602,\n",
      "            -0.22347897291183472,\n",
      "            0.08014810085296631,\n",
      "            0.08918589353561401,\n",
      "            0.07594883441925049,\n",
      "            -0.24513965845108032,\n",
      "            -0.004952669143676758,\n",
      "            0.10240459442138672,\n",
      "            -0.1697675585746765,\n",
      "            0.08513951301574707,\n",
      "            -0.1473943591117859,\n",
      "            0.1833840012550354,\n",
      "            -0.22581905126571655,\n",
      "            -0.05739104747772217,\n",
      "            -0.23218673467636108,\n",
      "            -0.10635673999786377,\n",
      "            -0.23604488372802734,\n",
      "            0.012416720390319824,\n",
      "            -0.21426552534103394,\n",
      "            -0.15738898515701294,\n",
      "            0.1025267243385315,\n",
      "            -0.025839269161224365,\n",
      "            -0.08011382818222046,\n",
      "            -0.13464349508285522,\n",
      "            -0.23652446269989014,\n",
      "            -0.00841456651687622,\n",
      "            0.02959543466567993,\n",
      "            -0.2255936861038208,\n",
      "            -0.13240641355514526,\n",
      "            -0.14777129888534546,\n",
      "            -0.09700757265090942,\n",
      "            -0.11127185821533203,\n",
      "            -0.0027521848678588867,\n",
      "            0.18953466415405273,\n",
      "            0.14653033018112183,\n",
      "            -0.13118910789489746,\n",
      "            -0.16892802715301514,\n",
      "            -0.07592219114303589,\n",
      "            0.05241960287094116,\n",
      "            0.23291456699371338,\n",
      "            -0.008068442344665527,\n",
      "            -0.013737499713897705,\n",
      "            0.1355571746826172,\n",
      "            0.08857005834579468,\n",
      "            0.19444584846496582,\n",
      "            -0.08785480260848999,\n",
      "            -0.1785649061203003,\n",
      "            -0.13793230056762695,\n",
      "            0.05087238550186157,\n",
      "            0.035571932792663574,\n",
      "            -0.015563547611236572,\n",
      "            -0.17380154132843018,\n",
      "            -0.04465150833129883,\n",
      "            0.1245378851890564,\n",
      "            -0.037682533264160156,\n",
      "            -0.15161997079849243,\n",
      "            -0.06292563676834106,\n",
      "            0.03292810916900635,\n",
      "            0.16897666454315186,\n",
      "            0.07013595104217529,\n",
      "            -0.08127939701080322,\n",
      "            0.12900257110595703,\n",
      "            0.014603376388549805\n",
      "          ],\n",
      "          [\n",
      "            0.35606861114501953,\n",
      "            0.0003957170993089676,\n",
      "            0.008849927224218845,\n",
      "            -0.05159781500697136,\n",
      "            0.03320376202464104,\n",
      "            0.04176613688468933,\n",
      "            -0.16950030624866486,\n",
      "            0.05111512541770935,\n",
      "            0.1271328330039978,\n",
      "            -0.19483929872512817,\n",
      "            0.03416261076927185,\n",
      "            -0.2352234274148941,\n",
      "            0.0735650509595871,\n",
      "            0.36284223198890686,\n",
      "            0.012171073816716671,\n",
      "            0.07354652136564255,\n",
      "            -0.07797916978597641,\n",
      "            0.1884399801492691,\n",
      "            -0.04955929517745972,\n",
      "            -0.070719875395298,\n",
      "            0.256917268037796,\n",
      "            0.12220285832881927,\n",
      "            -0.29886725544929504,\n",
      "            0.24529823660850525,\n",
      "            -0.24930782616138458,\n",
      "            -0.06790147721767426,\n",
      "            -0.12797069549560547,\n",
      "            -0.07307666540145874,\n",
      "            -0.1298757791519165,\n",
      "            -0.03923644870519638,\n",
      "            -0.22714278101921082,\n",
      "            0.02911095879971981,\n",
      "            0.05475792661309242,\n",
      "            -0.08308496326208115,\n",
      "            0.026266515254974365,\n",
      "            -0.1262134611606598,\n",
      "            0.19700057804584503,\n",
      "            -0.13297215104103088,\n",
      "            -0.2445688545703888,\n",
      "            0.15389706194400787,\n",
      "            0.20256423950195312,\n",
      "            0.1950094997882843,\n",
      "            0.207920104265213,\n",
      "            0.06677312403917313,\n",
      "            -0.23124276101589203,\n",
      "            0.39198869466781616,\n",
      "            0.062134530395269394,\n",
      "            0.08322110027074814,\n",
      "            -0.13727499544620514,\n",
      "            0.19195254147052765,\n",
      "            -0.15477481484413147,\n",
      "            0.05627691373229027,\n",
      "            0.18761666119098663,\n",
      "            -0.2291145622730255,\n",
      "            -0.17718583345413208,\n",
      "            -0.1999618113040924,\n",
      "            0.08781667053699493,\n",
      "            -0.06444240361452103,\n",
      "            0.10205541551113129,\n",
      "            0.009493186138570309,\n",
      "            0.19812709093093872,\n",
      "            -0.18592649698257446,\n",
      "            0.054830774664878845,\n",
      "            -0.17083029448986053\n",
      "          ],\n",
      "          [\n",
      "            -0.18827031552791595,\n",
      "            0.1714237928390503,\n",
      "            0.13795854151248932,\n",
      "            0.09700479358434677,\n",
      "            0.18612928688526154,\n",
      "            -0.18005451560020447,\n",
      "            0.13907915353775024,\n",
      "            0.09105782955884933,\n",
      "            0.1679822951555252,\n",
      "            0.057564739137887955,\n",
      "            0.2000836730003357,\n",
      "            0.2386312484741211,\n",
      "            0.06424222141504288,\n",
      "            0.020552780479192734,\n",
      "            0.09453167766332626,\n",
      "            0.12329891324043274,\n",
      "            -0.035271160304546356,\n",
      "            0.20567552745342255,\n",
      "            -0.059284161776304245,\n",
      "            -0.14419996738433838,\n",
      "            -0.06739891320466995,\n",
      "            0.016222842037677765,\n",
      "            0.17142555117607117,\n",
      "            0.020885394886136055,\n",
      "            0.012124698609113693,\n",
      "            0.19659750163555145,\n",
      "            -0.265004426240921,\n",
      "            0.11747533082962036,\n",
      "            0.22638913989067078,\n",
      "            0.14366212487220764,\n",
      "            -0.22363276779651642,\n",
      "            0.006363054271787405,\n",
      "            0.07723367214202881,\n",
      "            -0.11891766637563705,\n",
      "            0.02877199463546276,\n",
      "            -0.1970462054014206,\n",
      "            0.14954330027103424,\n",
      "            -0.047386690974235535,\n",
      "            -0.18442881107330322,\n",
      "            0.12253734469413757,\n",
      "            -0.1334196925163269,\n",
      "            0.19882741570472717,\n",
      "            0.24217772483825684,\n",
      "            -0.018159927800297737,\n",
      "            -0.09426427632570267,\n",
      "            -0.07261651754379272,\n",
      "            0.02526862360537052,\n",
      "            0.1868503987789154,\n",
      "            0.15271487832069397,\n",
      "            -0.02284737303853035,\n",
      "            0.09001128375530243,\n",
      "            0.02198546752333641,\n",
      "            0.23204727470874786,\n",
      "            -0.22173528373241425,\n",
      "            0.10726410150527954,\n",
      "            0.12812869250774384,\n",
      "            -0.1739083081483841,\n",
      "            -0.13868916034698486,\n",
      "            0.009742049500346184,\n",
      "            -0.06834783405065536,\n",
      "            0.006875780876725912,\n",
      "            -0.09968394041061401,\n",
      "            0.06156857684254646,\n",
      "            0.028336962684988976\n",
      "          ],\n",
      "          [\n",
      "            0.1889425814151764,\n",
      "            0.11727087199687958,\n",
      "            -0.18911193311214447,\n",
      "            -0.17321079969406128,\n",
      "            0.1725168526172638,\n",
      "            -0.0430319681763649,\n",
      "            0.0208654273301363,\n",
      "            0.12528032064437866,\n",
      "            -0.11964111030101776,\n",
      "            0.11088893562555313,\n",
      "            -0.1955072283744812,\n",
      "            0.1665906310081482,\n",
      "            0.18868601322174072,\n",
      "            0.06865038722753525,\n",
      "            -0.06665730476379395,\n",
      "            0.2482234537601471,\n",
      "            -0.21276801824569702,\n",
      "            -0.21642860770225525,\n",
      "            -0.1337728351354599,\n",
      "            -0.0012042172020301223,\n",
      "            -0.0223632100969553,\n",
      "            -0.19350315630435944,\n",
      "            -0.18458227813243866,\n",
      "            0.1800726354122162,\n",
      "            0.14776863157749176,\n",
      "            0.08306746184825897,\n",
      "            0.13131260871887207,\n",
      "            0.12981492280960083,\n",
      "            -0.14981192350387573,\n",
      "            -0.11252126097679138,\n",
      "            0.15170355141162872,\n",
      "            0.09583141654729843,\n",
      "            0.13765953481197357,\n",
      "            -0.18988671898841858,\n",
      "            0.15053090453147888,\n",
      "            -0.2041914016008377,\n",
      "            -0.033072274178266525,\n",
      "            0.15226306021213531,\n",
      "            0.21998582780361176,\n",
      "            -0.06812391430139542,\n",
      "            -0.23183976113796234,\n",
      "            0.1055317372083664,\n",
      "            0.007825972512364388,\n",
      "            -0.07219871133565903,\n",
      "            -0.24418845772743225,\n",
      "            -0.20499883592128754,\n",
      "            -0.13730239868164062,\n",
      "            -0.24683617055416107,\n",
      "            -0.05668635293841362,\n",
      "            0.10449884831905365,\n",
      "            -0.1905071884393692,\n",
      "            0.03850937262177467,\n",
      "            -0.05136485770344734,\n",
      "            0.1457449048757553,\n",
      "            -0.1874895691871643,\n",
      "            -0.04067825525999069,\n",
      "            0.08538789302110672,\n",
      "            -0.08685377985239029,\n",
      "            -0.153333842754364,\n",
      "            -0.12320379912853241,\n",
      "            -0.1560174822807312,\n",
      "            0.0543329119682312,\n",
      "            -0.06346398591995239,\n",
      "            -0.08751855045557022\n",
      "          ],\n",
      "          [\n",
      "            -0.1433766484260559,\n",
      "            -0.1754646897315979,\n",
      "            -0.10972630977630615,\n",
      "            -0.1933826208114624,\n",
      "            -0.14088398218154907,\n",
      "            0.1934993863105774,\n",
      "            -0.11043262481689453,\n",
      "            -0.19850486516952515,\n",
      "            -0.12070053815841675,\n",
      "            -0.2251567244529724,\n",
      "            -0.00814974308013916,\n",
      "            -0.056499183177948,\n",
      "            -0.22754788398742676,\n",
      "            -0.13911235332489014,\n",
      "            -0.2116376757621765,\n",
      "            0.011062085628509521,\n",
      "            0.18384027481079102,\n",
      "            -0.0023465752601623535,\n",
      "            -0.07521557807922363,\n",
      "            0.20982563495635986,\n",
      "            -5.441904067993164e-05,\n",
      "            0.1855493187904358,\n",
      "            0.20261722803115845,\n",
      "            -0.20944547653198242,\n",
      "            -0.23353677988052368,\n",
      "            -0.04824483394622803,\n",
      "            -0.19114112854003906,\n",
      "            0.10420477390289307,\n",
      "            0.035135090351104736,\n",
      "            -0.20425301790237427,\n",
      "            0.06647098064422607,\n",
      "            0.13656312227249146,\n",
      "            -0.2476578950881958,\n",
      "            -0.20585542917251587,\n",
      "            -0.18270838260650635,\n",
      "            0.10682123899459839,\n",
      "            -0.1484616994857788,\n",
      "            -0.2021932601928711,\n",
      "            -0.15860450267791748,\n",
      "            -0.053991496562957764,\n",
      "            -0.08936440944671631,\n",
      "            -0.10260409116744995,\n",
      "            0.05218249559402466,\n",
      "            0.06165963411331177,\n",
      "            0.21355944871902466,\n",
      "            -0.2082781195640564,\n",
      "            -0.18452787399291992,\n",
      "            0.014945626258850098,\n",
      "            -0.07465124130249023,\n",
      "            0.12906783819198608,\n",
      "            -0.16876906156539917,\n",
      "            0.062229812145233154,\n",
      "            0.14463627338409424,\n",
      "            -0.19510871171951294,\n",
      "            -0.22973334789276123,\n",
      "            -0.22382497787475586,\n",
      "            0.2156481146812439,\n",
      "            0.20202994346618652,\n",
      "            0.009458482265472412,\n",
      "            -0.15830230712890625,\n",
      "            -0.19626712799072266,\n",
      "            -0.14819598197937012,\n",
      "            0.0688776969909668,\n",
      "            0.07166880369186401\n",
      "          ],\n",
      "          [\n",
      "            0.20057281851768494,\n",
      "            -0.16695551574230194,\n",
      "            0.2160872519016266,\n",
      "            0.08376256376504898,\n",
      "            0.014724207110702991,\n",
      "            0.08036577701568604,\n",
      "            0.27056068181991577,\n",
      "            -0.11713295429944992,\n",
      "            0.23617593944072723,\n",
      "            -0.02601662091910839,\n",
      "            0.33298826217651367,\n",
      "            0.17155690491199493,\n",
      "            -0.13094328343868256,\n",
      "            0.30861321091651917,\n",
      "            0.13529641926288605,\n",
      "            0.16324953734874725,\n",
      "            -0.028860775753855705,\n",
      "            -0.12367095053195953,\n",
      "            -0.09401647746562958,\n",
      "            0.21042481064796448,\n",
      "            0.2753142714500427,\n",
      "            0.12795835733413696,\n",
      "            -0.09655322879552841,\n",
      "            0.22618666291236877,\n",
      "            -0.0009746940922923386,\n",
      "            -0.07161737233400345,\n",
      "            -0.13037873804569244,\n",
      "            -0.1845279335975647,\n",
      "            -0.13155843317508698,\n",
      "            0.21959710121154785,\n",
      "            0.03962930291891098,\n",
      "            -0.11036382615566254,\n",
      "            -0.1438552588224411,\n",
      "            0.247709721326828,\n",
      "            0.08879180997610092,\n",
      "            0.16301889717578888,\n",
      "            0.3507472574710846,\n",
      "            -0.24771028757095337,\n",
      "            -0.027978049591183662,\n",
      "            -0.2170528769493103,\n",
      "            -0.14424335956573486,\n",
      "            0.29000359773635864,\n",
      "            -0.0012444754829630256,\n",
      "            0.18583856523036957,\n",
      "            -0.11823025345802307,\n",
      "            0.06600786000490189,\n",
      "            -0.1806677281856537,\n",
      "            0.2470885068178177,\n",
      "            -0.32671162486076355,\n",
      "            0.10297257453203201,\n",
      "            -0.28303027153015137,\n",
      "            0.20285804569721222,\n",
      "            0.0428495928645134,\n",
      "            -0.22174806892871857,\n",
      "            0.07640182971954346,\n",
      "            0.1245456412434578,\n",
      "            -0.1846781224012375,\n",
      "            0.2786073684692383,\n",
      "            0.1297590732574463,\n",
      "            -0.05101179704070091,\n",
      "            -0.15154476463794708,\n",
      "            0.18864130973815918,\n",
      "            0.12362893670797348,\n",
      "            -0.23819243907928467\n",
      "          ],\n",
      "          [\n",
      "            0.33670565485954285,\n",
      "            -0.05972122400999069,\n",
      "            -0.24090158939361572,\n",
      "            -0.02270517684519291,\n",
      "            -0.05767469108104706,\n",
      "            -0.18254543840885162,\n",
      "            0.13112883269786835,\n",
      "            -0.0617220476269722,\n",
      "            0.09808547794818878,\n",
      "            -0.1600455939769745,\n",
      "            0.07725407928228378,\n",
      "            -0.10651051253080368,\n",
      "            -0.2334260493516922,\n",
      "            0.05327595770359039,\n",
      "            0.1642797738313675,\n",
      "            0.2952960133552551,\n",
      "            -0.15162985026836395,\n",
      "            0.3299642503261566,\n",
      "            0.22573675215244293,\n",
      "            -0.12682881951332092,\n",
      "            0.32114773988723755,\n",
      "            0.1031491681933403,\n",
      "            -0.312574565410614,\n",
      "            0.20394985377788544,\n",
      "            0.2485869824886322,\n",
      "            0.19196508824825287,\n",
      "            -0.2769600450992584,\n",
      "            -0.060983121395111084,\n",
      "            0.02690119855105877,\n",
      "            0.13052649796009064,\n",
      "            -0.21627722680568695,\n",
      "            0.008442649617791176,\n",
      "            0.17972975969314575,\n",
      "            -0.017038902267813683,\n",
      "            -0.0466032512485981,\n",
      "            0.20448529720306396,\n",
      "            0.34735190868377686,\n",
      "            0.08244746178388596,\n",
      "            -0.19208769500255585,\n",
      "            0.14986500144004822,\n",
      "            -0.2415914684534073,\n",
      "            0.055741772055625916,\n",
      "            -0.008661371655762196,\n",
      "            0.33447787165641785,\n",
      "            0.20961718261241913,\n",
      "            0.1056118905544281,\n",
      "            -0.2017938196659088,\n",
      "            -0.24346427619457245,\n",
      "            -0.2337125837802887,\n",
      "            0.16120560467243195,\n",
      "            -0.010851379483938217,\n",
      "            0.0113460598513484,\n",
      "            0.05635175481438637,\n",
      "            0.02110983058810234,\n",
      "            0.07644706964492798,\n",
      "            -0.07876882702112198,\n",
      "            -0.31457194685935974,\n",
      "            0.09377358108758926,\n",
      "            0.26278334856033325,\n",
      "            -0.016727767884731293,\n",
      "            0.19629287719726562,\n",
      "            -0.1298413872718811,\n",
      "            0.2565804719924927,\n",
      "            -0.16403953731060028\n",
      "          ],\n",
      "          [\n",
      "            0.1485556960105896,\n",
      "            0.13240696489810944,\n",
      "            0.22882339358329773,\n",
      "            -0.009866802953183651,\n",
      "            0.17516140639781952,\n",
      "            0.1660080999135971,\n",
      "            -0.07210058718919754,\n",
      "            -0.011734021827578545,\n",
      "            -0.12207746505737305,\n",
      "            0.11630383133888245,\n",
      "            -0.16990667581558228,\n",
      "            -0.22629840672016144,\n",
      "            0.10458361357450485,\n",
      "            -0.1650450974702835,\n",
      "            0.06004244089126587,\n",
      "            0.23409342765808105,\n",
      "            0.19046729803085327,\n",
      "            -0.15349321067333221,\n",
      "            0.04559619724750519,\n",
      "            0.17725320160388947,\n",
      "            0.16763952374458313,\n",
      "            -0.16967783868312836,\n",
      "            0.1777992695569992,\n",
      "            0.23191066086292267,\n",
      "            0.012843208387494087,\n",
      "            0.17133447527885437,\n",
      "            0.28948554396629333,\n",
      "            0.020891189575195312,\n",
      "            0.20415084064006805,\n",
      "            0.1621398627758026,\n",
      "            -0.03591397404670715,\n",
      "            0.06049233302474022,\n",
      "            0.08515404164791107,\n",
      "            -0.07934698462486267,\n",
      "            -0.11134473979473114,\n",
      "            0.024612752720713615,\n",
      "            -0.16365371644496918,\n",
      "            -0.2488013505935669,\n",
      "            -0.04358882084488869,\n",
      "            0.2504517138004303,\n",
      "            0.12402338534593582,\n",
      "            0.19340717792510986,\n",
      "            0.11153020709753036,\n",
      "            -0.14583994448184967,\n",
      "            0.18791463971138,\n",
      "            -0.23232701420783997,\n",
      "            -0.1634330004453659,\n",
      "            0.15367132425308228,\n",
      "            -0.15048600733280182,\n",
      "            -0.060953713953495026,\n",
      "            0.17705312371253967,\n",
      "            0.13130809366703033,\n",
      "            0.1646551489830017,\n",
      "            0.19340847432613373,\n",
      "            0.03887385129928589,\n",
      "            -0.14622433483600616,\n",
      "            -0.08702398836612701,\n",
      "            -0.069037526845932,\n",
      "            0.14448752999305725,\n",
      "            0.10245854407548904,\n",
      "            -0.12360262870788574,\n",
      "            0.08621525764465332,\n",
      "            0.054439395666122437,\n",
      "            0.04871643707156181\n",
      "          ],\n",
      "          [\n",
      "            0.106303371489048,\n",
      "            0.1922091245651245,\n",
      "            -0.15456324815750122,\n",
      "            -0.10594523698091507,\n",
      "            0.11259381473064423,\n",
      "            0.1580190509557724,\n",
      "            0.1419593244791031,\n",
      "            0.345946729183197,\n",
      "            0.05397990718483925,\n",
      "            0.33666902780532837,\n",
      "            -0.3722795844078064,\n",
      "            -0.14743643999099731,\n",
      "            0.19114552438259125,\n",
      "            -0.0632544755935669,\n",
      "            -0.1738366037607193,\n",
      "            -0.3244418203830719,\n",
      "            -0.007341161835938692,\n",
      "            0.01946985349059105,\n",
      "            0.07172403484582901,\n",
      "            0.0275027584284544,\n",
      "            0.019830066710710526,\n",
      "            0.042395081371068954,\n",
      "            0.2881779670715332,\n",
      "            0.20103701949119568,\n",
      "            0.001373877632431686,\n",
      "            -0.08981213718652725,\n",
      "            0.0692748874425888,\n",
      "            -0.21559536457061768,\n",
      "            -0.211246058344841,\n",
      "            -0.11069648712873459,\n",
      "            0.3412468135356903,\n",
      "            0.13210241496562958,\n",
      "            -0.1327390819787979,\n",
      "            -0.20519544184207916,\n",
      "            -0.00817061122506857,\n",
      "            0.03869150951504707,\n",
      "            0.05374155938625336,\n",
      "            0.20125100016593933,\n",
      "            0.2071942389011383,\n",
      "            -0.14219947159290314,\n",
      "            0.04202085733413696,\n",
      "            0.053225837647914886,\n",
      "            0.13523617386817932,\n",
      "            0.01976429857313633,\n",
      "            -0.012544588185846806,\n",
      "            -0.22484518587589264,\n",
      "            0.11286178231239319,\n",
      "            -0.1374484896659851,\n",
      "            0.33045217394828796,\n",
      "            -0.13666099309921265,\n",
      "            0.1895163357257843,\n",
      "            0.12054800242185593,\n",
      "            -0.1186135858297348,\n",
      "            0.4480595886707306,\n",
      "            0.10938496887683868,\n",
      "            0.2014685422182083,\n",
      "            0.4066486358642578,\n",
      "            -0.24236683547496796,\n",
      "            0.246532142162323,\n",
      "            -0.07393475621938705,\n",
      "            0.10802613943815231,\n",
      "            -0.026018202304840088,\n",
      "            -0.2547752261161804,\n",
      "            0.24810288846492767\n",
      "          ],\n",
      "          [\n",
      "            -0.17606422305107117,\n",
      "            0.022840024903416634,\n",
      "            0.00974196381866932,\n",
      "            0.2408910095691681,\n",
      "            -0.033227838575839996,\n",
      "            -0.12187869101762772,\n",
      "            0.17592787742614746,\n",
      "            0.1161208227276802,\n",
      "            -0.1480478197336197,\n",
      "            -0.22527584433555603,\n",
      "            -0.026752283796668053,\n",
      "            -0.11239701509475708,\n",
      "            -0.23844477534294128,\n",
      "            -0.15900714695453644,\n",
      "            -0.1718236804008484,\n",
      "            -0.16101284325122833,\n",
      "            -0.08379864692687988,\n",
      "            0.0727410614490509,\n",
      "            0.1457391232252121,\n",
      "            0.1773454248905182,\n",
      "            0.15879100561141968,\n",
      "            -0.15321187674999237,\n",
      "            -0.03938660770654678,\n",
      "            0.2351505011320114,\n",
      "            0.04504534602165222,\n",
      "            -0.1346765160560608,\n",
      "            0.06566530466079712,\n",
      "            -0.06885731220245361,\n",
      "            0.17492549121379852,\n",
      "            0.20105832815170288,\n",
      "            0.1241779699921608,\n",
      "            -0.23200692236423492,\n",
      "            -0.050799932330846786,\n",
      "            -0.20304858684539795,\n",
      "            -0.151802659034729,\n",
      "            0.08240193128585815,\n",
      "            -0.250458300113678,\n",
      "            -0.10444573312997818,\n",
      "            -0.16683495044708252,\n",
      "            0.11535066366195679,\n",
      "            -0.06836181879043579,\n",
      "            0.018603490665555,\n",
      "            0.17491689324378967,\n",
      "            -0.08166313916444778,\n",
      "            0.051852211356163025,\n",
      "            0.1472329944372177,\n",
      "            -0.1911652833223343,\n",
      "            0.14002731442451477,\n",
      "            0.12171685695648193,\n",
      "            -0.09435362368822098,\n",
      "            -0.12355437874794006,\n",
      "            0.18601544201374054,\n",
      "            -0.02667173556983471,\n",
      "            -0.24271641671657562,\n",
      "            0.03863966464996338,\n",
      "            -0.058791376650333405,\n",
      "            -0.24362461268901825,\n",
      "            -0.10205426067113876,\n",
      "            0.17988233268260956,\n",
      "            -0.027713719755411148,\n",
      "            0.15364575386047363,\n",
      "            0.0140610933303833,\n",
      "            -0.09765703976154327,\n",
      "            -0.14264312386512756\n",
      "          ],\n",
      "          [\n",
      "            0.30436310172080994,\n",
      "            -0.12290872633457184,\n",
      "            -0.09751938283443451,\n",
      "            0.17641881108283997,\n",
      "            -0.1350577175617218,\n",
      "            -0.3315953016281128,\n",
      "            0.14926069974899292,\n",
      "            -0.33409440517425537,\n",
      "            -0.06308154761791229,\n",
      "            0.046113431453704834,\n",
      "            0.25652042031288147,\n",
      "            0.13710476458072662,\n",
      "            -0.120163194835186,\n",
      "            0.22530657052993774,\n",
      "            0.10373814404010773,\n",
      "            0.28067097067832947,\n",
      "            0.02760455757379532,\n",
      "            0.2784240245819092,\n",
      "            1.9281398635939695e-05,\n",
      "            0.22621823847293854,\n",
      "            -0.0398869663476944,\n",
      "            0.2662999629974365,\n",
      "            -0.20203953981399536,\n",
      "            0.12690353393554688,\n",
      "            0.08452450484037399,\n",
      "            0.30579042434692383,\n",
      "            0.1464644968509674,\n",
      "            -0.08890604972839355,\n",
      "            -0.2534220516681671,\n",
      "            0.10420556366443634,\n",
      "            0.14679700136184692,\n",
      "            -0.09313885122537613,\n",
      "            -0.10333731025457382,\n",
      "            0.23321743309497833,\n",
      "            0.18113352358341217,\n",
      "            0.14331196248531342,\n",
      "            0.07320958375930786,\n",
      "            -0.18909282982349396,\n",
      "            -0.2037641704082489,\n",
      "            -0.03781095892190933,\n",
      "            0.08930985629558563,\n",
      "            0.13955603539943695,\n",
      "            0.25954753160476685,\n",
      "            0.17232055962085724,\n",
      "            -0.06681845337152481,\n",
      "            0.3684917986392975,\n",
      "            -0.12105242162942886,\n",
      "            -0.23781050741672516,\n",
      "            -0.21281465888023376,\n",
      "            0.3597884178161621,\n",
      "            -0.014374320395290852,\n",
      "            0.12183605879545212,\n",
      "            0.08298638463020325,\n",
      "            -0.037888407707214355,\n",
      "            0.029923902824521065,\n",
      "            0.15635351836681366,\n",
      "            -8.298858301714063e-05,\n",
      "            0.25151798129081726,\n",
      "            -0.058668091893196106,\n",
      "            0.06738346815109253,\n",
      "            -0.23190580308437347,\n",
      "            0.1619272232055664,\n",
      "            0.06442972272634506,\n",
      "            -0.06737916171550751\n",
      "          ],\n",
      "          [\n",
      "            0.13585007190704346,\n",
      "            -0.2151860147714615,\n",
      "            0.162857785820961,\n",
      "            -0.017024356871843338,\n",
      "            0.1328868418931961,\n",
      "            -0.16178032755851746,\n",
      "            0.03435376286506653,\n",
      "            -0.22287629544734955,\n",
      "            0.02132009156048298,\n",
      "            -0.19944538176059723,\n",
      "            0.056116048246622086,\n",
      "            0.060255181044340134,\n",
      "            -0.13267019391059875,\n",
      "            0.06324502825737,\n",
      "            0.1408005952835083,\n",
      "            0.01293405145406723,\n",
      "            -0.23114776611328125,\n",
      "            -0.13021709024906158,\n",
      "            0.1546049565076828,\n",
      "            -0.060894932597875595,\n",
      "            0.09386030584573746,\n",
      "            -0.07912296801805496,\n",
      "            -0.053233277052640915,\n",
      "            -0.1541324257850647,\n",
      "            0.03417113795876503,\n",
      "            0.030019966885447502,\n",
      "            -0.22400742769241333,\n",
      "            0.04710888862609863,\n",
      "            0.1833951324224472,\n",
      "            0.16807803511619568,\n",
      "            -0.16206665337085724,\n",
      "            -0.11725284159183502,\n",
      "            -0.043221745640039444,\n",
      "            0.15077106654644012,\n",
      "            0.06818883121013641,\n",
      "            0.11371542513370514,\n",
      "            0.19943425059318542,\n",
      "            -0.23338989913463593,\n",
      "            0.03981171175837517,\n",
      "            0.16149187088012695,\n",
      "            -0.013962626457214355,\n",
      "            0.13736896216869354,\n",
      "            -0.1582866758108139,\n",
      "            0.05822296068072319,\n",
      "            -0.14179936051368713,\n",
      "            0.12620936334133148,\n",
      "            0.07981943339109421,\n",
      "            -0.19317211210727692,\n",
      "            -0.0522746741771698,\n",
      "            -0.05578787252306938,\n",
      "            -0.24497194588184357,\n",
      "            -0.14435632526874542,\n",
      "            -0.054955679923295975,\n",
      "            0.15879325568675995,\n",
      "            -0.2235305905342102,\n",
      "            -0.19198106229305267,\n",
      "            0.11283569782972336,\n",
      "            -0.11318937689065933,\n",
      "            0.19751276075839996,\n",
      "            0.18017788231372833,\n",
      "            0.14015215635299683,\n",
      "            0.08218139410018921,\n",
      "            0.17493370175361633,\n",
      "            -0.13501380383968353\n",
      "          ],\n",
      "          [\n",
      "            -0.028337450698018074,\n",
      "            -0.09713670611381531,\n",
      "            0.05556821450591087,\n",
      "            0.1631467342376709,\n",
      "            0.16862118244171143,\n",
      "            0.061077360063791275,\n",
      "            0.08670961111783981,\n",
      "            0.2235225886106491,\n",
      "            -0.0002306954556843266,\n",
      "            -0.10650918632745743,\n",
      "            -0.009496035054326057,\n",
      "            0.17813043296337128,\n",
      "            0.1482725292444229,\n",
      "            -0.1017783060669899,\n",
      "            0.14665532112121582,\n",
      "            -0.12491793930530548,\n",
      "            -0.1682593822479248,\n",
      "            -0.19386324286460876,\n",
      "            0.002942591905593872,\n",
      "            0.0669977143406868,\n",
      "            -0.1730610877275467,\n",
      "            0.0028295498341321945,\n",
      "            0.08962082862854004,\n",
      "            0.04496873542666435,\n",
      "            0.10631495714187622,\n",
      "            0.09435874968767166,\n",
      "            -0.24443233013153076,\n",
      "            0.24601119756698608,\n",
      "            0.19601202011108398,\n",
      "            -0.13500285148620605,\n",
      "            -0.2528623342514038,\n",
      "            -0.11622839421033859,\n",
      "            0.10959433764219284,\n",
      "            0.24720315635204315,\n",
      "            -0.1316809058189392,\n",
      "            -0.08932079374790192,\n",
      "            0.06402109563350677,\n",
      "            -0.11928750574588776,\n",
      "            -0.03278513252735138,\n",
      "            -0.14819230139255524,\n",
      "            -0.18948084115982056,\n",
      "            -0.2698925733566284,\n",
      "            0.08442503213882446,\n",
      "            -0.09453947097063065,\n",
      "            0.19742240011692047,\n",
      "            -0.15588487684726715,\n",
      "            -0.2356361299753189,\n",
      "            -0.17590320110321045,\n",
      "            0.25114962458610535,\n",
      "            0.13143853843212128,\n",
      "            0.0123560456559062,\n",
      "            -0.23228934407234192,\n",
      "            -0.08857055008411407,\n",
      "            -0.18989260494709015,\n",
      "            -0.039272334426641464,\n",
      "            0.08016663789749146,\n",
      "            0.1547713279724121,\n",
      "            -0.07934936881065369,\n",
      "            0.19210056960582733,\n",
      "            -0.10949784517288208,\n",
      "            0.021583503112196922,\n",
      "            0.08447378873825073,\n",
      "            0.1042865589261055,\n",
      "            0.01776757463812828\n",
      "          ],\n",
      "          [\n",
      "            -0.0028699946124106646,\n",
      "            -0.006434664595872164,\n",
      "            0.18716034293174744,\n",
      "            0.18920081853866577,\n",
      "            -0.11020636558532715,\n",
      "            0.1638447642326355,\n",
      "            0.006105598993599415,\n",
      "            -0.03294576704502106,\n",
      "            -0.04335808753967285,\n",
      "            0.1625765562057495,\n",
      "            -0.13748924434185028,\n",
      "            0.045703768730163574,\n",
      "            -0.2442592829465866,\n",
      "            0.04610284045338631,\n",
      "            0.0019215941429138184,\n",
      "            0.1270647794008255,\n",
      "            -0.15260612964630127,\n",
      "            0.021546829491853714,\n",
      "            0.1461055874824524,\n",
      "            -0.22333848476409912,\n",
      "            -0.22294896841049194,\n",
      "            -0.18206319212913513,\n",
      "            -0.13124258816242218,\n",
      "            0.2307644784450531,\n",
      "            0.019890135154128075,\n",
      "            0.10743461549282074,\n",
      "            0.1926335245370865,\n",
      "            0.057634830474853516,\n",
      "            0.06672307848930359,\n",
      "            -0.011537459678947926,\n",
      "            -0.11024294793605804,\n",
      "            -0.05521407350897789,\n",
      "            -0.00788600742816925,\n",
      "            0.008268998004496098,\n",
      "            -0.23659376800060272,\n",
      "            -0.21964873373508453,\n",
      "            -0.10929754376411438,\n",
      "            0.03247402235865593,\n",
      "            0.16349101066589355,\n",
      "            0.08465991914272308,\n",
      "            -0.04546435922384262,\n",
      "            -0.19615694880485535,\n",
      "            -0.21295253932476044,\n",
      "            -0.029692545533180237,\n",
      "            0.23653772473335266,\n",
      "            0.08725223690271378,\n",
      "            0.07884318381547928,\n",
      "            0.1437360644340515,\n",
      "            -0.09775463491678238,\n",
      "            0.16445478796958923,\n",
      "            0.16024067997932434,\n",
      "            -0.039619460701942444,\n",
      "            0.09172336012125015,\n",
      "            -0.0801357552409172,\n",
      "            -0.0373457670211792,\n",
      "            -0.007771189324557781,\n",
      "            0.04501277580857277,\n",
      "            0.19147485494613647,\n",
      "            0.1118222251534462,\n",
      "            -0.07181958109140396,\n",
      "            -0.20608770847320557,\n",
      "            -0.10550463199615479,\n",
      "            0.04333536699414253,\n",
      "            -0.21544453501701355\n",
      "          ],\n",
      "          [\n",
      "            0.11078079789876938,\n",
      "            -0.12176656723022461,\n",
      "            -0.11792270094156265,\n",
      "            0.21343131363391876,\n",
      "            -0.020911572501063347,\n",
      "            0.14429990947246552,\n",
      "            -0.1316186934709549,\n",
      "            0.1194160133600235,\n",
      "            0.15422917902469635,\n",
      "            -0.11102782189846039,\n",
      "            -0.1139521449804306,\n",
      "            -0.18345224857330322,\n",
      "            0.004071716219186783,\n",
      "            0.24281176924705505,\n",
      "            -0.20707106590270996,\n",
      "            -0.06238215044140816,\n",
      "            -0.010765999555587769,\n",
      "            -0.1961752027273178,\n",
      "            0.11710295081138611,\n",
      "            0.15085077285766602,\n",
      "            -0.201920285820961,\n",
      "            0.11408735811710358,\n",
      "            0.09961842000484467,\n",
      "            -0.06949124485254288,\n",
      "            -0.13523288071155548,\n",
      "            0.19236284494400024,\n",
      "            0.1266636699438095,\n",
      "            0.14303523302078247,\n",
      "            0.0660877525806427,\n",
      "            0.0015626430977135897,\n",
      "            -0.14400748908519745,\n",
      "            0.06040089949965477,\n",
      "            -0.01855391077697277,\n",
      "            -0.1669618934392929,\n",
      "            0.13337409496307373,\n",
      "            0.18681947886943817,\n",
      "            0.12925122678279877,\n",
      "            0.15085530281066895,\n",
      "            -0.028425680473446846,\n",
      "            -0.027314109727740288,\n",
      "            -0.23449069261550903,\n",
      "            0.21623308956623077,\n",
      "            -0.05234126001596451,\n",
      "            0.20285353064537048,\n",
      "            0.12675750255584717,\n",
      "            0.03848106414079666,\n",
      "            -0.18717777729034424,\n",
      "            0.1673223078250885,\n",
      "            0.1368289440870285,\n",
      "            0.057473115622997284,\n",
      "            -0.19377748668193817,\n",
      "            0.20671497285366058,\n",
      "            0.21920882165431976,\n",
      "            0.19801081717014313,\n",
      "            -0.09070152044296265,\n",
      "            -0.06174919754266739,\n",
      "            -0.289324551820755,\n",
      "            -0.17899854481220245,\n",
      "            0.18046677112579346,\n",
      "            -0.0771232321858406,\n",
      "            0.11202480643987656,\n",
      "            -0.02435934543609619,\n",
      "            -0.144227996468544,\n",
      "            0.04793581739068031\n",
      "          ],\n",
      "          [\n",
      "            0.36813226342201233,\n",
      "            0.19240093231201172,\n",
      "            0.021846577525138855,\n",
      "            0.21295414865016937,\n",
      "            -0.10948330163955688,\n",
      "            -0.13101965188980103,\n",
      "            0.09442673623561859,\n",
      "            -0.06095264479517937,\n",
      "            -0.18788394331932068,\n",
      "            0.004229994025081396,\n",
      "            0.16220737993717194,\n",
      "            0.001058728899806738,\n",
      "            -0.25482267141342163,\n",
      "            -0.03557979688048363,\n",
      "            -0.04715406149625778,\n",
      "            0.19982241094112396,\n",
      "            -0.029844308272004128,\n",
      "            -0.029706796631217003,\n",
      "            -0.12872479856014252,\n",
      "            0.21903124451637268,\n",
      "            0.10512260347604752,\n",
      "            0.2965882122516632,\n",
      "            0.044657688587903976,\n",
      "            -0.14934010803699493,\n",
      "            -0.22482682764530182,\n",
      "            0.3232208788394928,\n",
      "            -0.08140161633491516,\n",
      "            0.15857386589050293,\n",
      "            0.03352478891611099,\n",
      "            -0.02194017358124256,\n",
      "            -0.287959486246109,\n",
      "            0.18496300280094147,\n",
      "            0.11750629544258118,\n",
      "            0.08891835063695908,\n",
      "            -0.23095323145389557,\n",
      "            -0.14316171407699585,\n",
      "            0.187587708234787,\n",
      "            0.11704686284065247,\n",
      "            -0.10093094408512115,\n",
      "            -0.277597576379776,\n",
      "            0.1070738285779953,\n",
      "            0.2925035357475281,\n",
      "            0.31885501742362976,\n",
      "            0.25477418303489685,\n",
      "            -0.017318855971097946,\n",
      "            0.29238080978393555,\n",
      "            0.16039499640464783,\n",
      "            -0.23874075710773468,\n",
      "            -0.12842455506324768,\n",
      "            -0.05433563515543938,\n",
      "            -0.0929805114865303,\n",
      "            -0.04281748831272125,\n",
      "            -0.19153770804405212,\n",
      "            -0.19732259213924408,\n",
      "            -0.01336824893951416,\n",
      "            -0.23489108681678772,\n",
      "            -0.35896843671798706,\n",
      "            0.0567205436527729,\n",
      "            0.18057090044021606,\n",
      "            0.08697856962680817,\n",
      "            -0.2472470998764038,\n",
      "            -0.18791824579238892,\n",
      "            -0.03554969280958176,\n",
      "            -0.1588335484266281\n",
      "          ],\n",
      "          [\n",
      "            0.05375344306230545,\n",
      "            -0.15495480597019196,\n",
      "            -0.17763277888298035,\n",
      "            -0.14869193732738495,\n",
      "            0.22209396958351135,\n",
      "            0.14193828403949738,\n",
      "            -0.023622650653123856,\n",
      "            0.15975609421730042,\n",
      "            0.09655673801898956,\n",
      "            -0.132012739777565,\n",
      "            0.16303367912769318,\n",
      "            -0.06606429815292358,\n",
      "            -0.049711741507053375,\n",
      "            -0.18739910423755646,\n",
      "            0.18811173737049103,\n",
      "            0.27490589022636414,\n",
      "            0.03757435828447342,\n",
      "            0.09442086517810822,\n",
      "            -0.10878979414701462,\n",
      "            -0.07084853202104568,\n",
      "            0.11826550215482712,\n",
      "            0.05388500541448593,\n",
      "            -0.29395437240600586,\n",
      "            -0.17807476222515106,\n",
      "            0.020448509603738785,\n",
      "            0.06225825101137161,\n",
      "            0.01067654974758625,\n",
      "            0.14852333068847656,\n",
      "            0.133269265294075,\n",
      "            0.24459071457386017,\n",
      "            0.2106463611125946,\n",
      "            -0.03738269582390785,\n",
      "            -0.16465231776237488,\n",
      "            0.016726776957511902,\n",
      "            -0.1177300214767456,\n",
      "            -0.0393097810447216,\n",
      "            0.10956145823001862,\n",
      "            -0.1861557960510254,\n",
      "            0.19315531849861145,\n",
      "            0.013073279522359371,\n",
      "            0.13749146461486816,\n",
      "            0.08051701635122299,\n",
      "            -0.022241855040192604,\n",
      "            -0.14106063544750214,\n",
      "            0.07998760044574738,\n",
      "            -0.14209884405136108,\n",
      "            0.15035659074783325,\n",
      "            -0.08167984336614609,\n",
      "            -0.27030640840530396,\n",
      "            0.285806804895401,\n",
      "            0.07152517884969711,\n",
      "            0.019388720393180847,\n",
      "            -0.021799549460411072,\n",
      "            -0.1772850900888443,\n",
      "            -0.035197913646698,\n",
      "            0.2393350452184677,\n",
      "            -0.03906405344605446,\n",
      "            0.2739706039428711,\n",
      "            0.2268390953540802,\n",
      "            0.21287140250205994,\n",
      "            -0.1526077389717102,\n",
      "            -0.0714232325553894,\n",
      "            -0.04688693583011627,\n",
      "            -0.028907088562846184\n",
      "          ],\n",
      "          [\n",
      "            -0.07918110489845276,\n",
      "            -0.14328759908676147,\n",
      "            0.1449383646249771,\n",
      "            0.08199895173311234,\n",
      "            0.16302287578582764,\n",
      "            0.27941060066223145,\n",
      "            -0.08241633325815201,\n",
      "            0.08783778548240662,\n",
      "            -0.19819657504558563,\n",
      "            0.1639241725206375,\n",
      "            -0.3610784411430359,\n",
      "            0.1347537338733673,\n",
      "            -0.20117156207561493,\n",
      "            -0.2633515000343323,\n",
      "            -0.08757385611534119,\n",
      "            -0.2274026721715927,\n",
      "            0.23596355319023132,\n",
      "            0.054062776267528534,\n",
      "            -0.24639801681041718,\n",
      "            -0.0319812186062336,\n",
      "            0.12478867918252945,\n",
      "            -0.290105402469635,\n",
      "            0.49918460845947266,\n",
      "            -0.04801303893327713,\n",
      "            -0.18234771490097046,\n",
      "            0.09741630405187607,\n",
      "            0.293456494808197,\n",
      "            0.13196414709091187,\n",
      "            -0.17571760714054108,\n",
      "            0.13678880035877228,\n",
      "            -0.0800304263830185,\n",
      "            -0.22547991573810577,\n",
      "            -0.1968463957309723,\n",
      "            -0.02765396237373352,\n",
      "            0.2037540078163147,\n",
      "            0.05267174914479256,\n",
      "            -0.1968412846326828,\n",
      "            -0.1811419129371643,\n",
      "            0.09616226702928543,\n",
      "            0.112073615193367,\n",
      "            0.19759565591812134,\n",
      "            0.21382316946983337,\n",
      "            -0.06550800800323486,\n",
      "            -0.353335976600647,\n",
      "            0.09647321701049805,\n",
      "            -0.04257655516266823,\n",
      "            0.27023276686668396,\n",
      "            -0.07455848157405853,\n",
      "            0.24470895528793335,\n",
      "            -0.17194132506847382,\n",
      "            0.14750702679157257,\n",
      "            0.025669505819678307,\n",
      "            -0.1426982283592224,\n",
      "            0.3911723792552948,\n",
      "            0.10904628038406372,\n",
      "            0.03508060425519943,\n",
      "            0.3668123185634613,\n",
      "            -0.028017079457640648,\n",
      "            -0.029742129147052765,\n",
      "            -0.22671784460544586,\n",
      "            0.16345451772212982,\n",
      "            -0.07660901546478271,\n",
      "            -0.23746341466903687,\n",
      "            0.11639981716871262\n",
      "          ],\n",
      "          [\n",
      "            0.2195403128862381,\n",
      "            -0.14046834409236908,\n",
      "            0.10532556474208832,\n",
      "            0.027453124523162842,\n",
      "            -0.09222929924726486,\n",
      "            0.028146354481577873,\n",
      "            0.15854664146900177,\n",
      "            -0.19391009211540222,\n",
      "            0.24615581333637238,\n",
      "            -0.10491996258497238,\n",
      "            0.45257213711738586,\n",
      "            0.17265953123569489,\n",
      "            0.21812835335731506,\n",
      "            0.32524850964546204,\n",
      "            -0.15259581804275513,\n",
      "            -0.08630678802728653,\n",
      "            -0.11137029528617859,\n",
      "            0.2143697291612625,\n",
      "            0.0005543293082155287,\n",
      "            -0.158305361866951,\n",
      "            0.09271564334630966,\n",
      "            0.19849714636802673,\n",
      "            -0.34819915890693665,\n",
      "            -0.13830287754535675,\n",
      "            -0.051546283066272736,\n",
      "            0.29394030570983887,\n",
      "            -0.23978392779827118,\n",
      "            0.15564000606536865,\n",
      "            -0.17648987472057343,\n",
      "            -0.17753848433494568,\n",
      "            -0.22295445203781128,\n",
      "            -0.24765431880950928,\n",
      "            -0.036633625626564026,\n",
      "            0.05980093404650688,\n",
      "            0.1079796552658081,\n",
      "            -0.14141099154949188,\n",
      "            0.10693157464265823,\n",
      "            -0.2166348099708557,\n",
      "            0.07307934761047363,\n",
      "            -0.16516625881195068,\n",
      "            0.05587923526763916,\n",
      "            0.23241376876831055,\n",
      "            0.36748602986335754,\n",
      "            -0.03218526020646095,\n",
      "            -0.05411200597882271,\n",
      "            0.34906327724456787,\n",
      "            -0.06653504818677902,\n",
      "            -0.103359155356884,\n",
      "            -0.15897221863269806,\n",
      "            0.2533068060874939,\n",
      "            -0.2196357250213623,\n",
      "            -0.009318049997091293,\n",
      "            0.0342908538877964,\n",
      "            -0.182248055934906,\n",
      "            -0.08877438306808472,\n",
      "            -0.05713440477848053,\n",
      "            -0.28928428888320923,\n",
      "            0.3525611460208893,\n",
      "            0.03584630787372589,\n",
      "            -0.23837237060070038,\n",
      "            0.05160905420780182,\n",
      "            0.02995985746383667,\n",
      "            -0.1563902497291565,\n",
      "            0.08909602463245392\n",
      "          ],\n",
      "          [\n",
      "            0.05500860884785652,\n",
      "            0.007375516928732395,\n",
      "            0.0835302472114563,\n",
      "            0.040948595851659775,\n",
      "            0.16692814230918884,\n",
      "            0.020086023956537247,\n",
      "            0.22412584722042084,\n",
      "            0.08104319125413895,\n",
      "            -0.017043206840753555,\n",
      "            -0.2410522848367691,\n",
      "            0.15681786835193634,\n",
      "            -0.1789717972278595,\n",
      "            0.14801311492919922,\n",
      "            0.11639498174190521,\n",
      "            0.11102950572967529,\n",
      "            0.29759150743484497,\n",
      "            -0.03348458558320999,\n",
      "            0.2979280948638916,\n",
      "            0.09891056269407272,\n",
      "            -0.11267102509737015,\n",
      "            -0.17903247475624084,\n",
      "            0.16568729281425476,\n",
      "            -0.31031420826911926,\n",
      "            -0.10652732849121094,\n",
      "            -0.21723243594169617,\n",
      "            0.050256844609975815,\n",
      "            0.111737459897995,\n",
      "            0.1399306058883667,\n",
      "            -0.18984001874923706,\n",
      "            0.03127004951238632,\n",
      "            -0.1586073786020279,\n",
      "            -0.16376487910747528,\n",
      "            0.056454092264175415,\n",
      "            0.24081461131572723,\n",
      "            0.058352284133434296,\n",
      "            0.01649506390094757,\n",
      "            -0.01305152103304863,\n",
      "            0.1992100030183792,\n",
      "            0.08191187679767609,\n",
      "            -0.21736730635166168,\n",
      "            -0.0856272429227829,\n",
      "            0.11438219249248505,\n",
      "            0.3238590955734253,\n",
      "            -0.11233730614185333,\n",
      "            0.15524108707904816,\n",
      "            0.14061221480369568,\n",
      "            -0.15016666054725647,\n",
      "            0.10666025429964066,\n",
      "            -0.29496774077415466,\n",
      "            0.06958185136318207,\n",
      "            0.03941347450017929,\n",
      "            -0.24892418086528778,\n",
      "            0.06785811483860016,\n",
      "            0.02134261466562748,\n",
      "            0.09859204292297363,\n",
      "            0.037640463560819626,\n",
      "            0.1186981201171875,\n",
      "            0.003655010601505637,\n",
      "            -0.10682085901498795,\n",
      "            0.23728486895561218,\n",
      "            0.18932299315929413,\n",
      "            -0.21549129486083984,\n",
      "            -0.026279550045728683,\n",
      "            -0.05406419187784195\n",
      "          ],\n",
      "          [\n",
      "            0.11862794309854507,\n",
      "            0.15536855161190033,\n",
      "            0.2374972403049469,\n",
      "            0.2073272317647934,\n",
      "            -0.13687138259410858,\n",
      "            0.09696497768163681,\n",
      "            -0.008077375590801239,\n",
      "            -0.04019925743341446,\n",
      "            -0.24617573618888855,\n",
      "            0.1795949786901474,\n",
      "            -0.037293460220098495,\n",
      "            -0.14055776596069336,\n",
      "            0.22654809057712555,\n",
      "            -0.18524813652038574,\n",
      "            -0.20242232084274292,\n",
      "            -0.048143766820430756,\n",
      "            0.02437746524810791,\n",
      "            0.16067996621131897,\n",
      "            -0.23674151301383972,\n",
      "            0.13306139409542084,\n",
      "            -0.1624651849269867,\n",
      "            -0.1401759833097458,\n",
      "            -0.15209165215492249,\n",
      "            -0.008712226524949074,\n",
      "            0.17303822934627533,\n",
      "            -0.21594588458538055,\n",
      "            -0.0019697179086506367,\n",
      "            -0.24988281726837158,\n",
      "            0.2451804131269455,\n",
      "            0.16446848213672638,\n",
      "            -0.13963790237903595,\n",
      "            0.19489887356758118,\n",
      "            0.03237231820821762,\n",
      "            -0.15397991240024567,\n",
      "            0.13583146035671234,\n",
      "            0.16726253926753998,\n",
      "            -0.011822020635008812,\n",
      "            0.08643016964197159,\n",
      "            -0.1998179405927658,\n",
      "            0.0028772156219929457,\n",
      "            -0.15514642000198364,\n",
      "            -0.1494484394788742,\n",
      "            -0.06855977326631546,\n",
      "            0.24272987246513367,\n",
      "            0.004763164557516575,\n",
      "            -0.08929216116666794,\n",
      "            0.03548668324947357,\n",
      "            -0.009198755957186222,\n",
      "            -0.19046397507190704,\n",
      "            0.18017375469207764,\n",
      "            0.22525760531425476,\n",
      "            -0.10891902446746826,\n",
      "            0.17584724724292755,\n",
      "            -0.14401650428771973,\n",
      "            -0.240736722946167,\n",
      "            0.18070943653583527,\n",
      "            -0.045156899839639664,\n",
      "            0.06627442687749863,\n",
      "            -0.05588991940021515,\n",
      "            -0.11186379194259644,\n",
      "            -0.03018289804458618,\n",
      "            -0.08314687013626099,\n",
      "            0.07996660470962524,\n",
      "            0.016085872426629066\n",
      "          ],\n",
      "          [\n",
      "            0.1808532178401947,\n",
      "            -0.17875561118125916,\n",
      "            0.05500439554452896,\n",
      "            -0.034539178013801575,\n",
      "            -0.22406136989593506,\n",
      "            -0.043283771723508835,\n",
      "            0.1166578084230423,\n",
      "            0.16465508937835693,\n",
      "            0.09768428653478622,\n",
      "            0.1805562525987625,\n",
      "            -0.02671426720917225,\n",
      "            -0.2443692684173584,\n",
      "            0.02954399399459362,\n",
      "            -0.03738375008106232,\n",
      "            -0.14378756284713745,\n",
      "            0.14557579159736633,\n",
      "            -0.13399863243103027,\n",
      "            0.13607780635356903,\n",
      "            0.10216401517391205,\n",
      "            0.09487252682447433,\n",
      "            0.06052253767848015,\n",
      "            0.03055327758193016,\n",
      "            0.3175029158592224,\n",
      "            -0.12701188027858734,\n",
      "            -0.04783143475651741,\n",
      "            0.0030532984528690577,\n",
      "            -0.024819567799568176,\n",
      "            0.002726614475250244,\n",
      "            -0.054916948080062866,\n",
      "            0.12086457759141922,\n",
      "            -0.009353156201541424,\n",
      "            0.15370668470859528,\n",
      "            -0.038204681128263474,\n",
      "            -0.07331746816635132,\n",
      "            -0.16564418375492096,\n",
      "            -0.22713468968868256,\n",
      "            0.149180069565773,\n",
      "            0.007598095573484898,\n",
      "            0.21681950986385345,\n",
      "            0.26581722497940063,\n",
      "            0.2140452116727829,\n",
      "            0.1651417315006256,\n",
      "            -0.03573708236217499,\n",
      "            0.15288780629634857,\n",
      "            0.21339134871959686,\n",
      "            0.017692815512418747,\n",
      "            0.027307989075779915,\n",
      "            -0.026068298146128654,\n",
      "            -0.1526031196117401,\n",
      "            -0.07108378410339355,\n",
      "            0.12211090326309204,\n",
      "            -0.2147451937198639,\n",
      "            -0.0748140886425972,\n",
      "            0.2903755307197571,\n",
      "            0.005394631531089544,\n",
      "            0.000434123125160113,\n",
      "            0.13293825089931488,\n",
      "            -0.20487673580646515,\n",
      "            0.027451781556010246,\n",
      "            -0.04300432279706001,\n",
      "            0.0650261640548706,\n",
      "            0.20370596647262573,\n",
      "            -0.18049103021621704,\n",
      "            0.25827792286872864\n",
      "          ]\n",
      "        ],\n",
      "        \"bias\": [\n",
      "          0.036124780774116516,\n",
      "          -0.020255332812666893,\n",
      "          0.03030441515147686,\n",
      "          0.06330069154500961,\n",
      "          0.04754292592406273,\n",
      "          0.031256113201379776,\n",
      "          -1.177789999928791e-05,\n",
      "          -0.015189144760370255,\n",
      "          0.006883854512125254,\n",
      "          0.02362710051238537,\n",
      "          0.0,\n",
      "          0.045396868139505386,\n",
      "          0.0,\n",
      "          -0.012536306865513325,\n",
      "          0.0,\n",
      "          0.06504052132368088,\n",
      "          0.085873544216156,\n",
      "          0.009947036392986774,\n",
      "          0.04622912034392357,\n",
      "          0.007147546857595444,\n",
      "          -0.035862989723682404,\n",
      "          0.08068623393774033,\n",
      "          0.07286574691534042,\n",
      "          -0.026836000382900238,\n",
      "          5.488452370627783e-05,\n",
      "          0.019643530249595642,\n",
      "          -0.004898476880043745,\n",
      "          0.07515653222799301,\n",
      "          0.08247505128383636,\n",
      "          0.047648001462221146,\n",
      "          -0.008247440680861473,\n",
      "          0.0\n",
      "        ],\n",
      "        \"activation\": \"relu\"\n",
      "      },\n",
      "      {\n",
      "        \"matrix\": [\n",
      "          [\n",
      "            0.37669551372528076,\n",
      "            -0.03240711987018585,\n",
      "            -0.10158049315214157,\n",
      "            -0.3089997172355652,\n",
      "            -0.5130119919776917,\n",
      "            0.14565503597259521,\n",
      "            0.17910724878311157,\n",
      "            0.34442338347435,\n",
      "            0.1414504200220108,\n",
      "            0.2660679519176483,\n",
      "            0.09417015314102173,\n",
      "            0.41845637559890747,\n",
      "            0.06001114845275879,\n",
      "            0.1288667917251587,\n",
      "            0.12237018346786499,\n",
      "            0.47585970163345337,\n",
      "            0.5179509520530701,\n",
      "            -0.41870853304862976,\n",
      "            -0.7443276047706604,\n",
      "            -0.031065678223967552,\n",
      "            0.36514803767204285,\n",
      "            0.1557081788778305,\n",
      "            0.3492183983325958,\n",
      "            -0.14037786424160004,\n",
      "            0.0496760755777359,\n",
      "            0.44836661219596863,\n",
      "            0.18188408017158508,\n",
      "            -0.8151816129684448,\n",
      "            0.8479392528533936,\n",
      "            0.2899399399757385,\n",
      "            -0.022835981100797653,\n",
      "            -0.1704913228750229\n",
      "          ]\n",
      "        ],\n",
      "        \"bias\": [\n",
      "          0.0\n",
      "        ],\n",
      "        \"activation\": \"identity\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "Delete\n",
      "Add new model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'responseHeader': {'status': 0, 'QTime': 632}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "modelName = 'ranknet-e1000-b64-l128-l64-l32-adadelta'\n",
    "solrModel = {\n",
    "  \"store\": \"thesis-ltr\",\n",
    "  \"class\": \"org.apache.solr.ltr.model.NeuralNetworkModel\",\n",
    "  \"name\": modelName,\n",
    "  \"features\": [\n",
    "  ],\n",
    "  \"params\": {}\n",
    "}\n",
    "\n",
    "featureNames = [\"title_coveredQueryTerms\",\n",
    "    \"headings_coveredQueryTerms\",\n",
    "    \"body_coveredQueryTerms\",\n",
    "    \"document_coveredQueryTerms\",\n",
    "    \"title_coveredQueryTermsRatio\",\n",
    "    \"headings_coveredQueryTermsRatio\",\n",
    "    \"body_coveredQueryTermsRatio\",\n",
    "    \"document_coveredQueryTermsRatio\",\n",
    "    \"title_tf\",\n",
    "    \"headings_tf\",\n",
    "    \"body_tf\",\n",
    "    \"document_tf\",\n",
    "    \"title_idf\",\n",
    "    \"headings_idf\",\n",
    "    \"body_idf\",\n",
    "    \"document_idf\",\n",
    "    \"title_tfidf\",\n",
    "    \"headings_tfidf\",\n",
    "    \"body_tfidf\",\n",
    "    \"document_tfidf\",\n",
    "    \"title_bm25\",\n",
    "    \"headings_bm25\",\n",
    "    \"body_bm25\",\n",
    "    \"document_bm25\"\n",
    "]\n",
    "\n",
    "for idx, featureName in enumerate(featureNames):\n",
    "    config = {\n",
    "        \"name\": featureName,\n",
    "        \"norm\": {\n",
    "            \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
    "            \"params\": {\n",
    "                \"min\": str(minimums[idx]),\n",
    "                \"max\": str(maximums[idx])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    solrModel['features'].append(config)\n",
    "    \n",
    "weights = model.get_weights()\n",
    "\n",
    "layers = []\n",
    "layers.append({\"matrix\": weights[0].T.tolist(),\n",
    "               \"bias\": weights[1].tolist(),\n",
    "               \"activation\": \"relu\"})\n",
    "layers.append({\"matrix\": weights[2].T.tolist(),\n",
    "               \"bias\": weights[3].tolist(),\n",
    "               \"activation\": \"relu\"})\n",
    "layers.append({\"matrix\": weights[4].T.tolist(),\n",
    "              \"bias\": weights[5].tolist(),\n",
    "              \"activation\": \"relu\"})\n",
    "layers.append({\"matrix\": weights[6].T.tolist(),\n",
    "              \"bias\": weights[7].tolist(),\n",
    "              \"activation\": \"identity\"})\n",
    "solrModel[\"params\"][\"layers\"] = layers\n",
    "\n",
    "\n",
    "print(json.dumps(solrModel, indent=2))\n",
    "\n",
    "# Upload the model after deleting the model\n",
    "print('Delete')\n",
    "requests.delete(f'http://localhost:8983/solr/thesis-ltr/schema/model-store/{modelName}').json()\n",
    "print('Add new model')\n",
    "requests.put(f'http://localhost:8983/solr/thesis-ltr/schema/model-store', json=solrModel).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to sample.json\n",
    "with open(f'./submission/ranknet/solrModels/{modelName}.json', \"w\") as jsonFile:\n",
    "    json.dump(solrModel, jsonFile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
