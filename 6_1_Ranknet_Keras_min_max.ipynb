{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow matplotlib scikit-learn pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import backend\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Activation, Add, Dense, Input, Lambda\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 24\n",
    "h_1_dim = 64\n",
    "h_2_dim = h_1_dim // 2\n",
    "h_3_dim = h_2_dim // 2\n",
    "\n",
    "# Model.\n",
    "h_1 = Dense(h_1_dim, activation = \"relu\")\n",
    "h_2 = Dense(h_2_dim, activation = \"relu\")\n",
    "h_3 = Dense(h_3_dim, activation = \"relu\")\n",
    "s = Dense(1)\n",
    "\n",
    "# Relevant document score.\n",
    "rel_doc = Input(shape = (INPUT_DIM, ), dtype = \"float32\")\n",
    "h_1_rel = h_1(rel_doc)\n",
    "h_2_rel = h_2(h_1_rel)\n",
    "h_3_rel = h_3(h_2_rel)\n",
    "rel_score = s(h_3_rel)\n",
    "\n",
    "# Irrelevant document score.\n",
    "irr_doc = Input(shape = (INPUT_DIM, ), dtype = \"float32\")\n",
    "h_1_irr = h_1(irr_doc)\n",
    "h_2_irr = h_2(h_1_irr)\n",
    "h_3_irr = h_3(h_2_irr)\n",
    "irr_score = s(h_3_irr)\n",
    "\n",
    "# Subtract scores.\n",
    "negated_irr_score = Lambda(lambda x: -1 * x, output_shape = (1, ))(irr_score)\n",
    "diff = Add()([rel_score, negated_irr_score])\n",
    "\n",
    "# Pass difference through sigmoid function.\n",
    "prob = Activation(\"sigmoid\")(diff)\n",
    "\n",
    "# Build model.\n",
    "model = Model(inputs = [rel_doc, irr_doc], outputs = prob)\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "def getJudgmentsBatchFileByFile():\n",
    "    \"\"\"\n",
    "    Returns a generator function that returns all the judgment batches files from the directory\n",
    "    \"\"\"\n",
    "    files = []\n",
    "   \n",
    "    files = [join('./loggedFeatures/devCorpusFeatures', file) for file in listdir('./loggedFeatures/devCorpusFeatures/') if isfile(join('./loggedFeatures/devCorpusFeatures', file))]\n",
    "    for file in files:\n",
    "        yield file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "judgments = []\n",
    "for file in getJudgmentsBatchFileByFile():\n",
    "  with open(file,'r') as f:\n",
    "    reader = csv.reader(f, delimiter=' ')\n",
    "    for row in reader:\n",
    "      data = []\n",
    "      for element in row:\n",
    "        data.append(element.replace(',', ''))\n",
    "      judgments.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgments.sort(key = lambda judgments: judgments[1])\n",
    "print(judgments[0])\n",
    "print(judgments[1])\n",
    "print(judgments[2])\n",
    "print(judgments[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in judgments:\n",
    "  del j[2]\n",
    "\n",
    "print(judgments[0])\n",
    "print(judgments[1])\n",
    "print(judgments[2])\n",
    "print(judgments[3])\n",
    "print(judgments[4])\n",
    "print(judgments[5])\n",
    "print(judgments[6])\n",
    "print(judgments[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeFeatures(loggedJudgments):\n",
    "    minimums = [100 for _ in loggedJudgments[0][2:]]\n",
    "    maximums = [0 for _ in loggedJudgments[0][2:]]\n",
    "    \n",
    "    for judgment in loggedJudgments:\n",
    "        for idx, feature in enumerate(judgment[2:]):\n",
    "            if minimums[idx] > float(feature):\n",
    "                minimums[idx] = float(feature)\n",
    "            \n",
    "            if maximums[idx] < float(feature):\n",
    "                maximums[idx] = float(feature)\n",
    "    \n",
    "    \n",
    "        \n",
    "    normedJudgments = []\n",
    "    for judgment in loggedJudgments:\n",
    "        normedFeatures = [0 for _ in judgment[2:]]\n",
    "        for idx, feature in enumerate(judgment[2:]):\n",
    "            normedFeatures[idx] = (float(feature) - minimums[idx]) / (maximums[idx] - minimums[idx])\n",
    "        normedJudgment = judgment[:2]\n",
    "        normedJudgment.extend(normedFeatures)\n",
    "        normedJudgments.append(normedJudgment)\n",
    "    \n",
    "    return minimums, maximums, normedJudgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimums, maximums, normalizedJudgments = normalizeFeatures(judgments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairs\n",
    "xi = []\n",
    "xj = []\n",
    "pij = []\n",
    "\n",
    "for i in range(1, len(normalizedJudgments), 2):\n",
    "  if normalizedJudgments[i-1][0] < normalizedJudgments[i][0]:\n",
    "    _pij = 0\n",
    "    xi.append(normalizedJudgments[i - 1][2:])\n",
    "    xj.append(normalizedJudgments[i][2:])\n",
    "    pij.append(_pij)\n",
    "  elif normalizedJudgments[i-1][0] > normalizedJudgments[i][0]:\n",
    "    _pij = 1\n",
    "    xi.append(normalizedJudgments[i - 1][2:])\n",
    "    xj.append(normalizedJudgments[i][2:])\n",
    "    pij.append(_pij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = np.array(xi, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xj = np.array(xj, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pij = np.array(pij, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xi_train, xi_test, xj_train, xj_test, pij_train, pij_test = train_test_split(\n",
    "    xi, xj, pij, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "checkpointer = ModelCheckpoint(filepath = \"training/best_params.h5\", verbose = 1, save_best_only = True)\n",
    "history = model.fit([xi_train, xj_train], pij_train,\n",
    "                     epochs = NUM_EPOCHS, batch_size = BATCH_SIZE, validation_data=([xi_test, xj_test], pij_test),\n",
    "                     callbacks = [checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"training/best_params.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "modelName = 'thesis-ranknet-min-max-100-64-64-feature-work-adam-dev-features'\n",
    "solrModel = {\n",
    "  \"store\": \"thesis-ltr\",\n",
    "  \"class\": \"org.apache.solr.ltr.model.NeuralNetworkModel\",\n",
    "  \"name\": modelName,\n",
    "  \"features\": [\n",
    "  ],\n",
    "  \"params\": {}\n",
    "}\n",
    "\n",
    "featureNames = [\"title_coveredQueryTerms\",\n",
    "    \"headings_coveredQueryTerms\",\n",
    "    \"body_coveredQueryTerms\",\n",
    "    \"document_coveredQueryTerms\",\n",
    "    \"title_coveredQueryTermsRatio\",\n",
    "    \"headings_coveredQueryTermsRatio\",\n",
    "    \"body_coveredQueryTermsRatio\",\n",
    "    \"document_coveredQueryTermsRatio\",\n",
    "    \"title_tf\",\n",
    "    \"headings_tf\",\n",
    "    \"body_tf\",\n",
    "    \"document_tf\",\n",
    "    \"title_idf\",\n",
    "    \"headings_idf\",\n",
    "    \"body_idf\",\n",
    "    \"document_idf\",\n",
    "    \"title_tfidf\",\n",
    "    \"headings_tfidf\",\n",
    "    \"body_tfidf\",\n",
    "    \"document_tfidf\",\n",
    "    \"title_bm25\",\n",
    "    \"headings_bm25\",\n",
    "    \"body_bm25\",\n",
    "    \"document_bm25\"\n",
    "]\n",
    "\n",
    "for idx, featureName in enumerate(featureNames):\n",
    "    config = {\n",
    "        \"name\": featureName,\n",
    "        \"norm\": {\n",
    "            \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
    "            \"params\": {\n",
    "                \"min\": str(minimums[idx]),\n",
    "                \"max\": str(maximums[idx])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    solrModel['features'].append(config)\n",
    "    \n",
    "weights = model.get_weights()\n",
    "\n",
    "layers = []\n",
    "layers.append({\"matrix\": weights[0].T.tolist(),\n",
    "               \"bias\": weights[1].tolist(),\n",
    "               \"activation\": \"relu\"})\n",
    "layers.append({\"matrix\": weights[2].T.tolist(),\n",
    "               \"bias\": weights[3].tolist(),\n",
    "               \"activation\": \"relu\"})\n",
    "layers.append({\"matrix\": weights[4].T.tolist(),\n",
    "              \"bias\": weights[5].tolist(),\n",
    "              \"activation\": \"relu\"})\n",
    "layers.append({\"matrix\": weights[6].T.tolist(),\n",
    "              \"bias\": weights[7].tolist(),\n",
    "              \"activation\": \"identity\"})\n",
    "solrModel[\"params\"][\"layers\"] = layers\n",
    "\n",
    "\n",
    "print(json.dumps(solrModel, indent=2))\n",
    "\n",
    "# Upload the model after deleting the model\n",
    "print('Delete')\n",
    "requests.delete(f'http://localhost:8983/solr/thesis-ltr/schema/model-store/{modelName}').json()\n",
    "print('Add new model')\n",
    "requests.put(f'http://localhost:8983/solr/thesis-ltr/schema/model-store', json=solrModel).json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
