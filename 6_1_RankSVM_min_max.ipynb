{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "def getJudgmentsBatchFileByFile():\n",
    "    \"\"\"\n",
    "    Returns a generator function that returns all the judgment batches files from the directory\n",
    "    \"\"\"\n",
    "    files = []\n",
    "   \n",
    "    files = [join('./loggedFeatures', file) for file in listdir('./loggedFeatures') if isfile(join('./loggedFeatures', file))]\n",
    "    for file in files:\n",
    "        yield file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "judgments = []\n",
    "for file in getJudgmentsBatchFileByFile():\n",
    "  with open(file,'r') as f:\n",
    "    reader = csv.reader(f, delimiter=' ')\n",
    "    for row in reader:\n",
    "      data = []\n",
    "      for element in row:\n",
    "        data.append(element.replace(',', ''))\n",
    "      judgments.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgments.sort(key = lambda judgments: judgments[1])\n",
    "print(judgments[0])\n",
    "print(judgments[1])\n",
    "print(judgments[2])\n",
    "print(judgments[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in judgments:\n",
    "  del j[2]\n",
    "\n",
    "print(judgments[0])\n",
    "print(judgments[1])\n",
    "print(judgments[2])\n",
    "print(judgments[3])\n",
    "print(judgments[4])\n",
    "print(judgments[5])\n",
    "print(judgments[6])\n",
    "print(judgments[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeFeatures(loggedJudgments):\n",
    "    minimums = [100 for _ in loggedJudgments[0][2:]]\n",
    "    maximums = [0 for _ in loggedJudgments[0][2:]]\n",
    "    \n",
    "    for judgment in loggedJudgments:\n",
    "        for idx, feature in enumerate(judgment[2:]):\n",
    "            if minimums[idx] > float(feature):\n",
    "                minimums[idx] = float(feature)\n",
    "            \n",
    "            if maximums[idx] < float(feature):\n",
    "                maximums[idx] = float(feature)\n",
    "    \n",
    "    \n",
    "        \n",
    "    normedJudgments = []\n",
    "    for judgment in loggedJudgments:\n",
    "        normedFeatures = [0 for _ in judgment[2:]]\n",
    "        for idx, feature in enumerate(judgment[2:]):\n",
    "            normedFeatures[idx] = (float(feature) - minimums[idx]) / (maximums[idx] - minimums[idx])\n",
    "        normedJudgment = judgment[:2]\n",
    "        normedJudgment.extend(normedFeatures)\n",
    "        normedJudgments.append(normedJudgment)\n",
    "    \n",
    "    return minimums, maximums, normedJudgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimums, maximums, normalizedJudgments = normalizeFeatures(judgments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def pairwiseTransformation(normalizedJudgments):\n",
    "    \n",
    "    predictorDeltas = []\n",
    "    featureDeltas = []\n",
    "    \n",
    "    for qid, queryJudgments in groupby(normalizedJudgments, key=lambda j: j[1]):\n",
    "    \n",
    "        queryJudgmentsCopy1 = list(queryJudgments)\n",
    "        queryJudgmentsCopy2 = list(queryJudgmentsCopy1)\n",
    "        \n",
    "        # Examine every judgment combo for this query, \n",
    "        # if they're different, store the pairwise difference:\n",
    "        # +1 if judgment1 more relevant\n",
    "        # -1 if judgment2 more relevant\n",
    "        for judgment1 in queryJudgmentsCopy1:\n",
    "            \n",
    "            for judgment2 in queryJudgmentsCopy2:\n",
    "                \n",
    "                j1_features=np.array(judgment1[2:])\n",
    "                j2_features=np.array(judgment2[2:])\n",
    "                \n",
    "                if int(judgment1[0]) > int(judgment2[0]):\n",
    "                    predictorDeltas.append(+1)\n",
    "                    featureDeltas.append(j1_features-j2_features)\n",
    "                    \n",
    "                elif int(judgment1[0]) < int(judgment2[0]):\n",
    "                    predictorDeltas.append(-1)\n",
    "                    featureDeltas.append(j1_features-j2_features)\n",
    "                    \n",
    "    # For training purposes, we return these as numpy arrays\n",
    "    return np.array(featureDeltas), np.array(predictorDeltas)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDeltas, predictorDeltas = pairwiseTransformation(normalizedJudgments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "featureDeltas_train, featureDeltas_test, predictorDeltas_train, predictorDeltas_test = train_test_split(\n",
    "    featureDeltas, predictorDeltas, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model = svm.LinearSVC(max_iter=1000000000, verbose=1, C=1, penalty=\"l2\", loss='squared_hinge')\n",
    "model.fit(featureDeltas_train, predictorDeltas_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation should be here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, featureDeltas_train, predictorDeltas_train)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(featureDeltas_test, predictorDeltas_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solr model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import requests\n",
    "import json\n",
    "\n",
    "modelName = 'thesis-svm-1-l2-squared_hinge-min-max-cross_validated'\n",
    "linearModel = {\n",
    "  \"store\": \"thesis-ltr\",\n",
    "  \"class\": \"org.apache.solr.ltr.model.LinearModel\",\n",
    "  \"name\": modelName,\n",
    "  \"features\": [\n",
    "  ],\n",
    "  \"params\": {\n",
    "      \"weights\": {\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "featureNames = [\"title_coveredQueryTerms\",\n",
    "    \"headings_coveredQueryTerms\",\n",
    "    \"body_coveredQueryTerms\",\n",
    "    \"document_coveredQueryTerms\",\n",
    "    \"title_coveredQueryTermsRatio\",\n",
    "    \"headings_coveredQueryTermsRatio\",\n",
    "    \"body_coveredQueryTermsRatio\",\n",
    "    \"document_coveredQueryTermsRatio\",\n",
    "    \"title_tf\",\n",
    "    \"headings_tf\",\n",
    "    \"body_tf\",\n",
    "    \"document_tf\",\n",
    "    \"title_idf\",\n",
    "    \"headings_idf\",\n",
    "    \"body_idf\",\n",
    "    \"document_idf\",\n",
    "    \"title_tfidf\",\n",
    "    \"headings_tfidf\",\n",
    "    \"body_tfidf\",\n",
    "    \"document_tfidf\",\n",
    "    \"title_bm25\",\n",
    "    \"headings_bm25\",\n",
    "    \"body_bm25\",\n",
    "    \"document_bm25\"\n",
    "]\n",
    "\n",
    "for idx, featureName in enumerate(featureNames):\n",
    "    config = {\n",
    "        \"name\": featureName,\n",
    "        \"norm\": {\n",
    "            \"class\": \"org.apache.solr.ltr.norm.MinMaxNormalizer\",\n",
    "            \"params\": {\n",
    "                \"min\": str(minimums[idx]),\n",
    "                \"max\": str(maximums[idx])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    linearModel['features'].append(config)\n",
    "    linearModel['params']['weights'][featureName] =  model.coef_[0][idx] \n",
    "\n",
    "print(json.dumps(linearModel, indent=2))\n",
    "\n",
    "# Upload the model after deleting the model\n",
    "print('Delete')\n",
    "requests.delete(f'http://localhost:8983/solr/thesis-ltr/schema/model-store/{modelName}').json()\n",
    "print('Add new model')\n",
    "requests.put(f'http://localhost:8983/solr/thesis-ltr/schema/model-store', json=linearModel).json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
