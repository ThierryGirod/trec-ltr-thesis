{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow matplotlib scikit-learn pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import backend\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Activation, Add, Dense, Input, Lambda\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 24\n",
    "h_1_dim = 64\n",
    "h_2_dim = h_1_dim // 2\n",
    "h_3_dim = h_2_dim // 2\n",
    "\n",
    "# Model.\n",
    "h_1 = Dense(h_1_dim, activation = \"relu\")\n",
    "h_2 = Dense(h_2_dim, activation = \"relu\")\n",
    "h_3 = Dense(h_3_dim, activation = \"relu\")\n",
    "s = Dense(1)\n",
    "\n",
    "# Relevant document score.\n",
    "rel_doc = Input(shape = (INPUT_DIM, ), dtype = \"float32\")\n",
    "h_1_rel = h_1(rel_doc)\n",
    "h_2_rel = h_2(h_1_rel)\n",
    "h_3_rel = h_3(h_2_rel)\n",
    "rel_score = s(h_3_rel)\n",
    "\n",
    "# Irrelevant document score.\n",
    "irr_doc = Input(shape = (INPUT_DIM, ), dtype = \"float32\")\n",
    "h_1_irr = h_1(irr_doc)\n",
    "h_2_irr = h_2(h_1_irr)\n",
    "h_3_irr = h_3(h_2_irr)\n",
    "irr_score = s(h_3_irr)\n",
    "\n",
    "# Subtract scores.\n",
    "negated_irr_score = Lambda(lambda x: -1 * x, output_shape = (1, ))(irr_score)\n",
    "diff = Add()([rel_score, negated_irr_score])\n",
    "\n",
    "# Pass difference through sigmoid function.\n",
    "prob = Activation(\"sigmoid\")(diff)\n",
    "\n",
    "# Build model.\n",
    "model = Model(inputs = [rel_doc, irr_doc], outputs = prob)\n",
    "model.compile(optimizer = \"adagrad\", loss = \"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "def getJudgmentsBatchFileByFile():\n",
    "    \"\"\"\n",
    "    Returns a generator function that returns all the judgment batches files from the directory\n",
    "    \"\"\"\n",
    "    files = []\n",
    "   \n",
    "    files = [join('./loggedFeatures', file) for file in listdir('./loggedFeatures') if isfile(join('./loggedFeatures', file))]\n",
    "    for file in files:\n",
    "        yield file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "judgments = []\n",
    "for file in getJudgmentsBatchFileByFile():\n",
    "  with open(file,'r') as f:\n",
    "    reader = csv.reader(f, delimiter=' ')\n",
    "    for row in reader:\n",
    "      data = []\n",
    "      for element in row:\n",
    "        data.append(element.replace(',', ''))\n",
    "      judgments.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgments.sort(key = lambda judgments: judgments[1])\n",
    "print(judgments[0])\n",
    "print(judgments[1])\n",
    "print(judgments[2])\n",
    "print(judgments[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in judgments:\n",
    "  del j[2]\n",
    "\n",
    "print(judgments[0])\n",
    "print(judgments[1])\n",
    "print(judgments[2])\n",
    "print(judgments[3])\n",
    "print(judgments[4])\n",
    "print(judgments[5])\n",
    "print(judgments[6])\n",
    "print(judgments[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairs\n",
    "xi = []\n",
    "xj = []\n",
    "pij = []\n",
    "judgmentPairs = []\n",
    "\n",
    "for i in range(1, len(judgments), 2):\n",
    "  judgmentPairs.append([judgments[i - 1], judgments[i]])\n",
    "  xi.append(judgments[i - 1][2:])\n",
    "  xj.append(judgments[i][2:])\n",
    "  if judgments[i-1][0] == judgments[i][0]:\n",
    "    _pij = 0.5\n",
    "  elif judgments[i-1][0] > judgments[i][0]:\n",
    "    _pij = 1\n",
    "  else: \n",
    "    _pij = 0\n",
    "  pij.append(_pij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = np.array(xi, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xj = np.array(xj, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pij = np.array(pij, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xi_train, xi_test, xj_train, xj_test, pij_train, pij_test = train_test_split(\n",
    "    xi, xj, pij, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "checkpointer = ModelCheckpoint(filepath = \"training/valid_params.h5\", verbose = 1, save_best_only = True)\n",
    "history = model.fit([xi_train, xj_train], pij_train,\n",
    "                     epochs = NUM_EPOCHS, batch_size = BATCH_SIZE, validation_data=([xi_test, xj_test], pij_test),\n",
    "                     callbacks = [checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "modelName = \"thesis-ranknet\"\n",
    "solrModel = {\"store\" : \"thesis-ltr\",\n",
    "              \"name\" :  modelName,\n",
    "              \"class\" : \"org.apache.solr.ltr.model.NeuralNetworkModel\",\n",
    "              \"features\" : [\n",
    "                { \"name\" : \"title_coveredQueryTerms\" },\n",
    "                { \"name\" : \"headings_coveredQueryTerms\" },\n",
    "                { \"name\" : \"body_coveredQueryTerms\" },\n",
    "                { \"name\" : \"document_coveredQueryTerms\" },\n",
    "                { \"name\" : \"title_coveredQueryTermsRatio\" },\n",
    "                { \"name\" : \"headings_coveredQueryTermsRatio\" },\n",
    "                { \"name\" : \"body_coveredQueryTermsRatio\" },\n",
    "                { \"name\" : \"document_coveredQueryTermsRatio\" },\n",
    "                { \"name\" : \"title_tf\" },\n",
    "                { \"name\" : \"headings_tf\" },\n",
    "                { \"name\" : \"body_tf\" },\n",
    "                { \"name\" : \"document_tf\" },\n",
    "                { \"name\" : \"title_idf\" },\n",
    "                { \"name\" : \"headings_idf\" },\n",
    "                { \"name\" : \"body_idf\" },\n",
    "                { \"name\" : \"document_idf\" },\n",
    "                { \"name\" : \"title_tfidf\" },\n",
    "                { \"name\" : \"headings_tfidf\" },\n",
    "                { \"name\" : \"body_tfidf\" },\n",
    "                { \"name\" : \"document_tfidf\" },\n",
    "                { \"name\" : \"title_bm25\" },\n",
    "                { \"name\" : \"headings_bm25\" },\n",
    "                { \"name\" : \"body_bm25\" },\n",
    "                { \"name\" : \"document_bm25\" }\n",
    "              ],\n",
    "              \"params\": {}}\n",
    "\n",
    "weights = model.get_weights()\n",
    "\n",
    "layers = []\n",
    "layers.append({\"matrix\": weights[0].T.tolist(),\n",
    "               \"bias\": weights[1].tolist(),\n",
    "               \"activation\": \"relu\"})\n",
    "layers.append({\"matrix\": weights[2].T.tolist(),\n",
    "               \"bias\": weights[3].tolist(),\n",
    "               \"activation\": \"relu\"})\n",
    "layers.append({\"matrix\": weights[4].T.tolist(),\n",
    "              \"bias\": weights[5].tolist(),\n",
    "              \"activation\": \"relu\"})\n",
    "layers.append({\"matrix\": weights[6].T.tolist(),\n",
    "              \"bias\": weights[7].tolist(),\n",
    "              \"activation\": \"identity\"})\n",
    "solrModel[\"params\"][\"layers\"] = layers\n",
    "\n",
    "\n",
    "print(json.dumps(solrModel, indent=2))\n",
    "\n",
    "# Upload the model after deleting the model\n",
    "print('Delete')\n",
    "requests.delete(f'http://localhost:8983/solr/thesis-ltr/schema/model-store/{modelName}').json()\n",
    "print('Add new model')\n",
    "requests.put(f'http://localhost:8983/solr/thesis-ltr/schema/model-store', json=solrModel).json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
